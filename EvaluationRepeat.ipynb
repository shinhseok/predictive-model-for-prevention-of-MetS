{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88b906f2-3835-4b95-9330-ae422f55d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import joblib\n",
    "import sidetable\n",
    "\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations\n",
    "from sklearn.calibration import calibration_curve, CalibrationDisplay, CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "from numpy import array\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from MetsDataSamplingSeed import get_mets_data, get_metric, get_calib_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e503d89-08e9-4595-a8be-5fb59070cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calib_prob(prob, label, beta):    \n",
    "    return beta*prob/(beta*prob-prob+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bafa5f7a-9b29-49f6-a30b-f32d87c65c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrated_plot(model_name, model, X_train, y_train, X_valid, y_valid, X_test, y_test, bins=10, method = 'sigmoid', is_tabnet=False):\n",
    "    # uncalibrated\n",
    "    #model.fit(X_train, y_train)\n",
    "    #y_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # calibrated\n",
    "    calibrator = CalibratedClassifierCV(model, method=method, cv='prefit') #isotonic\n",
    "    if is_tabnet:\n",
    "        y_prob = model.predict_proba(X_test.values[:])[:,1]\n",
    "        calibrator.fit(X_valid.values[:], y_valid.values[:])\n",
    "        y_hat = calibrator.predict_proba(X_test.values[:])[:,1]\n",
    "    else :\n",
    "        y_prob = model.predict_proba(X_test)[:,1]\n",
    "        calibrator.fit(X_valid, y_valid)\n",
    "        y_hat = calibrator.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    prob_true_uncalibrated, prob_pred_uncalibrated = calibration_curve(y_test, y_prob, n_bins=bins)\n",
    "    prob_true_calibrated, prob_pred_calibrated = calibration_curve(y_test, y_hat, n_bins=bins)\n",
    "    \n",
    "    brier_score_org = brier_score_loss(y_test, y_prob)\n",
    "    brier_score_cali = brier_score_loss(y_test, y_hat)\n",
    "    \n",
    "    # plot perfectly calibrated\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "\n",
    "    # plot model reliabilities\n",
    "    plt.title('Calibration Plot')\n",
    "    plt.plot(prob_pred_uncalibrated, prob_true_uncalibrated, color = 'c', marker='o', label=(model_name+':'+ str(round(brier_score_org,3))))\n",
    "    plt.plot(prob_pred_calibrated, prob_true_calibrated, color = 'm', marker='s', label=(model_name+'(calibrated):'+str(round(brier_score_cali,3))))\n",
    "    plt.xlabel('Mean predicted value')\n",
    "    plt.ylabel('Fraction of positives')\n",
    "    plt.legend()\n",
    "    plt.savefig('./fig/'+model_name+'_calibration.png')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title('Probability Histogram')\n",
    "    plt.hist(y_prob, bins=10, color='c', alpha=0.5, label='origin')\n",
    "    plt.hist(y_hat, bins=10, color='m', alpha=0.5, label='calibrated')\n",
    "    plt.xlabel('Predictied Probabilty')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.savefig('./fig/'+model_name+'_histogram.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return y_prob, y_hat, brier_score_cali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1493714f-ad2d-4554-b28a-09d67d51f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = range(0, 300, 10)\n",
    "all_ft = ['WC', 'BP', 'BPWC_add', 'BPWC_mul', 'BPWC_dif','bWC','whr','CUNBAE', 'clbe', 'G1_INT', 'ss18', 'fate', 'smoke_merge_0','regrp15','regrp18','regrp19','regrp38']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5974342-1c2e-4e2d-8e92-a9aa1ac16c82",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc8a9b51-7242-4e33-b09b-972674be2cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling:  1\n",
      "sampling:  2\n",
      "sampling:  3\n",
      "sampling:  4\n",
      "sampling:  5\n",
      "sampling:  6\n",
      "sampling:  7\n",
      "sampling:  8\n",
      "sampling:  9\n",
      "sampling:  10\n",
      "sampling:  11\n",
      "sampling:  12\n",
      "sampling:  13\n",
      "sampling:  14\n",
      "sampling:  15\n",
      "sampling:  16\n",
      "sampling:  17\n",
      "sampling:  18\n",
      "sampling:  19\n",
      "sampling:  20\n",
      "sampling:  21\n",
      "sampling:  22\n",
      "sampling:  23\n",
      "sampling:  24\n",
      "sampling:  25\n",
      "sampling:  26\n",
      "sampling:  27\n",
      "sampling:  28\n",
      "sampling:  29\n",
      "sampling:  30\n"
     ]
    }
   ],
   "source": [
    "# For LR\n",
    "lr_a = ['sbp', 'CUNBAE', 'AVI', 'dbp']\n",
    "lr_b = ['drink_0', 'drdu', 'dr_soju', 'ss04', 'exer_merge', 'smoke_merge_0']\n",
    "lr_c = ['AVI', 'sbp', 'CUNBAE', 'dbp', 'drdu', 'smoke_merge_0']\n",
    "lr_d = ['WC', 'BP', 'CUNBAE', 'clbe', 'smoke_merge_0']\n",
    "\n",
    "args = {\n",
    "    'penalty' : 'none',\n",
    "    #'solver' : 'liblinear',\n",
    "    'random_state' : 100,\n",
    "    #'C': 1.0\n",
    "}\n",
    "\n",
    "prob_all = None\n",
    "prob_idx = [0]\n",
    "res_all = pd.DataFrame()\n",
    "\n",
    "for i, s in enumerate(seeds) :\n",
    "    print('sampling: ', i+1)\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test, _, _, valid_info, test_info, beta, tau, _ = get_mets_data(\n",
    "        one_hot=True, \n",
    "        resampling = False,\n",
    "        feature_set = None, # 7 = 'anthropometric', 8 = 'lifestyle', 9 = 'anthropometric+lifestyle', 10 = 'anthropometric+lifestyle+synthesis'\n",
    "        set_feature = lr_d,\n",
    "        add_feature= False,\n",
    "        is_tabnet = False,\n",
    "        is_eval = False,\n",
    "        seed = s\n",
    "    )\n",
    "    sc = StandardScaler()\n",
    "    sX_train = sc.fit_transform(X_train)\n",
    "    sX_valid = sc.transform(X_valid)\n",
    "    sX_test = sc.transform(X_test)\n",
    "\n",
    "    X_train_lr = pd.DataFrame(sX_train, columns=X_train.columns)\n",
    "    X_valid_lr = pd.DataFrame(sX_valid, columns=X_valid.columns)\n",
    "    X_test_lr = pd.DataFrame(sX_test, columns=X_test.columns)\n",
    "    \n",
    "    lr = LogisticRegression(**args)\n",
    "    lr.fit(X_train_lr, y_train)\n",
    "    prob = lr.predict_proba(X_valid_lr)\n",
    "    \n",
    "    res = get_metric(prob, y_valid, 0.5)\n",
    "    res = pd.DataFrame.from_dict(res, orient='index')\n",
    "    res_all = pd.concat([res_all,res], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a327b92-2c4a-4a10-bba7-dbd622553700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>bac</th>\n",
       "      <th>recall</th>\n",
       "      <th>ppv</th>\n",
       "      <th>npv</th>\n",
       "      <th>sepecificity</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.794622</td>\n",
       "      <td>0.804579</td>\n",
       "      <td>0.818282</td>\n",
       "      <td>0.382996</td>\n",
       "      <td>0.964839</td>\n",
       "      <td>0.790875</td>\n",
       "      <td>0.521681</td>\n",
       "      <td>0.887254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.004660</td>\n",
       "      <td>0.006437</td>\n",
       "      <td>0.013719</td>\n",
       "      <td>0.010109</td>\n",
       "      <td>0.002753</td>\n",
       "      <td>0.005763</td>\n",
       "      <td>0.010068</td>\n",
       "      <td>0.004833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.785917</td>\n",
       "      <td>0.793280</td>\n",
       "      <td>0.795167</td>\n",
       "      <td>0.368223</td>\n",
       "      <td>0.959504</td>\n",
       "      <td>0.777310</td>\n",
       "      <td>0.507396</td>\n",
       "      <td>0.878202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.791640</td>\n",
       "      <td>0.799923</td>\n",
       "      <td>0.809593</td>\n",
       "      <td>0.374969</td>\n",
       "      <td>0.962717</td>\n",
       "      <td>0.787919</td>\n",
       "      <td>0.512824</td>\n",
       "      <td>0.884452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.794127</td>\n",
       "      <td>0.805930</td>\n",
       "      <td>0.820244</td>\n",
       "      <td>0.380184</td>\n",
       "      <td>0.965337</td>\n",
       "      <td>0.791078</td>\n",
       "      <td>0.519484</td>\n",
       "      <td>0.887155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.796969</td>\n",
       "      <td>0.809584</td>\n",
       "      <td>0.828752</td>\n",
       "      <td>0.390936</td>\n",
       "      <td>0.966964</td>\n",
       "      <td>0.794395</td>\n",
       "      <td>0.529200</td>\n",
       "      <td>0.890831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.804705</td>\n",
       "      <td>0.814833</td>\n",
       "      <td>0.840855</td>\n",
       "      <td>0.406164</td>\n",
       "      <td>0.969779</td>\n",
       "      <td>0.801944</td>\n",
       "      <td>0.541850</td>\n",
       "      <td>0.895939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             acc        bac     recall        ppv        npv  sepecificity  \\\n",
       "count  30.000000  30.000000  30.000000  30.000000  30.000000     30.000000   \n",
       "mean    0.794622   0.804579   0.818282   0.382996   0.964839      0.790875   \n",
       "std     0.004660   0.006437   0.013719   0.010109   0.002753      0.005763   \n",
       "min     0.785917   0.793280   0.795167   0.368223   0.959504      0.777310   \n",
       "25%     0.791640   0.799923   0.809593   0.374969   0.962717      0.787919   \n",
       "50%     0.794127   0.805930   0.820244   0.380184   0.965337      0.791078   \n",
       "75%     0.796969   0.809584   0.828752   0.390936   0.966964      0.794395   \n",
       "max     0.804705   0.814833   0.840855   0.406164   0.969779      0.801944   \n",
       "\n",
       "              f1        auc  \n",
       "count  30.000000  30.000000  \n",
       "mean    0.521681   0.887254  \n",
       "std     0.010068   0.004833  \n",
       "min     0.507396   0.878202  \n",
       "25%     0.512824   0.884452  \n",
       "50%     0.519484   0.887155  \n",
       "75%     0.529200   0.890831  \n",
       "max     0.541850   0.895939  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_all_lr = res_all\n",
    "res_all_lr.transpose().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbb884c2-fd2e-4a57-8302-6de624099a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_all.transpose().to_csv('./fig/feature_d_logisticregression.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9292cf6-bf9b-4645-9445-dfc616f95ffb",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed653523-bed4-4992-86a7-0fe540b1ce00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling:  1\n",
      "sampling:  2\n",
      "sampling:  3\n",
      "sampling:  4\n",
      "sampling:  5\n",
      "sampling:  6\n",
      "sampling:  7\n",
      "sampling:  8\n",
      "sampling:  9\n",
      "sampling:  10\n",
      "sampling:  11\n",
      "sampling:  12\n",
      "sampling:  13\n",
      "sampling:  14\n",
      "sampling:  15\n",
      "sampling:  16\n",
      "sampling:  17\n",
      "sampling:  18\n",
      "sampling:  19\n",
      "sampling:  20\n",
      "sampling:  21\n",
      "sampling:  22\n",
      "sampling:  23\n",
      "sampling:  24\n",
      "sampling:  25\n",
      "sampling:  26\n",
      "sampling:  27\n",
      "sampling:  28\n",
      "sampling:  29\n",
      "sampling:  30\n"
     ]
    }
   ],
   "source": [
    "# For DT\n",
    "dt_a = ['waist', 'sbp', 'BFP', 'BAI', 'WHtR']\n",
    "dt_b = ['ss18', 'ss05', 'ss23', 'ss12', 'ss14', 'ss19', 'ss20']\n",
    "dt_c = ['waist', 'sbp', 'BFP', 'BAI', 'WHtR']\n",
    "dt_d = ['BPWC_add', 'BPWC_mul', 'BPWC_dif']\n",
    "\n",
    "args = {#'criterion': 'entropy',\n",
    "        #'max_depth': 5,\n",
    "        #'max_features': None,\n",
    "        #'min_samples_leaf': 100,\n",
    "        #'min_samples_split': 0.01,\n",
    "        #'splitter': 'best',\n",
    "        'random_state' : 100}\n",
    "\n",
    "prob_all = None\n",
    "res_all = pd.DataFrame()\n",
    "\n",
    "for i, s in enumerate(seeds) :\n",
    "    print('sampling: ', i+1)\n",
    "    \n",
    "    X_train_dt, y_train, X_valid_dt, y_valid, X_test_dt, y_test, _, _, valid_info, test_info, beta, tau, _ = get_mets_data(\n",
    "        one_hot=True, \n",
    "        resampling = False,\n",
    "        feature_set = None, # 7 = 'anthropometric', 8 = 'lifestyle', 9 = 'anthropometric+lifestyle', 10 = 'anthropometric+lifestyle+synthesis'\n",
    "        set_feature = dt_d,#dt_d,\n",
    "        add_feature= False,\n",
    "        is_tabnet = False,\n",
    "        is_eval = False,\n",
    "        seed = s\n",
    "    )\n",
    "\n",
    "    dt = DecisionTreeClassifier(**args)\n",
    "    dt.fit(X_train_dt, y_train)\n",
    "    \n",
    "    prob = dt.predict_proba(X_valid_dt)\n",
    "\n",
    "    res = get_metric(prob, y_valid, 0.5)\n",
    "    res = pd.DataFrame.from_dict(res, orient='index')\n",
    "    res_all = pd.concat([res_all,res], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0f45333-e7c7-4a73-a30b-9f9b8d563473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>bac</th>\n",
       "      <th>recall</th>\n",
       "      <th>ppv</th>\n",
       "      <th>npv</th>\n",
       "      <th>sepecificity</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.736749</td>\n",
       "      <td>0.765889</td>\n",
       "      <td>0.806009</td>\n",
       "      <td>0.318053</td>\n",
       "      <td>0.959351</td>\n",
       "      <td>0.725769</td>\n",
       "      <td>0.456004</td>\n",
       "      <td>0.792142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.006895</td>\n",
       "      <td>0.007998</td>\n",
       "      <td>0.016789</td>\n",
       "      <td>0.010990</td>\n",
       "      <td>0.003365</td>\n",
       "      <td>0.008379</td>\n",
       "      <td>0.012189</td>\n",
       "      <td>0.010519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.721819</td>\n",
       "      <td>0.748774</td>\n",
       "      <td>0.771111</td>\n",
       "      <td>0.297404</td>\n",
       "      <td>0.951541</td>\n",
       "      <td>0.709266</td>\n",
       "      <td>0.433355</td>\n",
       "      <td>0.770794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.733028</td>\n",
       "      <td>0.760622</td>\n",
       "      <td>0.793473</td>\n",
       "      <td>0.307956</td>\n",
       "      <td>0.957805</td>\n",
       "      <td>0.719476</td>\n",
       "      <td>0.444846</td>\n",
       "      <td>0.785111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.736107</td>\n",
       "      <td>0.765975</td>\n",
       "      <td>0.805982</td>\n",
       "      <td>0.318271</td>\n",
       "      <td>0.959240</td>\n",
       "      <td>0.724563</td>\n",
       "      <td>0.457897</td>\n",
       "      <td>0.792094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.741869</td>\n",
       "      <td>0.772215</td>\n",
       "      <td>0.816415</td>\n",
       "      <td>0.326159</td>\n",
       "      <td>0.961348</td>\n",
       "      <td>0.731399</td>\n",
       "      <td>0.465257</td>\n",
       "      <td>0.800954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.748184</td>\n",
       "      <td>0.779631</td>\n",
       "      <td>0.841981</td>\n",
       "      <td>0.338800</td>\n",
       "      <td>0.967068</td>\n",
       "      <td>0.744387</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.811304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             acc        bac     recall        ppv        npv  sepecificity  \\\n",
       "count  30.000000  30.000000  30.000000  30.000000  30.000000     30.000000   \n",
       "mean    0.736749   0.765889   0.806009   0.318053   0.959351      0.725769   \n",
       "std     0.006895   0.007998   0.016789   0.010990   0.003365      0.008379   \n",
       "min     0.721819   0.748774   0.771111   0.297404   0.951541      0.709266   \n",
       "25%     0.733028   0.760622   0.793473   0.307956   0.957805      0.719476   \n",
       "50%     0.736107   0.765975   0.805982   0.318271   0.959240      0.724563   \n",
       "75%     0.741869   0.772215   0.816415   0.326159   0.961348      0.731399   \n",
       "max     0.748184   0.779631   0.841981   0.338800   0.967068      0.744387   \n",
       "\n",
       "              f1        auc  \n",
       "count  30.000000  30.000000  \n",
       "mean    0.456004   0.792142  \n",
       "std     0.012189   0.010519  \n",
       "min     0.433355   0.770794  \n",
       "25%     0.444846   0.785111  \n",
       "50%     0.457897   0.792094  \n",
       "75%     0.465257   0.800954  \n",
       "max     0.477273   0.811304  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_all_dt = res_all\n",
    "res_all_dt.transpose().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d329fe6f-b748-4c9d-9114-d649b00e047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_all.transpose().to_csv('./fig/feature_d_decisiontree.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d2919-ef1a-4421-97f7-28e1aa9af92d",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9307429-3b01-44cf-a30d-c6307fbfa870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling:  1\n",
      "sampling:  2\n",
      "sampling:  3\n",
      "sampling:  4\n",
      "sampling:  5\n",
      "sampling:  6\n",
      "sampling:  7\n",
      "sampling:  8\n",
      "sampling:  9\n",
      "sampling:  10\n",
      "sampling:  11\n",
      "sampling:  12\n",
      "sampling:  13\n",
      "sampling:  14\n",
      "sampling:  15\n",
      "sampling:  16\n",
      "sampling:  17\n",
      "sampling:  18\n",
      "sampling:  19\n",
      "sampling:  20\n",
      "sampling:  21\n",
      "sampling:  22\n",
      "sampling:  23\n",
      "sampling:  24\n",
      "sampling:  25\n",
      "sampling:  26\n",
      "sampling:  27\n",
      "sampling:  28\n",
      "sampling:  29\n",
      "sampling:  30\n"
     ]
    }
   ],
   "source": [
    "# For RF\n",
    "rf_a = ['sbp', 'AVI', 'waist', 'dbp', 'bmi', 'BFP', 'weight', 'CUNBAE']\n",
    "rf_b = ['ss18', 'ss04', 'ss24', 'ss20', 'ss05', 'ss14']\n",
    "rf_c = ['AVI', 'sbp', 'waist', 'dbp', 'BFP', 'CUNBAE', 'ss18', 'ss20']\n",
    "rf_d = ['bWC', 'WC', 'BPWC_dif', 'CUNBAE', 'G1_INT', 'regrp15', 'ss18', 'fate']\n",
    "args = {#'bootstrap': True,\n",
    "    #'max_depth': 6,\n",
    "    #'min_samples_leaf': 4,\n",
    "    #'min_samples_split': 10,\n",
    "    #'n_estimators': 300,\n",
    "    'random_state' : 100}\n",
    "\n",
    "prob_all = None\n",
    "res_all = pd.DataFrame()\n",
    "\n",
    "for i, s in enumerate(seeds) :\n",
    "    print('sampling: ', i+1)\n",
    "    X_train_rf, y_train, X_valid_rf, y_valid, X_test_rf, y_test, _, _, valid_info, test_info, beta, tau, _ = get_mets_data(\n",
    "        one_hot=True, \n",
    "        resampling = False,\n",
    "        feature_set = None, # 7 = 'anthropometric', 8 = 'lifestyle', 9 = 'anthropometric+lifestyle', 10 = 'anthropometric+lifestyle+synthesis'\n",
    "        set_feature = rf_d,\n",
    "        add_feature= False,\n",
    "        is_tabnet = False,\n",
    "        is_eval = False,\n",
    "        seed = s\n",
    "    )\n",
    "\n",
    "    rf = RandomForestClassifier(**args)\n",
    "    rf.fit(X_train_rf, y_train)\n",
    "    \n",
    "    prob = rf.predict_proba(X_valid_rf)\n",
    "    \n",
    "    res = get_metric(prob, y_valid, 0.5)\n",
    "    res = pd.DataFrame.from_dict(res, orient='index')\n",
    "    res_all = pd.concat([res_all,res], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "053f249a-2686-4974-97a4-9a7ac14e9c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>bac</th>\n",
       "      <th>recall</th>\n",
       "      <th>ppv</th>\n",
       "      <th>npv</th>\n",
       "      <th>sepecificity</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.772361</td>\n",
       "      <td>0.810826</td>\n",
       "      <td>0.863787</td>\n",
       "      <td>0.361402</td>\n",
       "      <td>0.972286</td>\n",
       "      <td>0.757865</td>\n",
       "      <td>0.509506</td>\n",
       "      <td>0.885146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.005215</td>\n",
       "      <td>0.006654</td>\n",
       "      <td>0.013571</td>\n",
       "      <td>0.009956</td>\n",
       "      <td>0.002820</td>\n",
       "      <td>0.006171</td>\n",
       "      <td>0.010520</td>\n",
       "      <td>0.004691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.763025</td>\n",
       "      <td>0.795776</td>\n",
       "      <td>0.834884</td>\n",
       "      <td>0.346062</td>\n",
       "      <td>0.966853</td>\n",
       "      <td>0.745288</td>\n",
       "      <td>0.493471</td>\n",
       "      <td>0.875281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.769222</td>\n",
       "      <td>0.806202</td>\n",
       "      <td>0.854321</td>\n",
       "      <td>0.353336</td>\n",
       "      <td>0.970176</td>\n",
       "      <td>0.753750</td>\n",
       "      <td>0.500608</td>\n",
       "      <td>0.881982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.772340</td>\n",
       "      <td>0.811039</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.360157</td>\n",
       "      <td>0.972622</td>\n",
       "      <td>0.757876</td>\n",
       "      <td>0.508677</td>\n",
       "      <td>0.885041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.775182</td>\n",
       "      <td>0.815148</td>\n",
       "      <td>0.870481</td>\n",
       "      <td>0.366317</td>\n",
       "      <td>0.974008</td>\n",
       "      <td>0.761506</td>\n",
       "      <td>0.517494</td>\n",
       "      <td>0.888348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.785128</td>\n",
       "      <td>0.824404</td>\n",
       "      <td>0.898534</td>\n",
       "      <td>0.380834</td>\n",
       "      <td>0.978365</td>\n",
       "      <td>0.771319</td>\n",
       "      <td>0.531229</td>\n",
       "      <td>0.894162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             acc        bac     recall        ppv        npv  sepecificity  \\\n",
       "count  30.000000  30.000000  30.000000  30.000000  30.000000     30.000000   \n",
       "mean    0.772361   0.810826   0.863787   0.361402   0.972286      0.757865   \n",
       "std     0.005215   0.006654   0.013571   0.009956   0.002820      0.006171   \n",
       "min     0.763025   0.795776   0.834884   0.346062   0.966853      0.745288   \n",
       "25%     0.769222   0.806202   0.854321   0.353336   0.970176      0.753750   \n",
       "50%     0.772340   0.811039   0.862745   0.360157   0.972622      0.757876   \n",
       "75%     0.775182   0.815148   0.870481   0.366317   0.974008      0.761506   \n",
       "max     0.785128   0.824404   0.898534   0.380834   0.978365      0.771319   \n",
       "\n",
       "              f1        auc  \n",
       "count  30.000000  30.000000  \n",
       "mean    0.509506   0.885146  \n",
       "std     0.010520   0.004691  \n",
       "min     0.493471   0.875281  \n",
       "25%     0.500608   0.881982  \n",
       "50%     0.508677   0.885041  \n",
       "75%     0.517494   0.888348  \n",
       "max     0.531229   0.894162  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_all_rf = res_all\n",
    "res_all_rf.transpose().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a80fdf01-c230-441a-afd7-66145069a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_all.transpose().to_csv('./fig/feature_d_randomforest.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a878d6-bc33-4da5-8d7c-c2fd6a566d30",
   "metadata": {},
   "source": [
    "### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3ac35ba-eb67-4220-9daf-6e2f263ea030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling:  1\n",
      "sampling:  2\n",
      "sampling:  3\n",
      "sampling:  4\n",
      "sampling:  5\n",
      "sampling:  6\n",
      "sampling:  7\n",
      "sampling:  8\n",
      "sampling:  9\n",
      "sampling:  10\n",
      "sampling:  11\n",
      "sampling:  12\n",
      "sampling:  13\n",
      "sampling:  14\n",
      "sampling:  15\n",
      "sampling:  16\n",
      "sampling:  17\n",
      "sampling:  18\n",
      "sampling:  19\n",
      "sampling:  20\n",
      "sampling:  21\n",
      "sampling:  22\n",
      "sampling:  23\n",
      "sampling:  24\n",
      "sampling:  25\n",
      "sampling:  26\n",
      "sampling:  27\n",
      "sampling:  28\n",
      "sampling:  29\n",
      "sampling:  30\n"
     ]
    }
   ],
   "source": [
    "# For Xgb\n",
    "xgb_a = ['sbp', 'AVI', 'BFP', 'dbp', 'whr', 'CUNBAE', 'bmi']\n",
    "xgb_b = ['sm_total', 'w087', 'eat5_0', 'dr_soju', 'w026', 'w018']\n",
    "xgb_c = ['AVI', 'sbp', 'BFP', 'dbp', 'whr', 'CUNBAE', 'bmi']\n",
    "xgb_d = ['BPWC_add', 'WC', 'bWC', 'whr', 'BP', 'regrp18']\n",
    "\n",
    "args = {#'learning_rate': 0.05, \n",
    "        #'max_depth': 4, \n",
    "        #'n_estimators': 200, \n",
    "        #'subsample': 0.6,\n",
    "        'random_state' : 100}\n",
    "\n",
    "prob_all = None\n",
    "res_all = pd.DataFrame()\n",
    "\n",
    "for i, s in enumerate(seeds) :\n",
    "    print('sampling: ', i+1)\n",
    "    X_train_xgb, y_train, X_valid_xgb, y_valid, X_test_xgb, y_test, _, _, valid_info, test_info, beta, tau, _ = get_mets_data(\n",
    "        one_hot=True, \n",
    "        resampling = False,\n",
    "        feature_set = None, # 7 = 'anthropometric', 8 = 'lifestyle', 9 = 'anthropometric+lifestyle', 10 = 'anthropometric+lifestyle+synthesis'\n",
    "        set_feature = xgb_d,\n",
    "        add_feature= False,\n",
    "        is_tabnet = False,\n",
    "        is_eval = False,\n",
    "        seed = s\n",
    "    )\n",
    "    \n",
    "    xgb = XGBClassifier(**args)\n",
    "    xgb.fit(X_train_xgb, y_train)\n",
    "\n",
    "    prob = xgb.predict_proba(X_valid_xgb)\n",
    "        \n",
    "    res = get_metric(xgb.predict_proba(X_valid_xgb), y_valid, 0.5)\n",
    "    res = pd.DataFrame.from_dict(res, orient='index')\n",
    "    res_all = pd.concat([res_all,res], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e35ae7b-cea1-40c8-8cc2-3fb11a9621d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>bac</th>\n",
       "      <th>recall</th>\n",
       "      <th>ppv</th>\n",
       "      <th>npv</th>\n",
       "      <th>sepecificity</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.773634</td>\n",
       "      <td>0.808695</td>\n",
       "      <td>0.856961</td>\n",
       "      <td>0.362039</td>\n",
       "      <td>0.971026</td>\n",
       "      <td>0.760428</td>\n",
       "      <td>0.508937</td>\n",
       "      <td>0.883715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.005226</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.011325</td>\n",
       "      <td>0.010229</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>0.006110</td>\n",
       "      <td>0.010442</td>\n",
       "      <td>0.004686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.765867</td>\n",
       "      <td>0.797865</td>\n",
       "      <td>0.835414</td>\n",
       "      <td>0.342170</td>\n",
       "      <td>0.966208</td>\n",
       "      <td>0.750728</td>\n",
       "      <td>0.487583</td>\n",
       "      <td>0.874141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.770287</td>\n",
       "      <td>0.804495</td>\n",
       "      <td>0.848738</td>\n",
       "      <td>0.355457</td>\n",
       "      <td>0.969297</td>\n",
       "      <td>0.755533</td>\n",
       "      <td>0.500819</td>\n",
       "      <td>0.880732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.772261</td>\n",
       "      <td>0.806724</td>\n",
       "      <td>0.855061</td>\n",
       "      <td>0.359409</td>\n",
       "      <td>0.970863</td>\n",
       "      <td>0.760052</td>\n",
       "      <td>0.508442</td>\n",
       "      <td>0.883603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.776287</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.863973</td>\n",
       "      <td>0.367640</td>\n",
       "      <td>0.972902</td>\n",
       "      <td>0.763606</td>\n",
       "      <td>0.515825</td>\n",
       "      <td>0.886578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.788759</td>\n",
       "      <td>0.819584</td>\n",
       "      <td>0.885613</td>\n",
       "      <td>0.384080</td>\n",
       "      <td>0.977074</td>\n",
       "      <td>0.777086</td>\n",
       "      <td>0.529311</td>\n",
       "      <td>0.892713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             acc        bac     recall        ppv        npv  sepecificity  \\\n",
       "count  30.000000  30.000000  30.000000  30.000000  30.000000     30.000000   \n",
       "mean    0.773634   0.808695   0.856961   0.362039   0.971026      0.760428   \n",
       "std     0.005226   0.005784   0.011325   0.010229   0.002431      0.006110   \n",
       "min     0.765867   0.797865   0.835414   0.342170   0.966208      0.750728   \n",
       "25%     0.770287   0.804495   0.848738   0.355457   0.969297      0.755533   \n",
       "50%     0.772261   0.806724   0.855061   0.359409   0.970863      0.760052   \n",
       "75%     0.776287   0.813427   0.863973   0.367640   0.972902      0.763606   \n",
       "max     0.788759   0.819584   0.885613   0.384080   0.977074      0.777086   \n",
       "\n",
       "              f1        auc  \n",
       "count  30.000000  30.000000  \n",
       "mean    0.508937   0.883715  \n",
       "std     0.010442   0.004686  \n",
       "min     0.487583   0.874141  \n",
       "25%     0.500819   0.880732  \n",
       "50%     0.508442   0.883603  \n",
       "75%     0.515825   0.886578  \n",
       "max     0.529311   0.892713  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_all_xgb = res_all\n",
    "res_all_xgb.transpose().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88e38c46-321a-43e2-969f-9e634d823a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_all.transpose().to_csv('./fig/feature_d_xgboost.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12dbd87-c16e-4769-9ed1-c011d53d301d",
   "metadata": {},
   "source": [
    "### TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76b82057-4bfb-4ff6-aa6d-a5d0d139f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train) :\n",
    "    model.fit(\n",
    "    X_train=X_train.values[:], y_train=y_train.values[:],\n",
    "    #eval_set=[(X_train.values[:], y_train.values[:]), (X_valid.values[:], y_valid.values[:])],\n",
    "    #eval_name=['train', 'valid'],\n",
    "    eval_metric=['auc'],\n",
    "    max_epochs=100, \n",
    "    patience=50,\n",
    "    batch_size=1024, \n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    weights=1,\n",
    "    drop_last=False)\n",
    "    return model\n",
    "\n",
    "def train_with_valid(model, X_train, y_train, X_valid, y_valid) :\n",
    "    model.fit(\n",
    "    X_train=X_train.values[:], y_train=y_train.values[:],\n",
    "    eval_set=[(X_train.values[:], y_train.values[:]), (X_valid.values[:], y_valid.values[:])],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['auc'],\n",
    "    max_epochs=100, \n",
    "    patience=50,\n",
    "    batch_size=1024, \n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    weights=1,\n",
    "    drop_last=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1233f5ef-12db-4c92-ab23-3fff358640a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WC',\n",
       " 'BP',\n",
       " 'BPWC_add',\n",
       " 'BPWC_mul',\n",
       " 'BPWC_dif',\n",
       " 'bWC',\n",
       " 'whr',\n",
       " 'CUNBAE',\n",
       " 'clbe',\n",
       " 'G1_INT',\n",
       " 'ss18',\n",
       " 'fate',\n",
       " 'smoke_merge_0',\n",
       " 'regrp15',\n",
       " 'regrp18',\n",
       " 'regrp19',\n",
       " 'regrp38']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6ded794-245f-4fa1-af0d-100551514250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69133 | train_auc: 0.65511 | valid_auc: 0.65726 |  0:00:00s\n",
      "epoch 1  | loss: 0.48603 | train_auc: 0.76084 | valid_auc: 0.76587 |  0:00:01s\n",
      "epoch 2  | loss: 0.45039 | train_auc: 0.80735 | valid_auc: 0.80332 |  0:00:01s\n",
      "epoch 3  | loss: 0.43453 | train_auc: 0.81288 | valid_auc: 0.81305 |  0:00:02s\n",
      "epoch 4  | loss: 0.42724 | train_auc: 0.86337 | valid_auc: 0.86604 |  0:00:02s\n",
      "epoch 5  | loss: 0.42434 | train_auc: 0.87254 | valid_auc: 0.87701 |  0:00:03s\n",
      "epoch 6  | loss: 0.42927 | train_auc: 0.85049 | valid_auc: 0.85186 |  0:00:03s\n",
      "epoch 7  | loss: 0.42642 | train_auc: 0.87092 | valid_auc: 0.87729 |  0:00:04s\n",
      "epoch 8  | loss: 0.42199 | train_auc: 0.87737 | valid_auc: 0.88393 |  0:00:04s\n",
      "epoch 9  | loss: 0.4198  | train_auc: 0.88023 | valid_auc: 0.88517 |  0:00:05s\n",
      "epoch 10 | loss: 0.41929 | train_auc: 0.87903 | valid_auc: 0.88444 |  0:00:06s\n",
      "epoch 11 | loss: 0.42083 | train_auc: 0.88359 | valid_auc: 0.88698 |  0:00:06s\n",
      "epoch 12 | loss: 0.41411 | train_auc: 0.88575 | valid_auc: 0.88927 |  0:00:07s\n",
      "epoch 13 | loss: 0.4221  | train_auc: 0.88346 | valid_auc: 0.88855 |  0:00:07s\n",
      "epoch 14 | loss: 0.41974 | train_auc: 0.88622 | valid_auc: 0.89013 |  0:00:08s\n",
      "epoch 15 | loss: 0.41822 | train_auc: 0.8865  | valid_auc: 0.89069 |  0:00:08s\n",
      "epoch 16 | loss: 0.42532 | train_auc: 0.88695 | valid_auc: 0.89195 |  0:00:09s\n",
      "epoch 17 | loss: 0.42179 | train_auc: 0.88805 | valid_auc: 0.89112 |  0:00:10s\n",
      "epoch 18 | loss: 0.42074 | train_auc: 0.88746 | valid_auc: 0.89114 |  0:00:10s\n",
      "epoch 19 | loss: 0.41832 | train_auc: 0.88689 | valid_auc: 0.88992 |  0:00:11s\n",
      "epoch 20 | loss: 0.4163  | train_auc: 0.88791 | valid_auc: 0.89247 |  0:00:11s\n",
      "epoch 21 | loss: 0.42346 | train_auc: 0.88811 | valid_auc: 0.89265 |  0:00:12s\n",
      "epoch 22 | loss: 0.41872 | train_auc: 0.88846 | valid_auc: 0.89226 |  0:00:12s\n",
      "epoch 23 | loss: 0.42543 | train_auc: 0.88891 | valid_auc: 0.89143 |  0:00:13s\n",
      "epoch 24 | loss: 0.41461 | train_auc: 0.88887 | valid_auc: 0.89133 |  0:00:13s\n",
      "epoch 25 | loss: 0.41925 | train_auc: 0.88821 | valid_auc: 0.89214 |  0:00:14s\n",
      "epoch 26 | loss: 0.42475 | train_auc: 0.88874 | valid_auc: 0.89171 |  0:00:14s\n",
      "epoch 27 | loss: 0.4164  | train_auc: 0.88894 | valid_auc: 0.89092 |  0:00:15s\n",
      "epoch 28 | loss: 0.42189 | train_auc: 0.88851 | valid_auc: 0.89197 |  0:00:16s\n",
      "epoch 29 | loss: 0.41655 | train_auc: 0.88749 | valid_auc: 0.89063 |  0:00:16s\n",
      "epoch 30 | loss: 0.41926 | train_auc: 0.88912 | valid_auc: 0.89231 |  0:00:17s\n",
      "epoch 31 | loss: 0.41134 | train_auc: 0.8884  | valid_auc: 0.89101 |  0:00:17s\n",
      "epoch 32 | loss: 0.42732 | train_auc: 0.88939 | valid_auc: 0.89259 |  0:00:18s\n",
      "epoch 33 | loss: 0.42052 | train_auc: 0.88931 | valid_auc: 0.89327 |  0:00:18s\n",
      "epoch 34 | loss: 0.41466 | train_auc: 0.88905 | valid_auc: 0.89276 |  0:00:19s\n",
      "epoch 35 | loss: 0.41936 | train_auc: 0.88862 | valid_auc: 0.89184 |  0:00:19s\n",
      "epoch 36 | loss: 0.41223 | train_auc: 0.88906 | valid_auc: 0.89088 |  0:00:20s\n",
      "epoch 37 | loss: 0.41244 | train_auc: 0.889   | valid_auc: 0.8925  |  0:00:20s\n",
      "epoch 38 | loss: 0.4289  | train_auc: 0.88849 | valid_auc: 0.89195 |  0:00:21s\n",
      "epoch 39 | loss: 0.41621 | train_auc: 0.88923 | valid_auc: 0.89245 |  0:00:22s\n",
      "epoch 40 | loss: 0.41323 | train_auc: 0.88958 | valid_auc: 0.8914  |  0:00:22s\n",
      "epoch 41 | loss: 0.41494 | train_auc: 0.88901 | valid_auc: 0.89121 |  0:00:23s\n",
      "epoch 42 | loss: 0.41151 | train_auc: 0.88935 | valid_auc: 0.89129 |  0:00:23s\n",
      "epoch 43 | loss: 0.41815 | train_auc: 0.88931 | valid_auc: 0.89301 |  0:00:24s\n",
      "epoch 44 | loss: 0.42025 | train_auc: 0.88959 | valid_auc: 0.89176 |  0:00:24s\n",
      "epoch 45 | loss: 0.41076 | train_auc: 0.88928 | valid_auc: 0.89271 |  0:00:25s\n",
      "epoch 46 | loss: 0.41748 | train_auc: 0.88958 | valid_auc: 0.89213 |  0:00:25s\n",
      "epoch 47 | loss: 0.41274 | train_auc: 0.8895  | valid_auc: 0.89051 |  0:00:26s\n",
      "epoch 48 | loss: 0.41915 | train_auc: 0.88979 | valid_auc: 0.89166 |  0:00:26s\n",
      "epoch 49 | loss: 0.41407 | train_auc: 0.88881 | valid_auc: 0.89088 |  0:00:27s\n",
      "epoch 50 | loss: 0.40704 | train_auc: 0.8892  | valid_auc: 0.89185 |  0:00:28s\n",
      "epoch 51 | loss: 0.42073 | train_auc: 0.88933 | valid_auc: 0.89273 |  0:00:28s\n",
      "epoch 52 | loss: 0.4166  | train_auc: 0.88863 | valid_auc: 0.89068 |  0:00:29s\n",
      "epoch 53 | loss: 0.41945 | train_auc: 0.88898 | valid_auc: 0.89199 |  0:00:29s\n",
      "epoch 54 | loss: 0.42382 | train_auc: 0.88942 | valid_auc: 0.89071 |  0:00:30s\n",
      "epoch 55 | loss: 0.42497 | train_auc: 0.88976 | valid_auc: 0.89213 |  0:00:30s\n",
      "epoch 56 | loss: 0.4189  | train_auc: 0.88957 | valid_auc: 0.89197 |  0:00:31s\n",
      "epoch 57 | loss: 0.41865 | train_auc: 0.88949 | valid_auc: 0.89171 |  0:00:31s\n",
      "epoch 58 | loss: 0.41907 | train_auc: 0.88965 | valid_auc: 0.89336 |  0:00:32s\n",
      "epoch 59 | loss: 0.41503 | train_auc: 0.88876 | valid_auc: 0.89185 |  0:00:33s\n",
      "epoch 60 | loss: 0.40809 | train_auc: 0.88912 | valid_auc: 0.89238 |  0:00:33s\n",
      "epoch 61 | loss: 0.42158 | train_auc: 0.88913 | valid_auc: 0.89249 |  0:00:34s\n",
      "epoch 62 | loss: 0.41421 | train_auc: 0.88941 | valid_auc: 0.89222 |  0:00:34s\n",
      "epoch 63 | loss: 0.42126 | train_auc: 0.88796 | valid_auc: 0.89139 |  0:00:35s\n",
      "epoch 64 | loss: 0.41752 | train_auc: 0.88819 | valid_auc: 0.89226 |  0:00:35s\n",
      "epoch 65 | loss: 0.41448 | train_auc: 0.89011 | valid_auc: 0.8926  |  0:00:36s\n",
      "epoch 66 | loss: 0.42075 | train_auc: 0.88991 | valid_auc: 0.89222 |  0:00:36s\n",
      "epoch 67 | loss: 0.4181  | train_auc: 0.88979 | valid_auc: 0.89241 |  0:00:37s\n",
      "epoch 68 | loss: 0.41866 | train_auc: 0.88959 | valid_auc: 0.89335 |  0:00:37s\n",
      "epoch 69 | loss: 0.42557 | train_auc: 0.88947 | valid_auc: 0.89401 |  0:00:38s\n",
      "epoch 70 | loss: 0.41432 | train_auc: 0.88939 | valid_auc: 0.89288 |  0:00:39s\n",
      "epoch 71 | loss: 0.41548 | train_auc: 0.88948 | valid_auc: 0.89272 |  0:00:39s\n",
      "epoch 72 | loss: 0.41414 | train_auc: 0.88941 | valid_auc: 0.89191 |  0:00:40s\n",
      "epoch 73 | loss: 0.42803 | train_auc: 0.88841 | valid_auc: 0.89165 |  0:00:40s\n",
      "epoch 74 | loss: 0.41097 | train_auc: 0.88945 | valid_auc: 0.8936  |  0:00:41s\n",
      "epoch 75 | loss: 0.42162 | train_auc: 0.889   | valid_auc: 0.89247 |  0:00:41s\n",
      "epoch 76 | loss: 0.42055 | train_auc: 0.88923 | valid_auc: 0.8938  |  0:00:42s\n",
      "epoch 77 | loss: 0.40656 | train_auc: 0.88913 | valid_auc: 0.89351 |  0:00:42s\n",
      "epoch 78 | loss: 0.41858 | train_auc: 0.88974 | valid_auc: 0.89218 |  0:00:43s\n",
      "epoch 79 | loss: 0.42227 | train_auc: 0.88951 | valid_auc: 0.89331 |  0:00:43s\n",
      "epoch 80 | loss: 0.41237 | train_auc: 0.88943 | valid_auc: 0.89289 |  0:00:44s\n",
      "epoch 81 | loss: 0.41403 | train_auc: 0.88937 | valid_auc: 0.89295 |  0:00:45s\n",
      "epoch 82 | loss: 0.4149  | train_auc: 0.88909 | valid_auc: 0.89253 |  0:00:45s\n",
      "epoch 83 | loss: 0.42008 | train_auc: 0.88954 | valid_auc: 0.89258 |  0:00:46s\n",
      "epoch 84 | loss: 0.42106 | train_auc: 0.88936 | valid_auc: 0.89338 |  0:00:46s\n",
      "epoch 85 | loss: 0.41844 | train_auc: 0.88933 | valid_auc: 0.89197 |  0:00:47s\n",
      "epoch 86 | loss: 0.4113  | train_auc: 0.89006 | valid_auc: 0.89253 |  0:00:47s\n",
      "epoch 87 | loss: 0.41405 | train_auc: 0.88966 | valid_auc: 0.89313 |  0:00:48s\n",
      "epoch 88 | loss: 0.42089 | train_auc: 0.88884 | valid_auc: 0.89201 |  0:00:48s\n",
      "epoch 89 | loss: 0.41435 | train_auc: 0.88886 | valid_auc: 0.89159 |  0:00:49s\n",
      "epoch 90 | loss: 0.42233 | train_auc: 0.88958 | valid_auc: 0.89235 |  0:00:49s\n",
      "epoch 91 | loss: 0.4138  | train_auc: 0.88983 | valid_auc: 0.89265 |  0:00:50s\n",
      "epoch 92 | loss: 0.41016 | train_auc: 0.89026 | valid_auc: 0.89224 |  0:00:51s\n",
      "epoch 93 | loss: 0.41411 | train_auc: 0.89006 | valid_auc: 0.89216 |  0:00:51s\n",
      "epoch 94 | loss: 0.41652 | train_auc: 0.89022 | valid_auc: 0.89242 |  0:00:52s\n",
      "epoch 95 | loss: 0.41771 | train_auc: 0.88994 | valid_auc: 0.89301 |  0:00:52s\n",
      "epoch 96 | loss: 0.41526 | train_auc: 0.88943 | valid_auc: 0.89012 |  0:00:53s\n",
      "epoch 97 | loss: 0.41625 | train_auc: 0.88925 | valid_auc: 0.89168 |  0:00:53s\n",
      "epoch 98 | loss: 0.41393 | train_auc: 0.88942 | valid_auc: 0.89279 |  0:00:54s\n",
      "epoch 99 | loss: 0.41988 | train_auc: 0.88966 | valid_auc: 0.89317 |  0:00:54s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 69 and best_valid_auc = 0.89401\n",
      "sampling:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69993 | train_auc: 0.63875 | valid_auc: 0.63336 |  0:00:00s\n",
      "epoch 1  | loss: 0.48201 | train_auc: 0.7576  | valid_auc: 0.75736 |  0:00:01s\n",
      "epoch 2  | loss: 0.44222 | train_auc: 0.81477 | valid_auc: 0.81803 |  0:00:01s\n",
      "epoch 3  | loss: 0.44032 | train_auc: 0.84583 | valid_auc: 0.85106 |  0:00:02s\n",
      "epoch 4  | loss: 0.43043 | train_auc: 0.85717 | valid_auc: 0.86323 |  0:00:02s\n",
      "epoch 5  | loss: 0.43156 | train_auc: 0.86371 | valid_auc: 0.86776 |  0:00:03s\n",
      "epoch 6  | loss: 0.43077 | train_auc: 0.87053 | valid_auc: 0.87466 |  0:00:03s\n",
      "epoch 7  | loss: 0.43174 | train_auc: 0.87489 | valid_auc: 0.87728 |  0:00:04s\n",
      "epoch 8  | loss: 0.4213  | train_auc: 0.87591 | valid_auc: 0.87912 |  0:00:05s\n",
      "epoch 9  | loss: 0.41736 | train_auc: 0.87662 | valid_auc: 0.8817  |  0:00:05s\n",
      "epoch 10 | loss: 0.42228 | train_auc: 0.88003 | valid_auc: 0.88417 |  0:00:06s\n",
      "epoch 11 | loss: 0.4226  | train_auc: 0.87829 | valid_auc: 0.88506 |  0:00:06s\n",
      "epoch 12 | loss: 0.42747 | train_auc: 0.8824  | valid_auc: 0.8855  |  0:00:07s\n",
      "epoch 13 | loss: 0.41593 | train_auc: 0.88536 | valid_auc: 0.89113 |  0:00:07s\n",
      "epoch 14 | loss: 0.42368 | train_auc: 0.88515 | valid_auc: 0.89057 |  0:00:08s\n",
      "epoch 15 | loss: 0.42364 | train_auc: 0.88542 | valid_auc: 0.89284 |  0:00:08s\n",
      "epoch 16 | loss: 0.41743 | train_auc: 0.88785 | valid_auc: 0.8946  |  0:00:09s\n",
      "epoch 17 | loss: 0.4173  | train_auc: 0.88754 | valid_auc: 0.89331 |  0:00:10s\n",
      "epoch 18 | loss: 0.4222  | train_auc: 0.8879  | valid_auc: 0.89384 |  0:00:10s\n",
      "epoch 19 | loss: 0.42103 | train_auc: 0.88829 | valid_auc: 0.8942  |  0:00:11s\n",
      "epoch 20 | loss: 0.41948 | train_auc: 0.88763 | valid_auc: 0.89349 |  0:00:11s\n",
      "epoch 21 | loss: 0.41889 | train_auc: 0.88764 | valid_auc: 0.89233 |  0:00:12s\n",
      "epoch 22 | loss: 0.41128 | train_auc: 0.88859 | valid_auc: 0.8926  |  0:00:12s\n",
      "epoch 23 | loss: 0.4149  | train_auc: 0.88846 | valid_auc: 0.89463 |  0:00:13s\n",
      "epoch 24 | loss: 0.42096 | train_auc: 0.88813 | valid_auc: 0.89326 |  0:00:13s\n",
      "epoch 25 | loss: 0.41361 | train_auc: 0.88863 | valid_auc: 0.89321 |  0:00:14s\n",
      "epoch 26 | loss: 0.41445 | train_auc: 0.88874 | valid_auc: 0.89276 |  0:00:14s\n",
      "epoch 27 | loss: 0.4205  | train_auc: 0.88893 | valid_auc: 0.89446 |  0:00:15s\n",
      "epoch 28 | loss: 0.41952 | train_auc: 0.88888 | valid_auc: 0.89313 |  0:00:16s\n",
      "epoch 29 | loss: 0.40876 | train_auc: 0.88914 | valid_auc: 0.89285 |  0:00:16s\n",
      "epoch 30 | loss: 0.41732 | train_auc: 0.88921 | valid_auc: 0.8938  |  0:00:17s\n",
      "epoch 31 | loss: 0.41327 | train_auc: 0.88867 | valid_auc: 0.89429 |  0:00:17s\n",
      "epoch 32 | loss: 0.41931 | train_auc: 0.88851 | valid_auc: 0.89342 |  0:00:18s\n",
      "epoch 33 | loss: 0.41297 | train_auc: 0.88845 | valid_auc: 0.89223 |  0:00:18s\n",
      "epoch 34 | loss: 0.40722 | train_auc: 0.88899 | valid_auc: 0.89392 |  0:00:19s\n",
      "epoch 35 | loss: 0.42603 | train_auc: 0.88894 | valid_auc: 0.89397 |  0:00:19s\n",
      "epoch 36 | loss: 0.41024 | train_auc: 0.88936 | valid_auc: 0.89457 |  0:00:20s\n",
      "epoch 37 | loss: 0.42005 | train_auc: 0.88935 | valid_auc: 0.89315 |  0:00:20s\n",
      "epoch 38 | loss: 0.41339 | train_auc: 0.88972 | valid_auc: 0.89326 |  0:00:21s\n",
      "epoch 39 | loss: 0.41035 | train_auc: 0.88939 | valid_auc: 0.89405 |  0:00:22s\n",
      "epoch 40 | loss: 0.41704 | train_auc: 0.88919 | valid_auc: 0.89418 |  0:00:22s\n",
      "epoch 41 | loss: 0.42234 | train_auc: 0.88983 | valid_auc: 0.89378 |  0:00:23s\n",
      "epoch 42 | loss: 0.41434 | train_auc: 0.88971 | valid_auc: 0.89383 |  0:00:23s\n",
      "epoch 43 | loss: 0.41574 | train_auc: 0.8894  | valid_auc: 0.89413 |  0:00:24s\n",
      "epoch 44 | loss: 0.41511 | train_auc: 0.8875  | valid_auc: 0.89103 |  0:00:24s\n",
      "epoch 45 | loss: 0.41777 | train_auc: 0.8891  | valid_auc: 0.89337 |  0:00:25s\n",
      "epoch 46 | loss: 0.41236 | train_auc: 0.88937 | valid_auc: 0.894   |  0:00:25s\n",
      "epoch 47 | loss: 0.41987 | train_auc: 0.88847 | valid_auc: 0.89135 |  0:00:26s\n",
      "epoch 48 | loss: 0.40595 | train_auc: 0.88933 | valid_auc: 0.89323 |  0:00:27s\n",
      "epoch 49 | loss: 0.41326 | train_auc: 0.88919 | valid_auc: 0.89291 |  0:00:27s\n",
      "epoch 50 | loss: 0.41818 | train_auc: 0.88965 | valid_auc: 0.89362 |  0:00:28s\n",
      "epoch 51 | loss: 0.41655 | train_auc: 0.89029 | valid_auc: 0.89393 |  0:00:28s\n",
      "epoch 52 | loss: 0.41911 | train_auc: 0.88996 | valid_auc: 0.893   |  0:00:29s\n",
      "epoch 53 | loss: 0.40982 | train_auc: 0.88995 | valid_auc: 0.89265 |  0:00:29s\n",
      "epoch 54 | loss: 0.41068 | train_auc: 0.88973 | valid_auc: 0.89313 |  0:00:30s\n",
      "epoch 55 | loss: 0.42424 | train_auc: 0.88976 | valid_auc: 0.89388 |  0:00:30s\n",
      "epoch 56 | loss: 0.40862 | train_auc: 0.88997 | valid_auc: 0.89332 |  0:00:31s\n",
      "epoch 57 | loss: 0.41798 | train_auc: 0.88972 | valid_auc: 0.8949  |  0:00:32s\n",
      "epoch 58 | loss: 0.41562 | train_auc: 0.88911 | valid_auc: 0.89378 |  0:00:32s\n",
      "epoch 59 | loss: 0.41236 | train_auc: 0.88948 | valid_auc: 0.89412 |  0:00:33s\n",
      "epoch 60 | loss: 0.41243 | train_auc: 0.88915 | valid_auc: 0.89307 |  0:00:33s\n",
      "epoch 61 | loss: 0.42721 | train_auc: 0.88977 | valid_auc: 0.89422 |  0:00:34s\n",
      "epoch 62 | loss: 0.41781 | train_auc: 0.88976 | valid_auc: 0.89374 |  0:00:34s\n",
      "epoch 63 | loss: 0.41867 | train_auc: 0.88913 | valid_auc: 0.89437 |  0:00:35s\n",
      "epoch 64 | loss: 0.41482 | train_auc: 0.88848 | valid_auc: 0.89153 |  0:00:35s\n",
      "epoch 65 | loss: 0.41036 | train_auc: 0.88984 | valid_auc: 0.89418 |  0:00:36s\n",
      "epoch 66 | loss: 0.40545 | train_auc: 0.88998 | valid_auc: 0.89299 |  0:00:36s\n",
      "epoch 67 | loss: 0.41481 | train_auc: 0.88936 | valid_auc: 0.89298 |  0:00:37s\n",
      "epoch 68 | loss: 0.40109 | train_auc: 0.88998 | valid_auc: 0.89473 |  0:00:38s\n",
      "epoch 69 | loss: 0.4157  | train_auc: 0.88937 | valid_auc: 0.89483 |  0:00:38s\n",
      "epoch 70 | loss: 0.41158 | train_auc: 0.8899  | valid_auc: 0.89417 |  0:00:39s\n",
      "epoch 71 | loss: 0.40633 | train_auc: 0.88994 | valid_auc: 0.89452 |  0:00:39s\n",
      "epoch 72 | loss: 0.40799 | train_auc: 0.8901  | valid_auc: 0.89478 |  0:00:40s\n",
      "epoch 73 | loss: 0.4124  | train_auc: 0.89002 | valid_auc: 0.89519 |  0:00:40s\n",
      "epoch 74 | loss: 0.41789 | train_auc: 0.89002 | valid_auc: 0.89393 |  0:00:41s\n",
      "epoch 75 | loss: 0.42066 | train_auc: 0.89037 | valid_auc: 0.89468 |  0:00:41s\n",
      "epoch 76 | loss: 0.41362 | train_auc: 0.89014 | valid_auc: 0.89481 |  0:00:42s\n",
      "epoch 77 | loss: 0.4111  | train_auc: 0.89072 | valid_auc: 0.89439 |  0:00:42s\n",
      "epoch 78 | loss: 0.41041 | train_auc: 0.89012 | valid_auc: 0.89385 |  0:00:43s\n",
      "epoch 79 | loss: 0.42134 | train_auc: 0.88933 | valid_auc: 0.89473 |  0:00:44s\n",
      "epoch 80 | loss: 0.41943 | train_auc: 0.89035 | valid_auc: 0.89518 |  0:00:44s\n",
      "epoch 81 | loss: 0.4176  | train_auc: 0.88955 | valid_auc: 0.89287 |  0:00:45s\n",
      "epoch 82 | loss: 0.41712 | train_auc: 0.89012 | valid_auc: 0.89396 |  0:00:45s\n",
      "epoch 83 | loss: 0.411   | train_auc: 0.8897  | valid_auc: 0.89475 |  0:00:46s\n",
      "epoch 84 | loss: 0.41669 | train_auc: 0.89028 | valid_auc: 0.89475 |  0:00:46s\n",
      "epoch 85 | loss: 0.41715 | train_auc: 0.88904 | valid_auc: 0.89353 |  0:00:47s\n",
      "epoch 86 | loss: 0.41671 | train_auc: 0.88982 | valid_auc: 0.8937  |  0:00:47s\n",
      "epoch 87 | loss: 0.4087  | train_auc: 0.88964 | valid_auc: 0.89416 |  0:00:48s\n",
      "epoch 88 | loss: 0.41329 | train_auc: 0.89014 | valid_auc: 0.89532 |  0:00:48s\n",
      "epoch 89 | loss: 0.41803 | train_auc: 0.88996 | valid_auc: 0.89359 |  0:00:49s\n",
      "epoch 90 | loss: 0.41015 | train_auc: 0.89022 | valid_auc: 0.8942  |  0:00:50s\n",
      "epoch 91 | loss: 0.41883 | train_auc: 0.88991 | valid_auc: 0.8932  |  0:00:50s\n",
      "epoch 92 | loss: 0.41259 | train_auc: 0.89016 | valid_auc: 0.89168 |  0:00:51s\n",
      "epoch 93 | loss: 0.41067 | train_auc: 0.88974 | valid_auc: 0.89207 |  0:00:51s\n",
      "epoch 94 | loss: 0.40909 | train_auc: 0.89011 | valid_auc: 0.89296 |  0:00:52s\n",
      "epoch 95 | loss: 0.42354 | train_auc: 0.88989 | valid_auc: 0.89352 |  0:00:52s\n",
      "epoch 96 | loss: 0.4167  | train_auc: 0.88989 | valid_auc: 0.89461 |  0:00:53s\n",
      "epoch 97 | loss: 0.40985 | train_auc: 0.88979 | valid_auc: 0.89253 |  0:00:53s\n",
      "epoch 98 | loss: 0.41475 | train_auc: 0.8901  | valid_auc: 0.89394 |  0:00:54s\n",
      "epoch 99 | loss: 0.41263 | train_auc: 0.89035 | valid_auc: 0.89289 |  0:00:54s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 88 and best_valid_auc = 0.89532\n",
      "sampling:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.694   | train_auc: 0.68727 | valid_auc: 0.66727 |  0:00:00s\n",
      "epoch 1  | loss: 0.47163 | train_auc: 0.82752 | valid_auc: 0.8257  |  0:00:01s\n",
      "epoch 2  | loss: 0.45073 | train_auc: 0.83841 | valid_auc: 0.83935 |  0:00:01s\n",
      "epoch 3  | loss: 0.43685 | train_auc: 0.8606  | valid_auc: 0.8631  |  0:00:02s\n",
      "epoch 4  | loss: 0.43015 | train_auc: 0.86253 | valid_auc: 0.86418 |  0:00:02s\n",
      "epoch 5  | loss: 0.42853 | train_auc: 0.87372 | valid_auc: 0.87124 |  0:00:03s\n",
      "epoch 6  | loss: 0.42493 | train_auc: 0.87728 | valid_auc: 0.87605 |  0:00:03s\n",
      "epoch 7  | loss: 0.4253  | train_auc: 0.87331 | valid_auc: 0.87615 |  0:00:04s\n",
      "epoch 8  | loss: 0.42645 | train_auc: 0.8778  | valid_auc: 0.87661 |  0:00:04s\n",
      "epoch 9  | loss: 0.41454 | train_auc: 0.88469 | valid_auc: 0.883   |  0:00:05s\n",
      "epoch 10 | loss: 0.42115 | train_auc: 0.88302 | valid_auc: 0.88053 |  0:00:06s\n",
      "epoch 11 | loss: 0.42164 | train_auc: 0.88663 | valid_auc: 0.88466 |  0:00:06s\n",
      "epoch 12 | loss: 0.42151 | train_auc: 0.88792 | valid_auc: 0.88669 |  0:00:07s\n",
      "epoch 13 | loss: 0.42795 | train_auc: 0.88738 | valid_auc: 0.88542 |  0:00:07s\n",
      "epoch 14 | loss: 0.41574 | train_auc: 0.88889 | valid_auc: 0.88731 |  0:00:08s\n",
      "epoch 15 | loss: 0.41184 | train_auc: 0.89002 | valid_auc: 0.88754 |  0:00:08s\n",
      "epoch 16 | loss: 0.41459 | train_auc: 0.89047 | valid_auc: 0.8878  |  0:00:09s\n",
      "epoch 17 | loss: 0.41944 | train_auc: 0.88986 | valid_auc: 0.88772 |  0:00:10s\n",
      "epoch 18 | loss: 0.41347 | train_auc: 0.88917 | valid_auc: 0.88673 |  0:00:10s\n",
      "epoch 19 | loss: 0.41836 | train_auc: 0.88945 | valid_auc: 0.88737 |  0:00:11s\n",
      "epoch 20 | loss: 0.40809 | train_auc: 0.89022 | valid_auc: 0.88826 |  0:00:11s\n",
      "epoch 21 | loss: 0.41743 | train_auc: 0.89022 | valid_auc: 0.88843 |  0:00:12s\n",
      "epoch 22 | loss: 0.41474 | train_auc: 0.89061 | valid_auc: 0.88881 |  0:00:12s\n",
      "epoch 23 | loss: 0.41491 | train_auc: 0.89122 | valid_auc: 0.88846 |  0:00:13s\n",
      "epoch 24 | loss: 0.40902 | train_auc: 0.88931 | valid_auc: 0.88686 |  0:00:13s\n",
      "epoch 25 | loss: 0.41262 | train_auc: 0.89087 | valid_auc: 0.88759 |  0:00:14s\n",
      "epoch 26 | loss: 0.40526 | train_auc: 0.89088 | valid_auc: 0.88786 |  0:00:14s\n",
      "epoch 27 | loss: 0.41809 | train_auc: 0.89048 | valid_auc: 0.88788 |  0:00:15s\n",
      "epoch 28 | loss: 0.41502 | train_auc: 0.89027 | valid_auc: 0.88639 |  0:00:15s\n",
      "epoch 29 | loss: 0.42679 | train_auc: 0.89118 | valid_auc: 0.88835 |  0:00:16s\n",
      "epoch 30 | loss: 0.41136 | train_auc: 0.89091 | valid_auc: 0.88893 |  0:00:17s\n",
      "epoch 31 | loss: 0.42059 | train_auc: 0.8909  | valid_auc: 0.88826 |  0:00:17s\n",
      "epoch 32 | loss: 0.41516 | train_auc: 0.89142 | valid_auc: 0.88848 |  0:00:18s\n",
      "epoch 33 | loss: 0.41068 | train_auc: 0.89138 | valid_auc: 0.88926 |  0:00:18s\n",
      "epoch 34 | loss: 0.41531 | train_auc: 0.89137 | valid_auc: 0.88889 |  0:00:19s\n",
      "epoch 35 | loss: 0.41863 | train_auc: 0.89154 | valid_auc: 0.88956 |  0:00:19s\n",
      "epoch 36 | loss: 0.41501 | train_auc: 0.89078 | valid_auc: 0.88639 |  0:00:20s\n",
      "epoch 37 | loss: 0.41729 | train_auc: 0.89086 | valid_auc: 0.88964 |  0:00:20s\n",
      "epoch 38 | loss: 0.41347 | train_auc: 0.89144 | valid_auc: 0.88847 |  0:00:21s\n",
      "epoch 39 | loss: 0.40919 | train_auc: 0.89155 | valid_auc: 0.88713 |  0:00:22s\n",
      "epoch 40 | loss: 0.40677 | train_auc: 0.8911  | valid_auc: 0.88849 |  0:00:22s\n",
      "epoch 41 | loss: 0.40629 | train_auc: 0.89226 | valid_auc: 0.88807 |  0:00:23s\n",
      "epoch 42 | loss: 0.41929 | train_auc: 0.89185 | valid_auc: 0.88711 |  0:00:23s\n",
      "epoch 43 | loss: 0.40668 | train_auc: 0.8913  | valid_auc: 0.88677 |  0:00:24s\n",
      "epoch 44 | loss: 0.41639 | train_auc: 0.89111 | valid_auc: 0.88739 |  0:00:24s\n",
      "epoch 45 | loss: 0.41349 | train_auc: 0.89148 | valid_auc: 0.88856 |  0:00:25s\n",
      "epoch 46 | loss: 0.40807 | train_auc: 0.89203 | valid_auc: 0.88852 |  0:00:25s\n",
      "epoch 47 | loss: 0.41833 | train_auc: 0.89156 | valid_auc: 0.88837 |  0:00:26s\n",
      "epoch 48 | loss: 0.4135  | train_auc: 0.89243 | valid_auc: 0.88883 |  0:00:27s\n",
      "epoch 49 | loss: 0.41439 | train_auc: 0.89169 | valid_auc: 0.88708 |  0:00:27s\n",
      "epoch 50 | loss: 0.41652 | train_auc: 0.89191 | valid_auc: 0.88859 |  0:00:28s\n",
      "epoch 51 | loss: 0.40064 | train_auc: 0.89215 | valid_auc: 0.88713 |  0:00:28s\n",
      "epoch 52 | loss: 0.41519 | train_auc: 0.8909  | valid_auc: 0.88601 |  0:00:29s\n",
      "epoch 53 | loss: 0.41575 | train_auc: 0.89178 | valid_auc: 0.88844 |  0:00:29s\n",
      "epoch 54 | loss: 0.41091 | train_auc: 0.89267 | valid_auc: 0.88839 |  0:00:30s\n",
      "epoch 55 | loss: 0.4154  | train_auc: 0.89223 | valid_auc: 0.8891  |  0:00:30s\n",
      "epoch 56 | loss: 0.41394 | train_auc: 0.89233 | valid_auc: 0.88795 |  0:00:31s\n",
      "epoch 57 | loss: 0.4004  | train_auc: 0.89195 | valid_auc: 0.88914 |  0:00:32s\n",
      "epoch 58 | loss: 0.41048 | train_auc: 0.89135 | valid_auc: 0.88844 |  0:00:32s\n",
      "epoch 59 | loss: 0.42053 | train_auc: 0.89118 | valid_auc: 0.88747 |  0:00:33s\n",
      "epoch 60 | loss: 0.41504 | train_auc: 0.89113 | valid_auc: 0.88815 |  0:00:33s\n",
      "epoch 61 | loss: 0.41122 | train_auc: 0.89106 | valid_auc: 0.88661 |  0:00:34s\n",
      "epoch 62 | loss: 0.41095 | train_auc: 0.89134 | valid_auc: 0.88809 |  0:00:34s\n",
      "epoch 63 | loss: 0.42086 | train_auc: 0.8911  | valid_auc: 0.88886 |  0:00:35s\n",
      "epoch 64 | loss: 0.41318 | train_auc: 0.89171 | valid_auc: 0.88721 |  0:00:35s\n",
      "epoch 65 | loss: 0.41394 | train_auc: 0.89254 | valid_auc: 0.88918 |  0:00:36s\n",
      "epoch 66 | loss: 0.42556 | train_auc: 0.89194 | valid_auc: 0.88923 |  0:00:36s\n",
      "epoch 67 | loss: 0.41772 | train_auc: 0.89204 | valid_auc: 0.88789 |  0:00:37s\n",
      "epoch 68 | loss: 0.40602 | train_auc: 0.89213 | valid_auc: 0.88768 |  0:00:38s\n",
      "epoch 69 | loss: 0.41054 | train_auc: 0.89198 | valid_auc: 0.88876 |  0:00:38s\n",
      "epoch 70 | loss: 0.40599 | train_auc: 0.89157 | valid_auc: 0.88829 |  0:00:39s\n",
      "epoch 71 | loss: 0.41848 | train_auc: 0.89258 | valid_auc: 0.88795 |  0:00:39s\n",
      "epoch 72 | loss: 0.41392 | train_auc: 0.89268 | valid_auc: 0.8895  |  0:00:40s\n",
      "epoch 73 | loss: 0.41225 | train_auc: 0.89236 | valid_auc: 0.88919 |  0:00:40s\n",
      "epoch 74 | loss: 0.4135  | train_auc: 0.89276 | valid_auc: 0.8893  |  0:00:41s\n",
      "epoch 75 | loss: 0.41614 | train_auc: 0.8919  | valid_auc: 0.88824 |  0:00:41s\n",
      "epoch 76 | loss: 0.41018 | train_auc: 0.89258 | valid_auc: 0.88901 |  0:00:42s\n",
      "epoch 77 | loss: 0.40416 | train_auc: 0.89232 | valid_auc: 0.88806 |  0:00:43s\n",
      "epoch 78 | loss: 0.41565 | train_auc: 0.89248 | valid_auc: 0.88955 |  0:00:43s\n",
      "epoch 79 | loss: 0.41464 | train_auc: 0.89252 | valid_auc: 0.88831 |  0:00:44s\n",
      "epoch 80 | loss: 0.41027 | train_auc: 0.89258 | valid_auc: 0.88819 |  0:00:44s\n",
      "epoch 81 | loss: 0.41052 | train_auc: 0.89248 | valid_auc: 0.8887  |  0:00:45s\n",
      "epoch 82 | loss: 0.40845 | train_auc: 0.89168 | valid_auc: 0.88792 |  0:00:45s\n",
      "epoch 83 | loss: 0.41652 | train_auc: 0.8923  | valid_auc: 0.88818 |  0:00:46s\n",
      "epoch 84 | loss: 0.41044 | train_auc: 0.89214 | valid_auc: 0.88754 |  0:00:46s\n",
      "epoch 85 | loss: 0.40973 | train_auc: 0.89262 | valid_auc: 0.88891 |  0:00:47s\n",
      "epoch 86 | loss: 0.40389 | train_auc: 0.89208 | valid_auc: 0.88788 |  0:00:48s\n",
      "epoch 87 | loss: 0.40743 | train_auc: 0.89277 | valid_auc: 0.88912 |  0:00:48s\n",
      "\n",
      "Early stopping occurred at epoch 87 with best_epoch = 37 and best_valid_auc = 0.88964\n",
      "sampling:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.70154 | train_auc: 0.61814 | valid_auc: 0.60787 |  0:00:00s\n",
      "epoch 1  | loss: 0.48237 | train_auc: 0.81979 | valid_auc: 0.82177 |  0:00:01s\n",
      "epoch 2  | loss: 0.44074 | train_auc: 0.86191 | valid_auc: 0.8658  |  0:00:01s\n",
      "epoch 3  | loss: 0.43562 | train_auc: 0.82516 | valid_auc: 0.8252  |  0:00:02s\n",
      "epoch 4  | loss: 0.42249 | train_auc: 0.84859 | valid_auc: 0.85098 |  0:00:02s\n",
      "epoch 5  | loss: 0.42302 | train_auc: 0.86144 | valid_auc: 0.86276 |  0:00:03s\n",
      "epoch 6  | loss: 0.42093 | train_auc: 0.87302 | valid_auc: 0.87788 |  0:00:03s\n",
      "epoch 7  | loss: 0.42189 | train_auc: 0.87848 | valid_auc: 0.88328 |  0:00:04s\n",
      "epoch 8  | loss: 0.42298 | train_auc: 0.88055 | valid_auc: 0.88495 |  0:00:04s\n",
      "epoch 9  | loss: 0.40809 | train_auc: 0.88073 | valid_auc: 0.88517 |  0:00:05s\n",
      "epoch 10 | loss: 0.41775 | train_auc: 0.88519 | valid_auc: 0.88906 |  0:00:06s\n",
      "epoch 11 | loss: 0.42146 | train_auc: 0.8857  | valid_auc: 0.88912 |  0:00:06s\n",
      "epoch 12 | loss: 0.42435 | train_auc: 0.88605 | valid_auc: 0.88988 |  0:00:07s\n",
      "epoch 13 | loss: 0.41598 | train_auc: 0.88783 | valid_auc: 0.89148 |  0:00:07s\n",
      "epoch 14 | loss: 0.41299 | train_auc: 0.88676 | valid_auc: 0.89117 |  0:00:08s\n",
      "epoch 15 | loss: 0.42102 | train_auc: 0.88876 | valid_auc: 0.89207 |  0:00:08s\n",
      "epoch 16 | loss: 0.4212  | train_auc: 0.88888 | valid_auc: 0.89217 |  0:00:09s\n",
      "epoch 17 | loss: 0.42651 | train_auc: 0.8876  | valid_auc: 0.89203 |  0:00:09s\n",
      "epoch 18 | loss: 0.42223 | train_auc: 0.88883 | valid_auc: 0.89177 |  0:00:10s\n",
      "epoch 19 | loss: 0.40833 | train_auc: 0.88847 | valid_auc: 0.89159 |  0:00:11s\n",
      "epoch 20 | loss: 0.41871 | train_auc: 0.88918 | valid_auc: 0.89272 |  0:00:11s\n",
      "epoch 21 | loss: 0.41761 | train_auc: 0.88797 | valid_auc: 0.89183 |  0:00:12s\n",
      "epoch 22 | loss: 0.41457 | train_auc: 0.88909 | valid_auc: 0.89281 |  0:00:12s\n",
      "epoch 23 | loss: 0.41389 | train_auc: 0.88873 | valid_auc: 0.89256 |  0:00:13s\n",
      "epoch 24 | loss: 0.42641 | train_auc: 0.8895  | valid_auc: 0.89251 |  0:00:13s\n",
      "epoch 25 | loss: 0.40789 | train_auc: 0.88948 | valid_auc: 0.89269 |  0:00:14s\n",
      "epoch 26 | loss: 0.41585 | train_auc: 0.89003 | valid_auc: 0.89339 |  0:00:14s\n",
      "epoch 27 | loss: 0.4146  | train_auc: 0.89006 | valid_auc: 0.89306 |  0:00:15s\n",
      "epoch 28 | loss: 0.42431 | train_auc: 0.88987 | valid_auc: 0.89325 |  0:00:16s\n",
      "epoch 29 | loss: 0.41648 | train_auc: 0.88996 | valid_auc: 0.8926  |  0:00:16s\n",
      "epoch 30 | loss: 0.41599 | train_auc: 0.88957 | valid_auc: 0.89309 |  0:00:17s\n",
      "epoch 31 | loss: 0.4194  | train_auc: 0.89015 | valid_auc: 0.89277 |  0:00:17s\n",
      "epoch 32 | loss: 0.41815 | train_auc: 0.89026 | valid_auc: 0.89291 |  0:00:18s\n",
      "epoch 33 | loss: 0.41703 | train_auc: 0.88938 | valid_auc: 0.89305 |  0:00:18s\n",
      "epoch 34 | loss: 0.41878 | train_auc: 0.88966 | valid_auc: 0.89392 |  0:00:19s\n",
      "epoch 35 | loss: 0.4125  | train_auc: 0.88969 | valid_auc: 0.89411 |  0:00:19s\n",
      "epoch 36 | loss: 0.41153 | train_auc: 0.88964 | valid_auc: 0.89299 |  0:00:20s\n",
      "epoch 37 | loss: 0.41539 | train_auc: 0.8898  | valid_auc: 0.89372 |  0:00:20s\n",
      "epoch 38 | loss: 0.41908 | train_auc: 0.89043 | valid_auc: 0.89377 |  0:00:21s\n",
      "epoch 39 | loss: 0.41462 | train_auc: 0.89018 | valid_auc: 0.8929  |  0:00:22s\n",
      "epoch 40 | loss: 0.41601 | train_auc: 0.8903  | valid_auc: 0.89346 |  0:00:22s\n",
      "epoch 41 | loss: 0.40283 | train_auc: 0.89046 | valid_auc: 0.89319 |  0:00:23s\n",
      "epoch 42 | loss: 0.4066  | train_auc: 0.89035 | valid_auc: 0.89366 |  0:00:23s\n",
      "epoch 43 | loss: 0.41394 | train_auc: 0.89036 | valid_auc: 0.89296 |  0:00:24s\n",
      "epoch 44 | loss: 0.42032 | train_auc: 0.88997 | valid_auc: 0.89278 |  0:00:24s\n",
      "epoch 45 | loss: 0.41597 | train_auc: 0.89067 | valid_auc: 0.89302 |  0:00:25s\n",
      "epoch 46 | loss: 0.41919 | train_auc: 0.89055 | valid_auc: 0.89405 |  0:00:25s\n",
      "epoch 47 | loss: 0.41418 | train_auc: 0.8901  | valid_auc: 0.89338 |  0:00:26s\n",
      "epoch 48 | loss: 0.41239 | train_auc: 0.89036 | valid_auc: 0.89369 |  0:00:27s\n",
      "epoch 49 | loss: 0.41121 | train_auc: 0.89067 | valid_auc: 0.8935  |  0:00:27s\n",
      "epoch 50 | loss: 0.42033 | train_auc: 0.89048 | valid_auc: 0.89366 |  0:00:28s\n",
      "epoch 51 | loss: 0.41073 | train_auc: 0.8907  | valid_auc: 0.89407 |  0:00:28s\n",
      "epoch 52 | loss: 0.40999 | train_auc: 0.89007 | valid_auc: 0.89326 |  0:00:29s\n",
      "epoch 53 | loss: 0.42436 | train_auc: 0.89004 | valid_auc: 0.89443 |  0:00:29s\n",
      "epoch 54 | loss: 0.41656 | train_auc: 0.89018 | valid_auc: 0.89367 |  0:00:30s\n",
      "epoch 55 | loss: 0.41134 | train_auc: 0.89048 | valid_auc: 0.8934  |  0:00:30s\n",
      "epoch 56 | loss: 0.41901 | train_auc: 0.88999 | valid_auc: 0.89347 |  0:00:31s\n",
      "epoch 57 | loss: 0.41924 | train_auc: 0.88992 | valid_auc: 0.89349 |  0:00:32s\n",
      "epoch 58 | loss: 0.41725 | train_auc: 0.88989 | valid_auc: 0.89326 |  0:00:32s\n",
      "epoch 59 | loss: 0.41296 | train_auc: 0.89026 | valid_auc: 0.89468 |  0:00:33s\n",
      "epoch 60 | loss: 0.42104 | train_auc: 0.89035 | valid_auc: 0.89365 |  0:00:33s\n",
      "epoch 61 | loss: 0.41257 | train_auc: 0.88962 | valid_auc: 0.89208 |  0:00:34s\n",
      "epoch 62 | loss: 0.4205  | train_auc: 0.89012 | valid_auc: 0.89393 |  0:00:34s\n",
      "epoch 63 | loss: 0.41356 | train_auc: 0.89016 | valid_auc: 0.8947  |  0:00:35s\n",
      "epoch 64 | loss: 0.40784 | train_auc: 0.89044 | valid_auc: 0.89382 |  0:00:35s\n",
      "epoch 65 | loss: 0.41346 | train_auc: 0.89047 | valid_auc: 0.89353 |  0:00:36s\n",
      "epoch 66 | loss: 0.41415 | train_auc: 0.8903  | valid_auc: 0.89388 |  0:00:37s\n",
      "epoch 67 | loss: 0.40981 | train_auc: 0.89017 | valid_auc: 0.894   |  0:00:37s\n",
      "epoch 68 | loss: 0.40728 | train_auc: 0.89033 | valid_auc: 0.89305 |  0:00:38s\n",
      "epoch 69 | loss: 0.41611 | train_auc: 0.88935 | valid_auc: 0.8931  |  0:00:38s\n",
      "epoch 70 | loss: 0.41723 | train_auc: 0.89006 | valid_auc: 0.89326 |  0:00:39s\n",
      "epoch 71 | loss: 0.40932 | train_auc: 0.89016 | valid_auc: 0.89248 |  0:00:39s\n",
      "epoch 72 | loss: 0.4212  | train_auc: 0.89035 | valid_auc: 0.89278 |  0:00:40s\n",
      "epoch 73 | loss: 0.41401 | train_auc: 0.89063 | valid_auc: 0.89315 |  0:00:40s\n",
      "epoch 74 | loss: 0.40465 | train_auc: 0.89064 | valid_auc: 0.89354 |  0:00:41s\n",
      "epoch 75 | loss: 0.40994 | train_auc: 0.89071 | valid_auc: 0.89356 |  0:00:41s\n",
      "epoch 76 | loss: 0.41599 | train_auc: 0.88963 | valid_auc: 0.89193 |  0:00:42s\n",
      "epoch 77 | loss: 0.40922 | train_auc: 0.89071 | valid_auc: 0.89216 |  0:00:43s\n",
      "epoch 78 | loss: 0.41832 | train_auc: 0.89086 | valid_auc: 0.89322 |  0:00:43s\n",
      "epoch 79 | loss: 0.41637 | train_auc: 0.89062 | valid_auc: 0.893   |  0:00:44s\n",
      "epoch 80 | loss: 0.42162 | train_auc: 0.89056 | valid_auc: 0.89349 |  0:00:44s\n",
      "epoch 81 | loss: 0.4106  | train_auc: 0.891   | valid_auc: 0.89294 |  0:00:45s\n",
      "epoch 82 | loss: 0.4177  | train_auc: 0.89065 | valid_auc: 0.8928  |  0:00:45s\n",
      "epoch 83 | loss: 0.41195 | train_auc: 0.89023 | valid_auc: 0.89319 |  0:00:46s\n",
      "epoch 84 | loss: 0.40621 | train_auc: 0.89078 | valid_auc: 0.8935  |  0:00:46s\n",
      "epoch 85 | loss: 0.40404 | train_auc: 0.89035 | valid_auc: 0.89328 |  0:00:47s\n",
      "epoch 86 | loss: 0.40876 | train_auc: 0.89059 | valid_auc: 0.89343 |  0:00:48s\n",
      "epoch 87 | loss: 0.41667 | train_auc: 0.8908  | valid_auc: 0.89286 |  0:00:48s\n",
      "epoch 88 | loss: 0.41917 | train_auc: 0.89038 | valid_auc: 0.89364 |  0:00:49s\n",
      "epoch 89 | loss: 0.41775 | train_auc: 0.88966 | valid_auc: 0.892   |  0:00:49s\n",
      "epoch 90 | loss: 0.41334 | train_auc: 0.88989 | valid_auc: 0.89328 |  0:00:50s\n",
      "epoch 91 | loss: 0.40368 | train_auc: 0.88957 | valid_auc: 0.89259 |  0:00:50s\n",
      "epoch 92 | loss: 0.41531 | train_auc: 0.88975 | valid_auc: 0.89232 |  0:00:51s\n",
      "epoch 93 | loss: 0.421   | train_auc: 0.88901 | valid_auc: 0.89262 |  0:00:51s\n",
      "epoch 94 | loss: 0.41903 | train_auc: 0.8894  | valid_auc: 0.89228 |  0:00:52s\n",
      "epoch 95 | loss: 0.41635 | train_auc: 0.89001 | valid_auc: 0.8931  |  0:00:53s\n",
      "epoch 96 | loss: 0.4103  | train_auc: 0.8898  | valid_auc: 0.89392 |  0:00:53s\n",
      "epoch 97 | loss: 0.40841 | train_auc: 0.88954 | valid_auc: 0.8934  |  0:00:54s\n",
      "epoch 98 | loss: 0.41995 | train_auc: 0.88988 | valid_auc: 0.89373 |  0:00:54s\n",
      "epoch 99 | loss: 0.41109 | train_auc: 0.88989 | valid_auc: 0.89235 |  0:00:55s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 63 and best_valid_auc = 0.8947\n",
      "sampling:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69867 | train_auc: 0.67839 | valid_auc: 0.6704  |  0:00:00s\n",
      "epoch 1  | loss: 0.48597 | train_auc: 0.81083 | valid_auc: 0.81124 |  0:00:01s\n",
      "epoch 2  | loss: 0.43274 | train_auc: 0.84455 | valid_auc: 0.84382 |  0:00:01s\n",
      "epoch 3  | loss: 0.43607 | train_auc: 0.85189 | valid_auc: 0.85621 |  0:00:02s\n",
      "epoch 4  | loss: 0.4323  | train_auc: 0.86892 | valid_auc: 0.86988 |  0:00:02s\n",
      "epoch 5  | loss: 0.41901 | train_auc: 0.87239 | valid_auc: 0.87269 |  0:00:03s\n",
      "epoch 6  | loss: 0.42195 | train_auc: 0.87863 | valid_auc: 0.87963 |  0:00:03s\n",
      "epoch 7  | loss: 0.41966 | train_auc: 0.87853 | valid_auc: 0.87928 |  0:00:04s\n",
      "epoch 8  | loss: 0.41764 | train_auc: 0.88129 | valid_auc: 0.87932 |  0:00:04s\n",
      "epoch 9  | loss: 0.42011 | train_auc: 0.88134 | valid_auc: 0.87955 |  0:00:05s\n",
      "epoch 10 | loss: 0.40845 | train_auc: 0.8839  | valid_auc: 0.88249 |  0:00:06s\n",
      "epoch 11 | loss: 0.41881 | train_auc: 0.88598 | valid_auc: 0.88322 |  0:00:06s\n",
      "epoch 12 | loss: 0.40676 | train_auc: 0.88792 | valid_auc: 0.88531 |  0:00:07s\n",
      "epoch 13 | loss: 0.41912 | train_auc: 0.8867  | valid_auc: 0.88388 |  0:00:07s\n",
      "epoch 14 | loss: 0.42648 | train_auc: 0.88848 | valid_auc: 0.88574 |  0:00:08s\n",
      "epoch 15 | loss: 0.42025 | train_auc: 0.88947 | valid_auc: 0.88597 |  0:00:08s\n",
      "epoch 16 | loss: 0.42251 | train_auc: 0.88763 | valid_auc: 0.88492 |  0:00:09s\n",
      "epoch 17 | loss: 0.41709 | train_auc: 0.891   | valid_auc: 0.88611 |  0:00:09s\n",
      "epoch 18 | loss: 0.41138 | train_auc: 0.89069 | valid_auc: 0.88741 |  0:00:10s\n",
      "epoch 19 | loss: 0.41065 | train_auc: 0.89016 | valid_auc: 0.88695 |  0:00:11s\n",
      "epoch 20 | loss: 0.41422 | train_auc: 0.89155 | valid_auc: 0.88725 |  0:00:11s\n",
      "epoch 21 | loss: 0.41545 | train_auc: 0.89129 | valid_auc: 0.88745 |  0:00:12s\n",
      "epoch 22 | loss: 0.41899 | train_auc: 0.89092 | valid_auc: 0.88664 |  0:00:12s\n",
      "epoch 23 | loss: 0.41338 | train_auc: 0.89169 | valid_auc: 0.88767 |  0:00:13s\n",
      "epoch 24 | loss: 0.41308 | train_auc: 0.89153 | valid_auc: 0.88698 |  0:00:13s\n",
      "epoch 25 | loss: 0.41092 | train_auc: 0.89138 | valid_auc: 0.88697 |  0:00:14s\n",
      "epoch 26 | loss: 0.40545 | train_auc: 0.89182 | valid_auc: 0.88728 |  0:00:14s\n",
      "epoch 27 | loss: 0.41423 | train_auc: 0.89148 | valid_auc: 0.88684 |  0:00:15s\n",
      "epoch 28 | loss: 0.41355 | train_auc: 0.89175 | valid_auc: 0.88694 |  0:00:16s\n",
      "epoch 29 | loss: 0.42299 | train_auc: 0.89186 | valid_auc: 0.88787 |  0:00:16s\n",
      "epoch 30 | loss: 0.41339 | train_auc: 0.89177 | valid_auc: 0.8869  |  0:00:17s\n",
      "epoch 31 | loss: 0.4113  | train_auc: 0.89201 | valid_auc: 0.88874 |  0:00:17s\n",
      "epoch 32 | loss: 0.41844 | train_auc: 0.89192 | valid_auc: 0.88717 |  0:00:18s\n",
      "epoch 33 | loss: 0.41817 | train_auc: 0.89245 | valid_auc: 0.88792 |  0:00:18s\n",
      "epoch 34 | loss: 0.41665 | train_auc: 0.89214 | valid_auc: 0.88814 |  0:00:19s\n",
      "epoch 35 | loss: 0.41775 | train_auc: 0.89205 | valid_auc: 0.88872 |  0:00:19s\n",
      "epoch 36 | loss: 0.41435 | train_auc: 0.8921  | valid_auc: 0.88652 |  0:00:20s\n",
      "epoch 37 | loss: 0.41998 | train_auc: 0.89231 | valid_auc: 0.88753 |  0:00:21s\n",
      "epoch 38 | loss: 0.40886 | train_auc: 0.89225 | valid_auc: 0.88712 |  0:00:21s\n",
      "epoch 39 | loss: 0.41381 | train_auc: 0.89112 | valid_auc: 0.88631 |  0:00:22s\n",
      "epoch 40 | loss: 0.41188 | train_auc: 0.89231 | valid_auc: 0.88703 |  0:00:22s\n",
      "epoch 41 | loss: 0.41419 | train_auc: 0.89252 | valid_auc: 0.88682 |  0:00:23s\n",
      "epoch 42 | loss: 0.41058 | train_auc: 0.89225 | valid_auc: 0.88738 |  0:00:23s\n",
      "epoch 43 | loss: 0.41876 | train_auc: 0.89197 | valid_auc: 0.88714 |  0:00:24s\n",
      "epoch 44 | loss: 0.41114 | train_auc: 0.89234 | valid_auc: 0.88647 |  0:00:24s\n",
      "epoch 45 | loss: 0.40987 | train_auc: 0.89248 | valid_auc: 0.8851  |  0:00:25s\n",
      "epoch 46 | loss: 0.41203 | train_auc: 0.89275 | valid_auc: 0.88751 |  0:00:25s\n",
      "epoch 47 | loss: 0.4095  | train_auc: 0.89263 | valid_auc: 0.8867  |  0:00:26s\n",
      "epoch 48 | loss: 0.41134 | train_auc: 0.89211 | valid_auc: 0.88699 |  0:00:27s\n",
      "epoch 49 | loss: 0.40678 | train_auc: 0.89264 | valid_auc: 0.88687 |  0:00:27s\n",
      "epoch 50 | loss: 0.41514 | train_auc: 0.89276 | valid_auc: 0.8879  |  0:00:28s\n",
      "epoch 51 | loss: 0.40464 | train_auc: 0.89279 | valid_auc: 0.88767 |  0:00:28s\n",
      "epoch 52 | loss: 0.40195 | train_auc: 0.89192 | valid_auc: 0.88655 |  0:00:29s\n",
      "epoch 53 | loss: 0.40929 | train_auc: 0.89233 | valid_auc: 0.88645 |  0:00:29s\n",
      "epoch 54 | loss: 0.41708 | train_auc: 0.89218 | valid_auc: 0.88754 |  0:00:30s\n",
      "epoch 55 | loss: 0.40631 | train_auc: 0.89282 | valid_auc: 0.88843 |  0:00:30s\n",
      "epoch 56 | loss: 0.411   | train_auc: 0.8929  | valid_auc: 0.88733 |  0:00:31s\n",
      "epoch 57 | loss: 0.41074 | train_auc: 0.89291 | valid_auc: 0.88767 |  0:00:32s\n",
      "epoch 58 | loss: 0.40846 | train_auc: 0.89303 | valid_auc: 0.88723 |  0:00:32s\n",
      "epoch 59 | loss: 0.40845 | train_auc: 0.8926  | valid_auc: 0.88714 |  0:00:33s\n",
      "epoch 60 | loss: 0.41172 | train_auc: 0.8931  | valid_auc: 0.8878  |  0:00:33s\n",
      "epoch 61 | loss: 0.4084  | train_auc: 0.89282 | valid_auc: 0.88702 |  0:00:34s\n",
      "epoch 62 | loss: 0.41424 | train_auc: 0.89297 | valid_auc: 0.88699 |  0:00:34s\n",
      "epoch 63 | loss: 0.41568 | train_auc: 0.89306 | valid_auc: 0.88764 |  0:00:35s\n",
      "epoch 64 | loss: 0.41838 | train_auc: 0.89295 | valid_auc: 0.88721 |  0:00:35s\n",
      "epoch 65 | loss: 0.40592 | train_auc: 0.89231 | valid_auc: 0.88741 |  0:00:36s\n",
      "epoch 66 | loss: 0.40792 | train_auc: 0.89269 | valid_auc: 0.88736 |  0:00:37s\n",
      "epoch 67 | loss: 0.4166  | train_auc: 0.8926  | valid_auc: 0.88723 |  0:00:37s\n",
      "epoch 68 | loss: 0.4143  | train_auc: 0.89241 | valid_auc: 0.88671 |  0:00:38s\n",
      "epoch 69 | loss: 0.40857 | train_auc: 0.89285 | valid_auc: 0.88577 |  0:00:38s\n",
      "epoch 70 | loss: 0.41749 | train_auc: 0.89272 | valid_auc: 0.88693 |  0:00:39s\n",
      "epoch 71 | loss: 0.40679 | train_auc: 0.8926  | valid_auc: 0.8854  |  0:00:39s\n",
      "epoch 72 | loss: 0.40791 | train_auc: 0.89271 | valid_auc: 0.8874  |  0:00:40s\n",
      "epoch 73 | loss: 0.40737 | train_auc: 0.89283 | valid_auc: 0.88673 |  0:00:40s\n",
      "epoch 74 | loss: 0.40135 | train_auc: 0.89323 | valid_auc: 0.88685 |  0:00:41s\n",
      "epoch 75 | loss: 0.40903 | train_auc: 0.89268 | valid_auc: 0.88668 |  0:00:41s\n",
      "epoch 76 | loss: 0.41561 | train_auc: 0.89293 | valid_auc: 0.88707 |  0:00:42s\n",
      "epoch 77 | loss: 0.40827 | train_auc: 0.89279 | valid_auc: 0.88707 |  0:00:43s\n",
      "epoch 78 | loss: 0.4126  | train_auc: 0.89286 | valid_auc: 0.88651 |  0:00:43s\n",
      "epoch 79 | loss: 0.40825 | train_auc: 0.89285 | valid_auc: 0.88619 |  0:00:44s\n",
      "epoch 80 | loss: 0.41247 | train_auc: 0.89287 | valid_auc: 0.88735 |  0:00:44s\n",
      "epoch 81 | loss: 0.4125  | train_auc: 0.89314 | valid_auc: 0.88684 |  0:00:45s\n",
      "\n",
      "Early stopping occurred at epoch 81 with best_epoch = 31 and best_valid_auc = 0.88874\n",
      "sampling:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69116 | train_auc: 0.71351 | valid_auc: 0.71606 |  0:00:00s\n",
      "epoch 1  | loss: 0.47802 | train_auc: 0.83633 | valid_auc: 0.83192 |  0:00:01s\n",
      "epoch 2  | loss: 0.44324 | train_auc: 0.85137 | valid_auc: 0.8498  |  0:00:01s\n",
      "epoch 3  | loss: 0.43142 | train_auc: 0.84367 | valid_auc: 0.83952 |  0:00:02s\n",
      "epoch 4  | loss: 0.42303 | train_auc: 0.85634 | valid_auc: 0.85704 |  0:00:02s\n",
      "epoch 5  | loss: 0.4265  | train_auc: 0.87008 | valid_auc: 0.86979 |  0:00:03s\n",
      "epoch 6  | loss: 0.41894 | train_auc: 0.8775  | valid_auc: 0.87794 |  0:00:03s\n",
      "epoch 7  | loss: 0.42281 | train_auc: 0.87779 | valid_auc: 0.87929 |  0:00:04s\n",
      "epoch 8  | loss: 0.43086 | train_auc: 0.87888 | valid_auc: 0.88063 |  0:00:04s\n",
      "epoch 9  | loss: 0.41809 | train_auc: 0.88098 | valid_auc: 0.88212 |  0:00:05s\n",
      "epoch 10 | loss: 0.41261 | train_auc: 0.88255 | valid_auc: 0.8839  |  0:00:06s\n",
      "epoch 11 | loss: 0.42274 | train_auc: 0.88547 | valid_auc: 0.88784 |  0:00:06s\n",
      "epoch 12 | loss: 0.4264  | train_auc: 0.88619 | valid_auc: 0.88721 |  0:00:07s\n",
      "epoch 13 | loss: 0.42264 | train_auc: 0.8864  | valid_auc: 0.8875  |  0:00:07s\n",
      "epoch 14 | loss: 0.4179  | train_auc: 0.88775 | valid_auc: 0.88832 |  0:00:08s\n",
      "epoch 15 | loss: 0.41397 | train_auc: 0.88597 | valid_auc: 0.88522 |  0:00:08s\n",
      "epoch 16 | loss: 0.42018 | train_auc: 0.8882  | valid_auc: 0.88916 |  0:00:09s\n",
      "epoch 17 | loss: 0.41813 | train_auc: 0.88866 | valid_auc: 0.8883  |  0:00:09s\n",
      "epoch 18 | loss: 0.42434 | train_auc: 0.88839 | valid_auc: 0.88797 |  0:00:10s\n",
      "epoch 19 | loss: 0.4129  | train_auc: 0.88863 | valid_auc: 0.88857 |  0:00:11s\n",
      "epoch 20 | loss: 0.41733 | train_auc: 0.88776 | valid_auc: 0.88749 |  0:00:11s\n",
      "epoch 21 | loss: 0.41964 | train_auc: 0.88931 | valid_auc: 0.88853 |  0:00:12s\n",
      "epoch 22 | loss: 0.4151  | train_auc: 0.88922 | valid_auc: 0.88846 |  0:00:12s\n",
      "epoch 23 | loss: 0.41731 | train_auc: 0.88907 | valid_auc: 0.88763 |  0:00:13s\n",
      "epoch 24 | loss: 0.42216 | train_auc: 0.88935 | valid_auc: 0.88881 |  0:00:13s\n",
      "epoch 25 | loss: 0.40934 | train_auc: 0.8891  | valid_auc: 0.88852 |  0:00:14s\n",
      "epoch 26 | loss: 0.41441 | train_auc: 0.8894  | valid_auc: 0.88845 |  0:00:14s\n",
      "epoch 27 | loss: 0.4191  | train_auc: 0.88922 | valid_auc: 0.88729 |  0:00:15s\n",
      "epoch 28 | loss: 0.4164  | train_auc: 0.88964 | valid_auc: 0.88849 |  0:00:16s\n",
      "epoch 29 | loss: 0.41862 | train_auc: 0.88931 | valid_auc: 0.88803 |  0:00:16s\n",
      "epoch 30 | loss: 0.41017 | train_auc: 0.88993 | valid_auc: 0.88863 |  0:00:17s\n",
      "epoch 31 | loss: 0.42274 | train_auc: 0.88976 | valid_auc: 0.8889  |  0:00:17s\n",
      "epoch 32 | loss: 0.41727 | train_auc: 0.89008 | valid_auc: 0.88871 |  0:00:18s\n",
      "epoch 33 | loss: 0.41734 | train_auc: 0.89012 | valid_auc: 0.88888 |  0:00:18s\n",
      "epoch 34 | loss: 0.40549 | train_auc: 0.88955 | valid_auc: 0.88816 |  0:00:19s\n",
      "epoch 35 | loss: 0.41351 | train_auc: 0.88937 | valid_auc: 0.88752 |  0:00:19s\n",
      "epoch 36 | loss: 0.41666 | train_auc: 0.89038 | valid_auc: 0.88885 |  0:00:20s\n",
      "epoch 37 | loss: 0.41327 | train_auc: 0.89022 | valid_auc: 0.88812 |  0:00:21s\n",
      "epoch 38 | loss: 0.41789 | train_auc: 0.89067 | valid_auc: 0.88883 |  0:00:21s\n",
      "epoch 39 | loss: 0.41609 | train_auc: 0.89003 | valid_auc: 0.88767 |  0:00:22s\n",
      "epoch 40 | loss: 0.41051 | train_auc: 0.89036 | valid_auc: 0.88822 |  0:00:22s\n",
      "epoch 41 | loss: 0.41123 | train_auc: 0.88892 | valid_auc: 0.88731 |  0:00:23s\n",
      "epoch 42 | loss: 0.40738 | train_auc: 0.89067 | valid_auc: 0.88884 |  0:00:23s\n",
      "epoch 43 | loss: 0.41433 | train_auc: 0.89062 | valid_auc: 0.88872 |  0:00:24s\n",
      "epoch 44 | loss: 0.41663 | train_auc: 0.89039 | valid_auc: 0.88793 |  0:00:24s\n",
      "epoch 45 | loss: 0.41084 | train_auc: 0.89017 | valid_auc: 0.88828 |  0:00:25s\n",
      "epoch 46 | loss: 0.41386 | train_auc: 0.89043 | valid_auc: 0.88921 |  0:00:26s\n",
      "epoch 47 | loss: 0.41242 | train_auc: 0.89024 | valid_auc: 0.88801 |  0:00:26s\n",
      "epoch 48 | loss: 0.41385 | train_auc: 0.88988 | valid_auc: 0.88877 |  0:00:27s\n",
      "epoch 49 | loss: 0.41617 | train_auc: 0.88984 | valid_auc: 0.88807 |  0:00:27s\n",
      "epoch 50 | loss: 0.41481 | train_auc: 0.89038 | valid_auc: 0.88881 |  0:00:28s\n",
      "epoch 51 | loss: 0.41769 | train_auc: 0.89051 | valid_auc: 0.88847 |  0:00:28s\n",
      "epoch 52 | loss: 0.4032  | train_auc: 0.89045 | valid_auc: 0.88883 |  0:00:29s\n",
      "epoch 53 | loss: 0.41181 | train_auc: 0.88998 | valid_auc: 0.88873 |  0:00:29s\n",
      "epoch 54 | loss: 0.41013 | train_auc: 0.88973 | valid_auc: 0.88811 |  0:00:30s\n",
      "epoch 55 | loss: 0.41877 | train_auc: 0.89028 | valid_auc: 0.88902 |  0:00:31s\n",
      "epoch 56 | loss: 0.41306 | train_auc: 0.89024 | valid_auc: 0.88928 |  0:00:31s\n",
      "epoch 57 | loss: 0.41784 | train_auc: 0.8906  | valid_auc: 0.88833 |  0:00:32s\n",
      "epoch 58 | loss: 0.41386 | train_auc: 0.8905  | valid_auc: 0.88787 |  0:00:32s\n",
      "epoch 59 | loss: 0.41028 | train_auc: 0.89054 | valid_auc: 0.88733 |  0:00:33s\n",
      "epoch 60 | loss: 0.40625 | train_auc: 0.89055 | valid_auc: 0.88842 |  0:00:33s\n",
      "epoch 61 | loss: 0.40977 | train_auc: 0.89067 | valid_auc: 0.88829 |  0:00:34s\n",
      "epoch 62 | loss: 0.41514 | train_auc: 0.89034 | valid_auc: 0.88866 |  0:00:34s\n",
      "epoch 63 | loss: 0.41047 | train_auc: 0.89042 | valid_auc: 0.8885  |  0:00:35s\n",
      "epoch 64 | loss: 0.42583 | train_auc: 0.89015 | valid_auc: 0.88748 |  0:00:36s\n",
      "epoch 65 | loss: 0.41231 | train_auc: 0.88954 | valid_auc: 0.88593 |  0:00:36s\n",
      "epoch 66 | loss: 0.41678 | train_auc: 0.8898  | valid_auc: 0.88811 |  0:00:37s\n",
      "epoch 67 | loss: 0.41994 | train_auc: 0.89058 | valid_auc: 0.88769 |  0:00:37s\n",
      "epoch 68 | loss: 0.41001 | train_auc: 0.89034 | valid_auc: 0.88779 |  0:00:38s\n",
      "epoch 69 | loss: 0.40589 | train_auc: 0.89043 | valid_auc: 0.88825 |  0:00:38s\n",
      "epoch 70 | loss: 0.41233 | train_auc: 0.89056 | valid_auc: 0.88777 |  0:00:39s\n",
      "epoch 71 | loss: 0.40982 | train_auc: 0.8909  | valid_auc: 0.88806 |  0:00:39s\n",
      "epoch 72 | loss: 0.41636 | train_auc: 0.89014 | valid_auc: 0.88801 |  0:00:40s\n",
      "epoch 73 | loss: 0.41351 | train_auc: 0.89033 | valid_auc: 0.88794 |  0:00:40s\n",
      "epoch 74 | loss: 0.41308 | train_auc: 0.89021 | valid_auc: 0.88847 |  0:00:41s\n",
      "epoch 75 | loss: 0.41821 | train_auc: 0.89052 | valid_auc: 0.88773 |  0:00:42s\n",
      "epoch 76 | loss: 0.41778 | train_auc: 0.89043 | valid_auc: 0.88793 |  0:00:42s\n",
      "epoch 77 | loss: 0.41085 | train_auc: 0.8907  | valid_auc: 0.88804 |  0:00:43s\n",
      "epoch 78 | loss: 0.41552 | train_auc: 0.89058 | valid_auc: 0.88858 |  0:00:43s\n",
      "epoch 79 | loss: 0.41135 | train_auc: 0.89025 | valid_auc: 0.88849 |  0:00:44s\n",
      "epoch 80 | loss: 0.41922 | train_auc: 0.89043 | valid_auc: 0.88718 |  0:00:44s\n",
      "epoch 81 | loss: 0.41128 | train_auc: 0.89059 | valid_auc: 0.88805 |  0:00:45s\n",
      "epoch 82 | loss: 0.41034 | train_auc: 0.89003 | valid_auc: 0.8878  |  0:00:46s\n",
      "epoch 83 | loss: 0.42702 | train_auc: 0.88981 | valid_auc: 0.88795 |  0:00:46s\n",
      "epoch 84 | loss: 0.42206 | train_auc: 0.89    | valid_auc: 0.88779 |  0:00:47s\n",
      "epoch 85 | loss: 0.42257 | train_auc: 0.89034 | valid_auc: 0.88711 |  0:00:47s\n",
      "epoch 86 | loss: 0.41962 | train_auc: 0.8905  | valid_auc: 0.88591 |  0:00:48s\n",
      "epoch 87 | loss: 0.40743 | train_auc: 0.89027 | valid_auc: 0.88771 |  0:00:48s\n",
      "epoch 88 | loss: 0.40488 | train_auc: 0.89019 | valid_auc: 0.88698 |  0:00:49s\n",
      "epoch 89 | loss: 0.41732 | train_auc: 0.89058 | valid_auc: 0.88747 |  0:00:49s\n",
      "epoch 90 | loss: 0.41527 | train_auc: 0.88999 | valid_auc: 0.88732 |  0:00:50s\n",
      "epoch 91 | loss: 0.42072 | train_auc: 0.89028 | valid_auc: 0.88862 |  0:00:50s\n",
      "epoch 92 | loss: 0.41827 | train_auc: 0.89037 | valid_auc: 0.88796 |  0:00:51s\n",
      "epoch 93 | loss: 0.41592 | train_auc: 0.89036 | valid_auc: 0.88767 |  0:00:51s\n",
      "epoch 94 | loss: 0.41417 | train_auc: 0.89079 | valid_auc: 0.8876  |  0:00:52s\n",
      "epoch 95 | loss: 0.41196 | train_auc: 0.89091 | valid_auc: 0.8888  |  0:00:53s\n",
      "epoch 96 | loss: 0.40963 | train_auc: 0.89039 | valid_auc: 0.88761 |  0:00:53s\n",
      "epoch 97 | loss: 0.42022 | train_auc: 0.89004 | valid_auc: 0.88825 |  0:00:54s\n",
      "epoch 98 | loss: 0.41572 | train_auc: 0.8897  | valid_auc: 0.88897 |  0:00:54s\n",
      "epoch 99 | loss: 0.41119 | train_auc: 0.89075 | valid_auc: 0.88845 |  0:00:55s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 56 and best_valid_auc = 0.88928\n",
      "sampling:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.68788 | train_auc: 0.64793 | valid_auc: 0.64305 |  0:00:00s\n",
      "epoch 1  | loss: 0.46198 | train_auc: 0.81992 | valid_auc: 0.81704 |  0:00:01s\n",
      "epoch 2  | loss: 0.43615 | train_auc: 0.84619 | valid_auc: 0.84175 |  0:00:01s\n",
      "epoch 3  | loss: 0.43112 | train_auc: 0.86566 | valid_auc: 0.86815 |  0:00:02s\n",
      "epoch 4  | loss: 0.4212  | train_auc: 0.86848 | valid_auc: 0.86822 |  0:00:02s\n",
      "epoch 5  | loss: 0.42811 | train_auc: 0.87453 | valid_auc: 0.87673 |  0:00:03s\n",
      "epoch 6  | loss: 0.41386 | train_auc: 0.87962 | valid_auc: 0.88299 |  0:00:03s\n",
      "epoch 7  | loss: 0.41401 | train_auc: 0.88054 | valid_auc: 0.88358 |  0:00:04s\n",
      "epoch 8  | loss: 0.42599 | train_auc: 0.88439 | valid_auc: 0.88792 |  0:00:04s\n",
      "epoch 9  | loss: 0.40348 | train_auc: 0.88692 | valid_auc: 0.8899  |  0:00:05s\n",
      "epoch 10 | loss: 0.40669 | train_auc: 0.88919 | valid_auc: 0.89253 |  0:00:06s\n",
      "epoch 11 | loss: 0.41186 | train_auc: 0.88855 | valid_auc: 0.89148 |  0:00:06s\n",
      "epoch 12 | loss: 0.41387 | train_auc: 0.88934 | valid_auc: 0.89355 |  0:00:07s\n",
      "epoch 13 | loss: 0.41968 | train_auc: 0.88998 | valid_auc: 0.8942  |  0:00:07s\n",
      "epoch 14 | loss: 0.41352 | train_auc: 0.88909 | valid_auc: 0.89416 |  0:00:08s\n",
      "epoch 15 | loss: 0.41479 | train_auc: 0.89081 | valid_auc: 0.89546 |  0:00:08s\n",
      "epoch 16 | loss: 0.40855 | train_auc: 0.88902 | valid_auc: 0.89388 |  0:00:09s\n",
      "epoch 17 | loss: 0.41999 | train_auc: 0.89127 | valid_auc: 0.89548 |  0:00:10s\n",
      "epoch 18 | loss: 0.41258 | train_auc: 0.89082 | valid_auc: 0.8952  |  0:00:10s\n",
      "epoch 19 | loss: 0.41669 | train_auc: 0.8906  | valid_auc: 0.894   |  0:00:11s\n",
      "epoch 20 | loss: 0.4066  | train_auc: 0.89156 | valid_auc: 0.89533 |  0:00:11s\n",
      "epoch 21 | loss: 0.41212 | train_auc: 0.8925  | valid_auc: 0.89589 |  0:00:12s\n",
      "epoch 22 | loss: 0.40953 | train_auc: 0.89234 | valid_auc: 0.89617 |  0:00:12s\n",
      "epoch 23 | loss: 0.41178 | train_auc: 0.8923  | valid_auc: 0.89624 |  0:00:13s\n",
      "epoch 24 | loss: 0.41608 | train_auc: 0.89224 | valid_auc: 0.89606 |  0:00:13s\n",
      "epoch 25 | loss: 0.40208 | train_auc: 0.89206 | valid_auc: 0.89621 |  0:00:14s\n",
      "epoch 26 | loss: 0.41555 | train_auc: 0.89225 | valid_auc: 0.89551 |  0:00:14s\n",
      "epoch 27 | loss: 0.41772 | train_auc: 0.89225 | valid_auc: 0.89631 |  0:00:15s\n",
      "epoch 28 | loss: 0.41279 | train_auc: 0.89249 | valid_auc: 0.89621 |  0:00:16s\n",
      "epoch 29 | loss: 0.41796 | train_auc: 0.89262 | valid_auc: 0.89677 |  0:00:16s\n",
      "epoch 30 | loss: 0.41326 | train_auc: 0.89227 | valid_auc: 0.89508 |  0:00:17s\n",
      "epoch 31 | loss: 0.40927 | train_auc: 0.89265 | valid_auc: 0.8955  |  0:00:17s\n",
      "epoch 32 | loss: 0.41307 | train_auc: 0.89243 | valid_auc: 0.89551 |  0:00:18s\n",
      "epoch 33 | loss: 0.40899 | train_auc: 0.89303 | valid_auc: 0.89569 |  0:00:18s\n",
      "epoch 34 | loss: 0.41817 | train_auc: 0.89305 | valid_auc: 0.89557 |  0:00:19s\n",
      "epoch 35 | loss: 0.40435 | train_auc: 0.89312 | valid_auc: 0.89538 |  0:00:20s\n",
      "epoch 36 | loss: 0.41271 | train_auc: 0.89361 | valid_auc: 0.89466 |  0:00:20s\n",
      "epoch 37 | loss: 0.41301 | train_auc: 0.89315 | valid_auc: 0.89553 |  0:00:21s\n",
      "epoch 38 | loss: 0.41086 | train_auc: 0.89285 | valid_auc: 0.89556 |  0:00:21s\n",
      "epoch 39 | loss: 0.4109  | train_auc: 0.89339 | valid_auc: 0.89594 |  0:00:22s\n",
      "epoch 40 | loss: 0.41524 | train_auc: 0.89299 | valid_auc: 0.89421 |  0:00:22s\n",
      "epoch 41 | loss: 0.4158  | train_auc: 0.89165 | valid_auc: 0.89385 |  0:00:23s\n",
      "epoch 42 | loss: 0.40416 | train_auc: 0.8933  | valid_auc: 0.89536 |  0:00:23s\n",
      "epoch 43 | loss: 0.40364 | train_auc: 0.89375 | valid_auc: 0.89511 |  0:00:24s\n",
      "epoch 44 | loss: 0.39952 | train_auc: 0.89399 | valid_auc: 0.89485 |  0:00:24s\n",
      "epoch 45 | loss: 0.4122  | train_auc: 0.8937  | valid_auc: 0.89506 |  0:00:25s\n",
      "epoch 46 | loss: 0.41186 | train_auc: 0.89341 | valid_auc: 0.89462 |  0:00:26s\n",
      "epoch 47 | loss: 0.4082  | train_auc: 0.89381 | valid_auc: 0.89539 |  0:00:26s\n",
      "epoch 48 | loss: 0.40756 | train_auc: 0.89302 | valid_auc: 0.89476 |  0:00:27s\n",
      "epoch 49 | loss: 0.40035 | train_auc: 0.89387 | valid_auc: 0.89422 |  0:00:27s\n",
      "epoch 50 | loss: 0.41109 | train_auc: 0.89341 | valid_auc: 0.89476 |  0:00:28s\n",
      "epoch 51 | loss: 0.40972 | train_auc: 0.89359 | valid_auc: 0.89466 |  0:00:28s\n",
      "epoch 52 | loss: 0.41495 | train_auc: 0.89384 | valid_auc: 0.89402 |  0:00:29s\n",
      "epoch 53 | loss: 0.40344 | train_auc: 0.89353 | valid_auc: 0.89559 |  0:00:29s\n",
      "epoch 54 | loss: 0.41405 | train_auc: 0.8939  | valid_auc: 0.89574 |  0:00:30s\n",
      "epoch 55 | loss: 0.41833 | train_auc: 0.89398 | valid_auc: 0.89599 |  0:00:30s\n",
      "epoch 56 | loss: 0.41476 | train_auc: 0.89381 | valid_auc: 0.89496 |  0:00:31s\n",
      "epoch 57 | loss: 0.41293 | train_auc: 0.89408 | valid_auc: 0.89466 |  0:00:32s\n",
      "epoch 58 | loss: 0.41032 | train_auc: 0.89348 | valid_auc: 0.89487 |  0:00:32s\n",
      "epoch 59 | loss: 0.41418 | train_auc: 0.89345 | valid_auc: 0.89503 |  0:00:33s\n",
      "epoch 60 | loss: 0.40937 | train_auc: 0.89368 | valid_auc: 0.8951  |  0:00:33s\n",
      "epoch 61 | loss: 0.41289 | train_auc: 0.8933  | valid_auc: 0.89423 |  0:00:34s\n",
      "epoch 62 | loss: 0.41341 | train_auc: 0.89404 | valid_auc: 0.89378 |  0:00:34s\n",
      "epoch 63 | loss: 0.40789 | train_auc: 0.89364 | valid_auc: 0.89401 |  0:00:35s\n",
      "epoch 64 | loss: 0.40772 | train_auc: 0.89374 | valid_auc: 0.89542 |  0:00:35s\n",
      "epoch 65 | loss: 0.41273 | train_auc: 0.89378 | valid_auc: 0.89403 |  0:00:36s\n",
      "epoch 66 | loss: 0.40581 | train_auc: 0.89388 | valid_auc: 0.89469 |  0:00:37s\n",
      "epoch 67 | loss: 0.4113  | train_auc: 0.89391 | valid_auc: 0.89514 |  0:00:37s\n",
      "epoch 68 | loss: 0.40229 | train_auc: 0.89388 | valid_auc: 0.89523 |  0:00:38s\n",
      "epoch 69 | loss: 0.41213 | train_auc: 0.89382 | valid_auc: 0.8928  |  0:00:38s\n",
      "epoch 70 | loss: 0.41202 | train_auc: 0.89338 | valid_auc: 0.89593 |  0:00:39s\n",
      "epoch 71 | loss: 0.40472 | train_auc: 0.89327 | valid_auc: 0.89518 |  0:00:39s\n",
      "epoch 72 | loss: 0.41072 | train_auc: 0.89328 | valid_auc: 0.89536 |  0:00:40s\n",
      "epoch 73 | loss: 0.41167 | train_auc: 0.89347 | valid_auc: 0.89591 |  0:00:40s\n",
      "epoch 74 | loss: 0.40826 | train_auc: 0.89396 | valid_auc: 0.8959  |  0:00:41s\n",
      "epoch 75 | loss: 0.41111 | train_auc: 0.89376 | valid_auc: 0.89582 |  0:00:42s\n",
      "epoch 76 | loss: 0.41385 | train_auc: 0.89313 | valid_auc: 0.89657 |  0:00:42s\n",
      "epoch 77 | loss: 0.40423 | train_auc: 0.89372 | valid_auc: 0.89631 |  0:00:43s\n",
      "epoch 78 | loss: 0.40669 | train_auc: 0.89425 | valid_auc: 0.89523 |  0:00:43s\n",
      "epoch 79 | loss: 0.41094 | train_auc: 0.8937  | valid_auc: 0.89553 |  0:00:44s\n",
      "\n",
      "Early stopping occurred at epoch 79 with best_epoch = 29 and best_valid_auc = 0.89677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.68857 | train_auc: 0.56324 | valid_auc: 0.54878 |  0:00:00s\n",
      "epoch 1  | loss: 0.47614 | train_auc: 0.773   | valid_auc: 0.78704 |  0:00:01s\n",
      "epoch 2  | loss: 0.44058 | train_auc: 0.82724 | valid_auc: 0.82958 |  0:00:01s\n",
      "epoch 3  | loss: 0.43758 | train_auc: 0.85341 | valid_auc: 0.86078 |  0:00:02s\n",
      "epoch 4  | loss: 0.43203 | train_auc: 0.8597  | valid_auc: 0.8666  |  0:00:02s\n",
      "epoch 5  | loss: 0.42071 | train_auc: 0.86121 | valid_auc: 0.86571 |  0:00:03s\n",
      "epoch 6  | loss: 0.43569 | train_auc: 0.86675 | valid_auc: 0.8746  |  0:00:03s\n",
      "epoch 7  | loss: 0.42799 | train_auc: 0.87529 | valid_auc: 0.88515 |  0:00:04s\n",
      "epoch 8  | loss: 0.42777 | train_auc: 0.87712 | valid_auc: 0.88507 |  0:00:04s\n",
      "epoch 9  | loss: 0.43057 | train_auc: 0.87634 | valid_auc: 0.8852  |  0:00:05s\n",
      "epoch 10 | loss: 0.42506 | train_auc: 0.88092 | valid_auc: 0.89055 |  0:00:06s\n",
      "epoch 11 | loss: 0.427   | train_auc: 0.88058 | valid_auc: 0.88857 |  0:00:06s\n",
      "epoch 12 | loss: 0.42328 | train_auc: 0.88171 | valid_auc: 0.8897  |  0:00:07s\n",
      "epoch 13 | loss: 0.4308  | train_auc: 0.88335 | valid_auc: 0.8913  |  0:00:07s\n",
      "epoch 14 | loss: 0.42744 | train_auc: 0.88225 | valid_auc: 0.89068 |  0:00:08s\n",
      "epoch 15 | loss: 0.42971 | train_auc: 0.88482 | valid_auc: 0.89263 |  0:00:08s\n",
      "epoch 16 | loss: 0.42989 | train_auc: 0.88431 | valid_auc: 0.89255 |  0:00:09s\n",
      "epoch 17 | loss: 0.42998 | train_auc: 0.88527 | valid_auc: 0.89345 |  0:00:10s\n",
      "epoch 18 | loss: 0.42494 | train_auc: 0.88463 | valid_auc: 0.89273 |  0:00:10s\n",
      "epoch 19 | loss: 0.42618 | train_auc: 0.88441 | valid_auc: 0.89227 |  0:00:11s\n",
      "epoch 20 | loss: 0.41942 | train_auc: 0.88501 | valid_auc: 0.89226 |  0:00:11s\n",
      "epoch 21 | loss: 0.42271 | train_auc: 0.88526 | valid_auc: 0.89316 |  0:00:12s\n",
      "epoch 22 | loss: 0.43176 | train_auc: 0.8848  | valid_auc: 0.89159 |  0:00:12s\n",
      "epoch 23 | loss: 0.42808 | train_auc: 0.88561 | valid_auc: 0.89321 |  0:00:13s\n",
      "epoch 24 | loss: 0.41982 | train_auc: 0.88588 | valid_auc: 0.89305 |  0:00:13s\n",
      "epoch 25 | loss: 0.4345  | train_auc: 0.88596 | valid_auc: 0.8923  |  0:00:14s\n",
      "epoch 26 | loss: 0.42251 | train_auc: 0.88601 | valid_auc: 0.89254 |  0:00:14s\n",
      "epoch 27 | loss: 0.41805 | train_auc: 0.88604 | valid_auc: 0.89296 |  0:00:15s\n",
      "epoch 28 | loss: 0.42007 | train_auc: 0.88598 | valid_auc: 0.89166 |  0:00:16s\n",
      "epoch 29 | loss: 0.42328 | train_auc: 0.88611 | valid_auc: 0.89309 |  0:00:16s\n",
      "epoch 30 | loss: 0.42717 | train_auc: 0.88576 | valid_auc: 0.8932  |  0:00:17s\n",
      "epoch 31 | loss: 0.41259 | train_auc: 0.88607 | valid_auc: 0.89332 |  0:00:17s\n",
      "epoch 32 | loss: 0.42015 | train_auc: 0.88603 | valid_auc: 0.89253 |  0:00:18s\n",
      "epoch 33 | loss: 0.42552 | train_auc: 0.88614 | valid_auc: 0.89254 |  0:00:18s\n",
      "epoch 34 | loss: 0.41607 | train_auc: 0.88646 | valid_auc: 0.89327 |  0:00:19s\n",
      "epoch 35 | loss: 0.42125 | train_auc: 0.8865  | valid_auc: 0.89413 |  0:00:20s\n",
      "epoch 36 | loss: 0.42674 | train_auc: 0.88574 | valid_auc: 0.89311 |  0:00:20s\n",
      "epoch 37 | loss: 0.42725 | train_auc: 0.88588 | valid_auc: 0.89191 |  0:00:21s\n",
      "epoch 38 | loss: 0.42717 | train_auc: 0.88518 | valid_auc: 0.89259 |  0:00:21s\n",
      "epoch 39 | loss: 0.42326 | train_auc: 0.88592 | valid_auc: 0.89384 |  0:00:22s\n",
      "epoch 40 | loss: 0.42257 | train_auc: 0.88596 | valid_auc: 0.89277 |  0:00:22s\n",
      "epoch 41 | loss: 0.42382 | train_auc: 0.8861  | valid_auc: 0.89336 |  0:00:23s\n",
      "epoch 42 | loss: 0.42061 | train_auc: 0.88557 | valid_auc: 0.8932  |  0:00:23s\n",
      "epoch 43 | loss: 0.4199  | train_auc: 0.88656 | valid_auc: 0.8931  |  0:00:24s\n",
      "epoch 44 | loss: 0.41552 | train_auc: 0.88624 | valid_auc: 0.89237 |  0:00:25s\n",
      "epoch 45 | loss: 0.41841 | train_auc: 0.88642 | valid_auc: 0.89372 |  0:00:25s\n",
      "epoch 46 | loss: 0.41897 | train_auc: 0.88619 | valid_auc: 0.89319 |  0:00:26s\n",
      "epoch 47 | loss: 0.41929 | train_auc: 0.88608 | valid_auc: 0.8934  |  0:00:26s\n",
      "epoch 48 | loss: 0.41943 | train_auc: 0.88523 | valid_auc: 0.894   |  0:00:27s\n",
      "epoch 49 | loss: 0.42527 | train_auc: 0.88653 | valid_auc: 0.89293 |  0:00:27s\n",
      "epoch 50 | loss: 0.42498 | train_auc: 0.88671 | valid_auc: 0.89389 |  0:00:28s\n",
      "epoch 51 | loss: 0.41838 | train_auc: 0.88662 | valid_auc: 0.89445 |  0:00:28s\n",
      "epoch 52 | loss: 0.4137  | train_auc: 0.88641 | valid_auc: 0.89399 |  0:00:29s\n",
      "epoch 53 | loss: 0.41792 | train_auc: 0.88689 | valid_auc: 0.894   |  0:00:30s\n",
      "epoch 54 | loss: 0.43172 | train_auc: 0.88544 | valid_auc: 0.89376 |  0:00:30s\n",
      "epoch 55 | loss: 0.43026 | train_auc: 0.8862  | valid_auc: 0.89345 |  0:00:31s\n",
      "epoch 56 | loss: 0.42707 | train_auc: 0.88519 | valid_auc: 0.89266 |  0:00:31s\n",
      "epoch 57 | loss: 0.42068 | train_auc: 0.88679 | valid_auc: 0.89341 |  0:00:32s\n",
      "epoch 58 | loss: 0.41557 | train_auc: 0.8863  | valid_auc: 0.89332 |  0:00:32s\n",
      "epoch 59 | loss: 0.42562 | train_auc: 0.88676 | valid_auc: 0.89311 |  0:00:33s\n",
      "epoch 60 | loss: 0.42211 | train_auc: 0.88653 | valid_auc: 0.89229 |  0:00:33s\n",
      "epoch 61 | loss: 0.42234 | train_auc: 0.88617 | valid_auc: 0.89372 |  0:00:34s\n",
      "epoch 62 | loss: 0.42302 | train_auc: 0.88632 | valid_auc: 0.89285 |  0:00:34s\n",
      "epoch 63 | loss: 0.42368 | train_auc: 0.88685 | valid_auc: 0.89342 |  0:00:35s\n",
      "epoch 64 | loss: 0.4149  | train_auc: 0.88668 | valid_auc: 0.8939  |  0:00:36s\n",
      "epoch 65 | loss: 0.41812 | train_auc: 0.88722 | valid_auc: 0.89355 |  0:00:36s\n",
      "epoch 66 | loss: 0.41741 | train_auc: 0.88722 | valid_auc: 0.89351 |  0:00:37s\n",
      "epoch 67 | loss: 0.41929 | train_auc: 0.8868  | valid_auc: 0.89271 |  0:00:37s\n",
      "epoch 68 | loss: 0.42567 | train_auc: 0.88654 | valid_auc: 0.89396 |  0:00:38s\n",
      "epoch 69 | loss: 0.42332 | train_auc: 0.88698 | valid_auc: 0.89443 |  0:00:38s\n",
      "epoch 70 | loss: 0.42552 | train_auc: 0.88684 | valid_auc: 0.89471 |  0:00:39s\n",
      "epoch 71 | loss: 0.42113 | train_auc: 0.88686 | valid_auc: 0.8952  |  0:00:39s\n",
      "epoch 72 | loss: 0.4279  | train_auc: 0.88622 | valid_auc: 0.89403 |  0:00:40s\n",
      "epoch 73 | loss: 0.4245  | train_auc: 0.88646 | valid_auc: 0.89445 |  0:00:41s\n",
      "epoch 74 | loss: 0.41952 | train_auc: 0.88678 | valid_auc: 0.89374 |  0:00:41s\n",
      "epoch 75 | loss: 0.42162 | train_auc: 0.88674 | valid_auc: 0.89247 |  0:00:42s\n",
      "epoch 76 | loss: 0.41927 | train_auc: 0.88673 | valid_auc: 0.89373 |  0:00:42s\n",
      "epoch 77 | loss: 0.42403 | train_auc: 0.88716 | valid_auc: 0.89385 |  0:00:43s\n",
      "epoch 78 | loss: 0.41582 | train_auc: 0.88728 | valid_auc: 0.89442 |  0:00:43s\n",
      "epoch 79 | loss: 0.41578 | train_auc: 0.88747 | valid_auc: 0.8938  |  0:00:44s\n",
      "epoch 80 | loss: 0.41816 | train_auc: 0.88674 | valid_auc: 0.89395 |  0:00:44s\n",
      "epoch 81 | loss: 0.4219  | train_auc: 0.88726 | valid_auc: 0.89256 |  0:00:45s\n",
      "epoch 82 | loss: 0.42569 | train_auc: 0.88666 | valid_auc: 0.89352 |  0:00:46s\n",
      "epoch 83 | loss: 0.41756 | train_auc: 0.88693 | valid_auc: 0.89469 |  0:00:46s\n",
      "epoch 84 | loss: 0.41976 | train_auc: 0.8863  | valid_auc: 0.89435 |  0:00:47s\n",
      "epoch 85 | loss: 0.41561 | train_auc: 0.88624 | valid_auc: 0.89381 |  0:00:47s\n",
      "epoch 86 | loss: 0.42407 | train_auc: 0.88755 | valid_auc: 0.89326 |  0:00:48s\n",
      "epoch 87 | loss: 0.41917 | train_auc: 0.88721 | valid_auc: 0.89351 |  0:00:48s\n",
      "epoch 88 | loss: 0.41701 | train_auc: 0.88713 | valid_auc: 0.89277 |  0:00:49s\n",
      "epoch 89 | loss: 0.4162  | train_auc: 0.88765 | valid_auc: 0.89223 |  0:00:50s\n",
      "epoch 90 | loss: 0.4241  | train_auc: 0.88724 | valid_auc: 0.89356 |  0:00:50s\n",
      "epoch 91 | loss: 0.42416 | train_auc: 0.88752 | valid_auc: 0.8942  |  0:00:51s\n",
      "epoch 92 | loss: 0.42155 | train_auc: 0.88772 | valid_auc: 0.8936  |  0:00:51s\n",
      "epoch 93 | loss: 0.4257  | train_auc: 0.88766 | valid_auc: 0.8936  |  0:00:52s\n",
      "epoch 94 | loss: 0.41372 | train_auc: 0.88752 | valid_auc: 0.89352 |  0:00:52s\n",
      "epoch 95 | loss: 0.42118 | train_auc: 0.88598 | valid_auc: 0.8937  |  0:00:53s\n",
      "epoch 96 | loss: 0.42507 | train_auc: 0.88692 | valid_auc: 0.89359 |  0:00:53s\n",
      "epoch 97 | loss: 0.42512 | train_auc: 0.88751 | valid_auc: 0.89442 |  0:00:54s\n",
      "epoch 98 | loss: 0.42276 | train_auc: 0.88729 | valid_auc: 0.89366 |  0:00:54s\n",
      "epoch 99 | loss: 0.41723 | train_auc: 0.8873  | valid_auc: 0.8931  |  0:00:55s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 71 and best_valid_auc = 0.8952\n",
      "sampling:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69785 | train_auc: 0.54804 | valid_auc: 0.56854 |  0:00:00s\n",
      "epoch 1  | loss: 0.46273 | train_auc: 0.6042  | valid_auc: 0.63979 |  0:00:01s\n",
      "epoch 2  | loss: 0.4367  | train_auc: 0.7574  | valid_auc: 0.76828 |  0:00:01s\n",
      "epoch 3  | loss: 0.43552 | train_auc: 0.80032 | valid_auc: 0.80065 |  0:00:02s\n",
      "epoch 4  | loss: 0.42726 | train_auc: 0.85825 | valid_auc: 0.85976 |  0:00:02s\n",
      "epoch 5  | loss: 0.42726 | train_auc: 0.86496 | valid_auc: 0.85533 |  0:00:03s\n",
      "epoch 6  | loss: 0.41752 | train_auc: 0.87709 | valid_auc: 0.87032 |  0:00:03s\n",
      "epoch 7  | loss: 0.41847 | train_auc: 0.87796 | valid_auc: 0.872   |  0:00:04s\n",
      "epoch 8  | loss: 0.42417 | train_auc: 0.88294 | valid_auc: 0.87804 |  0:00:05s\n",
      "epoch 9  | loss: 0.41934 | train_auc: 0.88272 | valid_auc: 0.87871 |  0:00:05s\n",
      "epoch 10 | loss: 0.42933 | train_auc: 0.88422 | valid_auc: 0.8805  |  0:00:06s\n",
      "epoch 11 | loss: 0.41939 | train_auc: 0.88637 | valid_auc: 0.87979 |  0:00:06s\n",
      "epoch 12 | loss: 0.42126 | train_auc: 0.88811 | valid_auc: 0.8804  |  0:00:07s\n",
      "epoch 13 | loss: 0.4187  | train_auc: 0.88804 | valid_auc: 0.88481 |  0:00:07s\n",
      "epoch 14 | loss: 0.41831 | train_auc: 0.88853 | valid_auc: 0.88407 |  0:00:08s\n",
      "epoch 15 | loss: 0.41704 | train_auc: 0.88805 | valid_auc: 0.88298 |  0:00:08s\n",
      "epoch 16 | loss: 0.42006 | train_auc: 0.8897  | valid_auc: 0.88597 |  0:00:09s\n",
      "epoch 17 | loss: 0.41998 | train_auc: 0.88789 | valid_auc: 0.8843  |  0:00:10s\n",
      "epoch 18 | loss: 0.42151 | train_auc: 0.88938 | valid_auc: 0.88559 |  0:00:10s\n",
      "epoch 19 | loss: 0.4149  | train_auc: 0.88947 | valid_auc: 0.88343 |  0:00:11s\n",
      "epoch 20 | loss: 0.41533 | train_auc: 0.89036 | valid_auc: 0.88785 |  0:00:11s\n",
      "epoch 21 | loss: 0.41716 | train_auc: 0.89005 | valid_auc: 0.88473 |  0:00:12s\n",
      "epoch 22 | loss: 0.41089 | train_auc: 0.8905  | valid_auc: 0.88706 |  0:00:12s\n",
      "epoch 23 | loss: 0.41972 | train_auc: 0.89094 | valid_auc: 0.88796 |  0:00:13s\n",
      "epoch 24 | loss: 0.41582 | train_auc: 0.89091 | valid_auc: 0.88656 |  0:00:13s\n",
      "epoch 25 | loss: 0.41177 | train_auc: 0.89152 | valid_auc: 0.88763 |  0:00:14s\n",
      "epoch 26 | loss: 0.40677 | train_auc: 0.89092 | valid_auc: 0.8871  |  0:00:14s\n",
      "epoch 27 | loss: 0.41901 | train_auc: 0.89054 | valid_auc: 0.88691 |  0:00:15s\n",
      "epoch 28 | loss: 0.41453 | train_auc: 0.89161 | valid_auc: 0.88576 |  0:00:16s\n",
      "epoch 29 | loss: 0.41368 | train_auc: 0.89013 | valid_auc: 0.88489 |  0:00:16s\n",
      "epoch 30 | loss: 0.40974 | train_auc: 0.89077 | valid_auc: 0.88509 |  0:00:17s\n",
      "epoch 31 | loss: 0.41344 | train_auc: 0.89102 | valid_auc: 0.88682 |  0:00:17s\n",
      "epoch 32 | loss: 0.41375 | train_auc: 0.89098 | valid_auc: 0.88741 |  0:00:18s\n",
      "epoch 33 | loss: 0.41891 | train_auc: 0.89094 | valid_auc: 0.88788 |  0:00:18s\n",
      "epoch 34 | loss: 0.42309 | train_auc: 0.89032 | valid_auc: 0.88683 |  0:00:19s\n",
      "epoch 35 | loss: 0.41659 | train_auc: 0.89061 | valid_auc: 0.88737 |  0:00:19s\n",
      "epoch 36 | loss: 0.41923 | train_auc: 0.89033 | valid_auc: 0.88815 |  0:00:20s\n",
      "epoch 37 | loss: 0.41739 | train_auc: 0.89114 | valid_auc: 0.88737 |  0:00:21s\n",
      "epoch 38 | loss: 0.41997 | train_auc: 0.89005 | valid_auc: 0.88692 |  0:00:21s\n",
      "epoch 39 | loss: 0.41553 | train_auc: 0.88951 | valid_auc: 0.88696 |  0:00:22s\n",
      "epoch 40 | loss: 0.42035 | train_auc: 0.88987 | valid_auc: 0.88533 |  0:00:22s\n",
      "epoch 41 | loss: 0.41868 | train_auc: 0.89003 | valid_auc: 0.88677 |  0:00:23s\n",
      "epoch 42 | loss: 0.42392 | train_auc: 0.89029 | valid_auc: 0.88804 |  0:00:23s\n",
      "epoch 43 | loss: 0.41673 | train_auc: 0.89006 | valid_auc: 0.88578 |  0:00:24s\n",
      "epoch 44 | loss: 0.4218  | train_auc: 0.89036 | valid_auc: 0.88611 |  0:00:24s\n",
      "epoch 45 | loss: 0.42615 | train_auc: 0.8901  | valid_auc: 0.88622 |  0:00:25s\n",
      "epoch 46 | loss: 0.41619 | train_auc: 0.8897  | valid_auc: 0.88888 |  0:00:25s\n",
      "epoch 47 | loss: 0.41531 | train_auc: 0.8906  | valid_auc: 0.88664 |  0:00:26s\n",
      "epoch 48 | loss: 0.40885 | train_auc: 0.88981 | valid_auc: 0.88632 |  0:00:27s\n",
      "epoch 49 | loss: 0.41299 | train_auc: 0.89085 | valid_auc: 0.88732 |  0:00:27s\n",
      "epoch 50 | loss: 0.41429 | train_auc: 0.89055 | valid_auc: 0.88731 |  0:00:28s\n",
      "epoch 51 | loss: 0.41909 | train_auc: 0.89016 | valid_auc: 0.88829 |  0:00:28s\n",
      "epoch 52 | loss: 0.42101 | train_auc: 0.88999 | valid_auc: 0.8886  |  0:00:29s\n",
      "epoch 53 | loss: 0.40791 | train_auc: 0.89078 | valid_auc: 0.88726 |  0:00:29s\n",
      "epoch 54 | loss: 0.41476 | train_auc: 0.89088 | valid_auc: 0.88746 |  0:00:30s\n",
      "epoch 55 | loss: 0.41864 | train_auc: 0.89143 | valid_auc: 0.88828 |  0:00:30s\n",
      "epoch 56 | loss: 0.41596 | train_auc: 0.88994 | valid_auc: 0.88811 |  0:00:31s\n",
      "epoch 57 | loss: 0.41881 | train_auc: 0.89082 | valid_auc: 0.88696 |  0:00:32s\n",
      "epoch 58 | loss: 0.41162 | train_auc: 0.89084 | valid_auc: 0.88888 |  0:00:32s\n",
      "epoch 59 | loss: 0.41007 | train_auc: 0.89037 | valid_auc: 0.88873 |  0:00:33s\n",
      "epoch 60 | loss: 0.41197 | train_auc: 0.88938 | valid_auc: 0.88695 |  0:00:33s\n",
      "epoch 61 | loss: 0.41858 | train_auc: 0.89036 | valid_auc: 0.88868 |  0:00:34s\n",
      "epoch 62 | loss: 0.41417 | train_auc: 0.89092 | valid_auc: 0.88756 |  0:00:34s\n",
      "epoch 63 | loss: 0.42044 | train_auc: 0.89077 | valid_auc: 0.88725 |  0:00:35s\n",
      "epoch 64 | loss: 0.41963 | train_auc: 0.89109 | valid_auc: 0.88822 |  0:00:35s\n",
      "epoch 65 | loss: 0.402   | train_auc: 0.89094 | valid_auc: 0.88724 |  0:00:36s\n",
      "epoch 66 | loss: 0.41583 | train_auc: 0.89097 | valid_auc: 0.88997 |  0:00:37s\n",
      "epoch 67 | loss: 0.4131  | train_auc: 0.89089 | valid_auc: 0.88678 |  0:00:37s\n",
      "epoch 68 | loss: 0.41748 | train_auc: 0.89055 | valid_auc: 0.88603 |  0:00:38s\n",
      "epoch 69 | loss: 0.40812 | train_auc: 0.89151 | valid_auc: 0.88933 |  0:00:38s\n",
      "epoch 70 | loss: 0.41572 | train_auc: 0.89115 | valid_auc: 0.88674 |  0:00:39s\n",
      "epoch 71 | loss: 0.41434 | train_auc: 0.89083 | valid_auc: 0.88561 |  0:00:39s\n",
      "epoch 72 | loss: 0.41348 | train_auc: 0.89068 | valid_auc: 0.88797 |  0:00:40s\n",
      "epoch 73 | loss: 0.40848 | train_auc: 0.89084 | valid_auc: 0.88633 |  0:00:40s\n",
      "epoch 74 | loss: 0.41396 | train_auc: 0.89133 | valid_auc: 0.88815 |  0:00:41s\n",
      "epoch 75 | loss: 0.41733 | train_auc: 0.8913  | valid_auc: 0.88854 |  0:00:42s\n",
      "epoch 76 | loss: 0.41174 | train_auc: 0.89129 | valid_auc: 0.88823 |  0:00:42s\n",
      "epoch 77 | loss: 0.41265 | train_auc: 0.89108 | valid_auc: 0.88771 |  0:00:43s\n",
      "epoch 78 | loss: 0.42565 | train_auc: 0.89053 | valid_auc: 0.88767 |  0:00:43s\n",
      "epoch 79 | loss: 0.41825 | train_auc: 0.89053 | valid_auc: 0.88511 |  0:00:44s\n",
      "epoch 80 | loss: 0.41457 | train_auc: 0.89145 | valid_auc: 0.887   |  0:00:44s\n",
      "epoch 81 | loss: 0.40613 | train_auc: 0.89047 | valid_auc: 0.88626 |  0:00:45s\n",
      "epoch 82 | loss: 0.41987 | train_auc: 0.89048 | valid_auc: 0.88797 |  0:00:45s\n",
      "epoch 83 | loss: 0.41343 | train_auc: 0.88974 | valid_auc: 0.88865 |  0:00:46s\n",
      "epoch 84 | loss: 0.41702 | train_auc: 0.88997 | valid_auc: 0.88621 |  0:00:46s\n",
      "epoch 85 | loss: 0.41372 | train_auc: 0.89066 | valid_auc: 0.88639 |  0:00:47s\n",
      "epoch 86 | loss: 0.41551 | train_auc: 0.89091 | valid_auc: 0.88724 |  0:00:48s\n",
      "epoch 87 | loss: 0.4122  | train_auc: 0.89048 | valid_auc: 0.88738 |  0:00:48s\n",
      "epoch 88 | loss: 0.41501 | train_auc: 0.88977 | valid_auc: 0.88502 |  0:00:49s\n",
      "epoch 89 | loss: 0.41546 | train_auc: 0.89088 | valid_auc: 0.88699 |  0:00:49s\n",
      "epoch 90 | loss: 0.42054 | train_auc: 0.89126 | valid_auc: 0.88616 |  0:00:50s\n",
      "epoch 91 | loss: 0.41326 | train_auc: 0.89059 | valid_auc: 0.88786 |  0:00:50s\n",
      "epoch 92 | loss: 0.41558 | train_auc: 0.89101 | valid_auc: 0.88761 |  0:00:51s\n",
      "epoch 93 | loss: 0.41143 | train_auc: 0.89035 | valid_auc: 0.88636 |  0:00:52s\n",
      "epoch 94 | loss: 0.41762 | train_auc: 0.89035 | valid_auc: 0.88827 |  0:00:52s\n",
      "epoch 95 | loss: 0.41686 | train_auc: 0.89059 | valid_auc: 0.88844 |  0:00:53s\n",
      "epoch 96 | loss: 0.41397 | train_auc: 0.8899  | valid_auc: 0.88907 |  0:00:53s\n",
      "epoch 97 | loss: 0.41663 | train_auc: 0.88988 | valid_auc: 0.88704 |  0:00:54s\n",
      "epoch 98 | loss: 0.4153  | train_auc: 0.89079 | valid_auc: 0.88819 |  0:00:54s\n",
      "epoch 99 | loss: 0.41253 | train_auc: 0.89083 | valid_auc: 0.88512 |  0:00:55s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 66 and best_valid_auc = 0.88997\n",
      "sampling:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69368 | train_auc: 0.55542 | valid_auc: 0.55218 |  0:00:00s\n",
      "epoch 1  | loss: 0.47135 | train_auc: 0.7855  | valid_auc: 0.78026 |  0:00:01s\n",
      "epoch 2  | loss: 0.43832 | train_auc: 0.84068 | valid_auc: 0.83877 |  0:00:01s\n",
      "epoch 3  | loss: 0.42936 | train_auc: 0.85798 | valid_auc: 0.85908 |  0:00:02s\n",
      "epoch 4  | loss: 0.42912 | train_auc: 0.86132 | valid_auc: 0.86244 |  0:00:02s\n",
      "epoch 5  | loss: 0.42774 | train_auc: 0.86323 | valid_auc: 0.86645 |  0:00:03s\n",
      "epoch 6  | loss: 0.42412 | train_auc: 0.87358 | valid_auc: 0.8745  |  0:00:03s\n",
      "epoch 7  | loss: 0.42041 | train_auc: 0.87666 | valid_auc: 0.87578 |  0:00:04s\n",
      "epoch 8  | loss: 0.41799 | train_auc: 0.88145 | valid_auc: 0.8804  |  0:00:05s\n",
      "epoch 9  | loss: 0.42008 | train_auc: 0.88135 | valid_auc: 0.88041 |  0:00:05s\n",
      "epoch 10 | loss: 0.42249 | train_auc: 0.88394 | valid_auc: 0.88257 |  0:00:06s\n",
      "epoch 11 | loss: 0.40797 | train_auc: 0.88549 | valid_auc: 0.88234 |  0:00:06s\n",
      "epoch 12 | loss: 0.41461 | train_auc: 0.88743 | valid_auc: 0.88311 |  0:00:07s\n",
      "epoch 13 | loss: 0.41558 | train_auc: 0.88787 | valid_auc: 0.88389 |  0:00:07s\n",
      "epoch 14 | loss: 0.41979 | train_auc: 0.88783 | valid_auc: 0.88507 |  0:00:08s\n",
      "epoch 15 | loss: 0.41184 | train_auc: 0.8879  | valid_auc: 0.88461 |  0:00:08s\n",
      "epoch 16 | loss: 0.41723 | train_auc: 0.88875 | valid_auc: 0.88434 |  0:00:09s\n",
      "epoch 17 | loss: 0.41032 | train_auc: 0.88803 | valid_auc: 0.88423 |  0:00:10s\n",
      "epoch 18 | loss: 0.419   | train_auc: 0.88893 | valid_auc: 0.88359 |  0:00:10s\n",
      "epoch 19 | loss: 0.41502 | train_auc: 0.88887 | valid_auc: 0.8838  |  0:00:11s\n",
      "epoch 20 | loss: 0.41861 | train_auc: 0.88963 | valid_auc: 0.88443 |  0:00:11s\n",
      "epoch 21 | loss: 0.41836 | train_auc: 0.88992 | valid_auc: 0.88463 |  0:00:12s\n",
      "epoch 22 | loss: 0.42082 | train_auc: 0.88931 | valid_auc: 0.88401 |  0:00:12s\n",
      "epoch 23 | loss: 0.41387 | train_auc: 0.88976 | valid_auc: 0.88353 |  0:00:13s\n",
      "epoch 24 | loss: 0.41619 | train_auc: 0.88991 | valid_auc: 0.88465 |  0:00:14s\n",
      "epoch 25 | loss: 0.41508 | train_auc: 0.88981 | valid_auc: 0.88279 |  0:00:14s\n",
      "epoch 26 | loss: 0.4158  | train_auc: 0.88953 | valid_auc: 0.88399 |  0:00:15s\n",
      "epoch 27 | loss: 0.41331 | train_auc: 0.88977 | valid_auc: 0.88473 |  0:00:15s\n",
      "epoch 28 | loss: 0.42224 | train_auc: 0.88965 | valid_auc: 0.88378 |  0:00:16s\n",
      "epoch 29 | loss: 0.41696 | train_auc: 0.88936 | valid_auc: 0.8845  |  0:00:16s\n",
      "epoch 30 | loss: 0.4169  | train_auc: 0.88995 | valid_auc: 0.88323 |  0:00:17s\n",
      "epoch 31 | loss: 0.41938 | train_auc: 0.89042 | valid_auc: 0.88483 |  0:00:17s\n",
      "epoch 32 | loss: 0.41078 | train_auc: 0.89014 | valid_auc: 0.88441 |  0:00:18s\n",
      "epoch 33 | loss: 0.41385 | train_auc: 0.89049 | valid_auc: 0.88481 |  0:00:18s\n",
      "epoch 34 | loss: 0.4047  | train_auc: 0.88953 | valid_auc: 0.8849  |  0:00:19s\n",
      "epoch 35 | loss: 0.41319 | train_auc: 0.88981 | valid_auc: 0.88513 |  0:00:20s\n",
      "epoch 36 | loss: 0.41422 | train_auc: 0.88956 | valid_auc: 0.88374 |  0:00:20s\n",
      "epoch 37 | loss: 0.41756 | train_auc: 0.88956 | valid_auc: 0.88401 |  0:00:21s\n",
      "epoch 38 | loss: 0.41214 | train_auc: 0.89014 | valid_auc: 0.88513 |  0:00:21s\n",
      "epoch 39 | loss: 0.42003 | train_auc: 0.88952 | valid_auc: 0.88421 |  0:00:22s\n",
      "epoch 40 | loss: 0.41943 | train_auc: 0.88947 | valid_auc: 0.88425 |  0:00:22s\n",
      "epoch 41 | loss: 0.41228 | train_auc: 0.88904 | valid_auc: 0.88311 |  0:00:23s\n",
      "epoch 42 | loss: 0.41474 | train_auc: 0.88997 | valid_auc: 0.88473 |  0:00:23s\n",
      "epoch 43 | loss: 0.41881 | train_auc: 0.89024 | valid_auc: 0.88487 |  0:00:24s\n",
      "epoch 44 | loss: 0.41493 | train_auc: 0.89003 | valid_auc: 0.88475 |  0:00:25s\n",
      "epoch 45 | loss: 0.4218  | train_auc: 0.88899 | valid_auc: 0.88425 |  0:00:25s\n",
      "epoch 46 | loss: 0.41339 | train_auc: 0.88943 | valid_auc: 0.88303 |  0:00:26s\n",
      "epoch 47 | loss: 0.41321 | train_auc: 0.89012 | valid_auc: 0.88371 |  0:00:26s\n",
      "epoch 48 | loss: 0.4259  | train_auc: 0.88997 | valid_auc: 0.88441 |  0:00:27s\n",
      "epoch 49 | loss: 0.41359 | train_auc: 0.88936 | valid_auc: 0.88349 |  0:00:27s\n",
      "epoch 50 | loss: 0.41058 | train_auc: 0.89007 | valid_auc: 0.88509 |  0:00:28s\n",
      "epoch 51 | loss: 0.41405 | train_auc: 0.88951 | valid_auc: 0.88422 |  0:00:28s\n",
      "epoch 52 | loss: 0.40599 | train_auc: 0.88982 | valid_auc: 0.88506 |  0:00:29s\n",
      "epoch 53 | loss: 0.41577 | train_auc: 0.88972 | valid_auc: 0.88354 |  0:00:30s\n",
      "epoch 54 | loss: 0.412   | train_auc: 0.88918 | valid_auc: 0.88435 |  0:00:30s\n",
      "epoch 55 | loss: 0.41654 | train_auc: 0.88916 | valid_auc: 0.88375 |  0:00:31s\n",
      "epoch 56 | loss: 0.4111  | train_auc: 0.88926 | valid_auc: 0.88497 |  0:00:31s\n",
      "epoch 57 | loss: 0.40959 | train_auc: 0.88948 | valid_auc: 0.88374 |  0:00:32s\n",
      "epoch 58 | loss: 0.41698 | train_auc: 0.8899  | valid_auc: 0.88479 |  0:00:32s\n",
      "epoch 59 | loss: 0.41862 | train_auc: 0.88996 | valid_auc: 0.88448 |  0:00:33s\n",
      "epoch 60 | loss: 0.41987 | train_auc: 0.89007 | valid_auc: 0.88471 |  0:00:33s\n",
      "epoch 61 | loss: 0.4181  | train_auc: 0.8901  | valid_auc: 0.88382 |  0:00:34s\n",
      "epoch 62 | loss: 0.40898 | train_auc: 0.88927 | valid_auc: 0.88299 |  0:00:34s\n",
      "epoch 63 | loss: 0.417   | train_auc: 0.88977 | valid_auc: 0.88336 |  0:00:35s\n",
      "epoch 64 | loss: 0.42053 | train_auc: 0.88996 | valid_auc: 0.88521 |  0:00:36s\n",
      "epoch 65 | loss: 0.41469 | train_auc: 0.88993 | valid_auc: 0.88362 |  0:00:36s\n",
      "epoch 66 | loss: 0.41812 | train_auc: 0.88983 | valid_auc: 0.8848  |  0:00:37s\n",
      "epoch 67 | loss: 0.40955 | train_auc: 0.89009 | valid_auc: 0.88462 |  0:00:37s\n",
      "epoch 68 | loss: 0.41363 | train_auc: 0.88978 | valid_auc: 0.88503 |  0:00:38s\n",
      "epoch 69 | loss: 0.42233 | train_auc: 0.89001 | valid_auc: 0.88438 |  0:00:38s\n",
      "epoch 70 | loss: 0.42257 | train_auc: 0.88967 | valid_auc: 0.88427 |  0:00:39s\n",
      "epoch 71 | loss: 0.42098 | train_auc: 0.88982 | valid_auc: 0.88431 |  0:00:40s\n",
      "epoch 72 | loss: 0.41412 | train_auc: 0.88996 | valid_auc: 0.88502 |  0:00:40s\n",
      "epoch 73 | loss: 0.41384 | train_auc: 0.88987 | valid_auc: 0.88403 |  0:00:41s\n",
      "epoch 74 | loss: 0.4168  | train_auc: 0.88944 | valid_auc: 0.8849  |  0:00:41s\n",
      "epoch 75 | loss: 0.40752 | train_auc: 0.88986 | valid_auc: 0.88504 |  0:00:42s\n",
      "epoch 76 | loss: 0.41756 | train_auc: 0.88943 | valid_auc: 0.88497 |  0:00:42s\n",
      "epoch 77 | loss: 0.40902 | train_auc: 0.88875 | valid_auc: 0.88259 |  0:00:43s\n",
      "epoch 78 | loss: 0.41179 | train_auc: 0.88971 | valid_auc: 0.8841  |  0:00:43s\n",
      "epoch 79 | loss: 0.40914 | train_auc: 0.88993 | valid_auc: 0.88454 |  0:00:44s\n",
      "epoch 80 | loss: 0.40982 | train_auc: 0.88885 | valid_auc: 0.8824  |  0:00:44s\n",
      "epoch 81 | loss: 0.41678 | train_auc: 0.88997 | valid_auc: 0.88488 |  0:00:45s\n",
      "epoch 82 | loss: 0.41859 | train_auc: 0.88918 | valid_auc: 0.88472 |  0:00:46s\n",
      "epoch 83 | loss: 0.42087 | train_auc: 0.88995 | valid_auc: 0.88495 |  0:00:46s\n",
      "epoch 84 | loss: 0.41668 | train_auc: 0.88991 | valid_auc: 0.88477 |  0:00:47s\n",
      "epoch 85 | loss: 0.41964 | train_auc: 0.8901  | valid_auc: 0.88523 |  0:00:47s\n",
      "epoch 86 | loss: 0.42188 | train_auc: 0.88953 | valid_auc: 0.88455 |  0:00:48s\n",
      "epoch 87 | loss: 0.41542 | train_auc: 0.88986 | valid_auc: 0.88465 |  0:00:48s\n",
      "epoch 88 | loss: 0.4153  | train_auc: 0.89007 | valid_auc: 0.88397 |  0:00:49s\n",
      "epoch 89 | loss: 0.41695 | train_auc: 0.88923 | valid_auc: 0.88427 |  0:00:49s\n",
      "epoch 90 | loss: 0.41246 | train_auc: 0.88998 | valid_auc: 0.88511 |  0:00:50s\n",
      "epoch 91 | loss: 0.41265 | train_auc: 0.89027 | valid_auc: 0.88467 |  0:00:51s\n",
      "epoch 92 | loss: 0.40996 | train_auc: 0.88985 | valid_auc: 0.8849  |  0:00:51s\n",
      "epoch 93 | loss: 0.41162 | train_auc: 0.88993 | valid_auc: 0.88528 |  0:00:52s\n",
      "epoch 94 | loss: 0.41911 | train_auc: 0.8899  | valid_auc: 0.88483 |  0:00:52s\n",
      "epoch 95 | loss: 0.4147  | train_auc: 0.89009 | valid_auc: 0.88565 |  0:00:53s\n",
      "epoch 96 | loss: 0.41331 | train_auc: 0.89031 | valid_auc: 0.88499 |  0:00:53s\n",
      "epoch 97 | loss: 0.40488 | train_auc: 0.8904  | valid_auc: 0.88431 |  0:00:54s\n",
      "epoch 98 | loss: 0.41543 | train_auc: 0.89063 | valid_auc: 0.88447 |  0:00:54s\n",
      "epoch 99 | loss: 0.40465 | train_auc: 0.89076 | valid_auc: 0.88457 |  0:00:55s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 95 and best_valid_auc = 0.88565\n",
      "sampling:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.72443 | train_auc: 0.57727 | valid_auc: 0.5737  |  0:00:00s\n",
      "epoch 1  | loss: 0.49279 | train_auc: 0.7396  | valid_auc: 0.7427  |  0:00:01s\n",
      "epoch 2  | loss: 0.46417 | train_auc: 0.82071 | valid_auc: 0.82126 |  0:00:01s\n",
      "epoch 3  | loss: 0.4411  | train_auc: 0.84404 | valid_auc: 0.84779 |  0:00:02s\n",
      "epoch 4  | loss: 0.43254 | train_auc: 0.863   | valid_auc: 0.86874 |  0:00:02s\n",
      "epoch 5  | loss: 0.42497 | train_auc: 0.86388 | valid_auc: 0.8671  |  0:00:03s\n",
      "epoch 6  | loss: 0.42842 | train_auc: 0.87503 | valid_auc: 0.87962 |  0:00:03s\n",
      "epoch 7  | loss: 0.42589 | train_auc: 0.87595 | valid_auc: 0.8781  |  0:00:04s\n",
      "epoch 8  | loss: 0.42662 | train_auc: 0.8785  | valid_auc: 0.88089 |  0:00:05s\n",
      "epoch 9  | loss: 0.42462 | train_auc: 0.88146 | valid_auc: 0.88433 |  0:00:05s\n",
      "epoch 10 | loss: 0.41718 | train_auc: 0.8804  | valid_auc: 0.88338 |  0:00:06s\n",
      "epoch 11 | loss: 0.42411 | train_auc: 0.88345 | valid_auc: 0.88817 |  0:00:06s\n",
      "epoch 12 | loss: 0.42722 | train_auc: 0.88227 | valid_auc: 0.8843  |  0:00:07s\n",
      "epoch 13 | loss: 0.42758 | train_auc: 0.88558 | valid_auc: 0.89037 |  0:00:07s\n",
      "epoch 14 | loss: 0.41712 | train_auc: 0.88639 | valid_auc: 0.89083 |  0:00:08s\n",
      "epoch 15 | loss: 0.42425 | train_auc: 0.88606 | valid_auc: 0.89015 |  0:00:08s\n",
      "epoch 16 | loss: 0.42464 | train_auc: 0.88696 | valid_auc: 0.89016 |  0:00:09s\n",
      "epoch 17 | loss: 0.41116 | train_auc: 0.88614 | valid_auc: 0.88966 |  0:00:10s\n",
      "epoch 18 | loss: 0.42354 | train_auc: 0.88788 | valid_auc: 0.89231 |  0:00:10s\n",
      "epoch 19 | loss: 0.42688 | train_auc: 0.88687 | valid_auc: 0.89204 |  0:00:11s\n",
      "epoch 20 | loss: 0.42656 | train_auc: 0.88762 | valid_auc: 0.89319 |  0:00:11s\n",
      "epoch 21 | loss: 0.42814 | train_auc: 0.88779 | valid_auc: 0.89383 |  0:00:12s\n",
      "epoch 22 | loss: 0.41819 | train_auc: 0.88819 | valid_auc: 0.89357 |  0:00:12s\n",
      "epoch 23 | loss: 0.42181 | train_auc: 0.88836 | valid_auc: 0.89337 |  0:00:13s\n",
      "epoch 24 | loss: 0.42014 | train_auc: 0.88811 | valid_auc: 0.8931  |  0:00:13s\n",
      "epoch 25 | loss: 0.41869 | train_auc: 0.88818 | valid_auc: 0.89417 |  0:00:14s\n",
      "epoch 26 | loss: 0.4296  | train_auc: 0.88801 | valid_auc: 0.89234 |  0:00:14s\n",
      "epoch 27 | loss: 0.41965 | train_auc: 0.88716 | valid_auc: 0.89045 |  0:00:15s\n",
      "epoch 28 | loss: 0.4238  | train_auc: 0.88819 | valid_auc: 0.89132 |  0:00:16s\n",
      "epoch 29 | loss: 0.42211 | train_auc: 0.88804 | valid_auc: 0.892   |  0:00:16s\n",
      "epoch 30 | loss: 0.40954 | train_auc: 0.88769 | valid_auc: 0.89184 |  0:00:17s\n",
      "epoch 31 | loss: 0.41866 | train_auc: 0.88818 | valid_auc: 0.89301 |  0:00:17s\n",
      "epoch 32 | loss: 0.41809 | train_auc: 0.88868 | valid_auc: 0.89341 |  0:00:18s\n",
      "epoch 33 | loss: 0.41799 | train_auc: 0.88785 | valid_auc: 0.89285 |  0:00:18s\n",
      "epoch 34 | loss: 0.41694 | train_auc: 0.88904 | valid_auc: 0.89282 |  0:00:19s\n",
      "epoch 35 | loss: 0.41642 | train_auc: 0.88752 | valid_auc: 0.89155 |  0:00:20s\n",
      "epoch 36 | loss: 0.41218 | train_auc: 0.88863 | valid_auc: 0.89246 |  0:00:20s\n",
      "epoch 37 | loss: 0.41653 | train_auc: 0.88778 | valid_auc: 0.8916  |  0:00:21s\n",
      "epoch 38 | loss: 0.42527 | train_auc: 0.88877 | valid_auc: 0.89419 |  0:00:21s\n",
      "epoch 39 | loss: 0.41035 | train_auc: 0.8888  | valid_auc: 0.89231 |  0:00:22s\n",
      "epoch 40 | loss: 0.42006 | train_auc: 0.88887 | valid_auc: 0.89298 |  0:00:22s\n",
      "epoch 41 | loss: 0.41454 | train_auc: 0.88844 | valid_auc: 0.894   |  0:00:23s\n",
      "epoch 42 | loss: 0.42234 | train_auc: 0.88931 | valid_auc: 0.89235 |  0:00:23s\n",
      "epoch 43 | loss: 0.42085 | train_auc: 0.8896  | valid_auc: 0.89291 |  0:00:24s\n",
      "epoch 44 | loss: 0.41159 | train_auc: 0.88858 | valid_auc: 0.89352 |  0:00:24s\n",
      "epoch 45 | loss: 0.41948 | train_auc: 0.88903 | valid_auc: 0.89302 |  0:00:25s\n",
      "epoch 46 | loss: 0.42506 | train_auc: 0.88897 | valid_auc: 0.89255 |  0:00:26s\n",
      "epoch 47 | loss: 0.41307 | train_auc: 0.8891  | valid_auc: 0.89301 |  0:00:26s\n",
      "epoch 48 | loss: 0.42297 | train_auc: 0.88961 | valid_auc: 0.8922  |  0:00:27s\n",
      "epoch 49 | loss: 0.4177  | train_auc: 0.88985 | valid_auc: 0.89261 |  0:00:27s\n",
      "epoch 50 | loss: 0.41213 | train_auc: 0.8874  | valid_auc: 0.8925  |  0:00:28s\n",
      "epoch 51 | loss: 0.41605 | train_auc: 0.88912 | valid_auc: 0.8928  |  0:00:28s\n",
      "epoch 52 | loss: 0.42027 | train_auc: 0.88942 | valid_auc: 0.89188 |  0:00:29s\n",
      "epoch 53 | loss: 0.42496 | train_auc: 0.8893  | valid_auc: 0.89231 |  0:00:30s\n",
      "epoch 54 | loss: 0.4216  | train_auc: 0.88943 | valid_auc: 0.89212 |  0:00:30s\n",
      "epoch 55 | loss: 0.41799 | train_auc: 0.8889  | valid_auc: 0.89222 |  0:00:31s\n",
      "epoch 56 | loss: 0.4162  | train_auc: 0.889   | valid_auc: 0.89227 |  0:00:31s\n",
      "epoch 57 | loss: 0.41457 | train_auc: 0.88947 | valid_auc: 0.89285 |  0:00:32s\n",
      "epoch 58 | loss: 0.42761 | train_auc: 0.8894  | valid_auc: 0.89305 |  0:00:32s\n",
      "epoch 59 | loss: 0.41836 | train_auc: 0.8889  | valid_auc: 0.89233 |  0:00:33s\n",
      "epoch 60 | loss: 0.41496 | train_auc: 0.88865 | valid_auc: 0.89175 |  0:00:33s\n",
      "epoch 61 | loss: 0.41429 | train_auc: 0.88951 | valid_auc: 0.89286 |  0:00:34s\n",
      "epoch 62 | loss: 0.4141  | train_auc: 0.88921 | valid_auc: 0.89173 |  0:00:35s\n",
      "epoch 63 | loss: 0.42874 | train_auc: 0.88946 | valid_auc: 0.89228 |  0:00:35s\n",
      "epoch 64 | loss: 0.41457 | train_auc: 0.88924 | valid_auc: 0.89245 |  0:00:36s\n",
      "epoch 65 | loss: 0.42159 | train_auc: 0.88935 | valid_auc: 0.89173 |  0:00:36s\n",
      "epoch 66 | loss: 0.4154  | train_auc: 0.88939 | valid_auc: 0.89148 |  0:00:37s\n",
      "epoch 67 | loss: 0.41261 | train_auc: 0.88948 | valid_auc: 0.89192 |  0:00:37s\n",
      "epoch 68 | loss: 0.41838 | train_auc: 0.88917 | valid_auc: 0.89411 |  0:00:38s\n",
      "epoch 69 | loss: 0.41366 | train_auc: 0.88996 | valid_auc: 0.89373 |  0:00:38s\n",
      "epoch 70 | loss: 0.42603 | train_auc: 0.88986 | valid_auc: 0.89356 |  0:00:39s\n",
      "epoch 71 | loss: 0.42121 | train_auc: 0.88954 | valid_auc: 0.89334 |  0:00:40s\n",
      "epoch 72 | loss: 0.41411 | train_auc: 0.88943 | valid_auc: 0.89379 |  0:00:40s\n",
      "epoch 73 | loss: 0.41299 | train_auc: 0.88811 | valid_auc: 0.89229 |  0:00:41s\n",
      "epoch 74 | loss: 0.41645 | train_auc: 0.88883 | valid_auc: 0.89189 |  0:00:41s\n",
      "epoch 75 | loss: 0.42161 | train_auc: 0.88946 | valid_auc: 0.89353 |  0:00:42s\n",
      "epoch 76 | loss: 0.42431 | train_auc: 0.88908 | valid_auc: 0.89316 |  0:00:42s\n",
      "epoch 77 | loss: 0.41417 | train_auc: 0.88973 | valid_auc: 0.89389 |  0:00:43s\n",
      "epoch 78 | loss: 0.41685 | train_auc: 0.88954 | valid_auc: 0.89293 |  0:00:43s\n",
      "epoch 79 | loss: 0.41737 | train_auc: 0.88847 | valid_auc: 0.89169 |  0:00:44s\n",
      "epoch 80 | loss: 0.41978 | train_auc: 0.88881 | valid_auc: 0.8922  |  0:00:45s\n",
      "epoch 81 | loss: 0.41893 | train_auc: 0.88954 | valid_auc: 0.89314 |  0:00:45s\n",
      "epoch 82 | loss: 0.41146 | train_auc: 0.88921 | valid_auc: 0.89301 |  0:00:46s\n",
      "epoch 83 | loss: 0.41526 | train_auc: 0.88833 | valid_auc: 0.89384 |  0:00:46s\n",
      "epoch 84 | loss: 0.4194  | train_auc: 0.88911 | valid_auc: 0.8945  |  0:00:47s\n",
      "epoch 85 | loss: 0.42183 | train_auc: 0.88917 | valid_auc: 0.89503 |  0:00:47s\n",
      "epoch 86 | loss: 0.41785 | train_auc: 0.88945 | valid_auc: 0.89444 |  0:00:48s\n",
      "epoch 87 | loss: 0.4212  | train_auc: 0.88889 | valid_auc: 0.89319 |  0:00:48s\n",
      "epoch 88 | loss: 0.42309 | train_auc: 0.88896 | valid_auc: 0.89432 |  0:00:49s\n",
      "epoch 89 | loss: 0.42184 | train_auc: 0.88948 | valid_auc: 0.89422 |  0:00:50s\n",
      "epoch 90 | loss: 0.42633 | train_auc: 0.88834 | valid_auc: 0.89217 |  0:00:50s\n",
      "epoch 91 | loss: 0.41932 | train_auc: 0.88839 | valid_auc: 0.89412 |  0:00:51s\n",
      "epoch 92 | loss: 0.41801 | train_auc: 0.88952 | valid_auc: 0.89385 |  0:00:51s\n",
      "epoch 93 | loss: 0.42245 | train_auc: 0.88957 | valid_auc: 0.89311 |  0:00:52s\n",
      "epoch 94 | loss: 0.41724 | train_auc: 0.88895 | valid_auc: 0.8934  |  0:00:52s\n",
      "epoch 95 | loss: 0.41735 | train_auc: 0.88966 | valid_auc: 0.89291 |  0:00:53s\n",
      "epoch 96 | loss: 0.4092  | train_auc: 0.88948 | valid_auc: 0.89292 |  0:00:53s\n",
      "epoch 97 | loss: 0.40846 | train_auc: 0.88887 | valid_auc: 0.89417 |  0:00:54s\n",
      "epoch 98 | loss: 0.41672 | train_auc: 0.88935 | valid_auc: 0.89351 |  0:00:54s\n",
      "epoch 99 | loss: 0.41771 | train_auc: 0.88931 | valid_auc: 0.89288 |  0:00:55s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 85 and best_valid_auc = 0.89503\n",
      "sampling:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.70806 | train_auc: 0.52955 | valid_auc: 0.52022 |  0:00:00s\n",
      "epoch 1  | loss: 0.4932  | train_auc: 0.64513 | valid_auc: 0.63505 |  0:00:01s\n",
      "epoch 2  | loss: 0.45759 | train_auc: 0.6919  | valid_auc: 0.67986 |  0:00:01s\n",
      "epoch 3  | loss: 0.44817 | train_auc: 0.78518 | valid_auc: 0.77664 |  0:00:02s\n",
      "epoch 4  | loss: 0.4376  | train_auc: 0.85176 | valid_auc: 0.85241 |  0:00:02s\n",
      "epoch 5  | loss: 0.4388  | train_auc: 0.86331 | valid_auc: 0.86577 |  0:00:03s\n",
      "epoch 6  | loss: 0.43702 | train_auc: 0.86437 | valid_auc: 0.86585 |  0:00:03s\n",
      "epoch 7  | loss: 0.43722 | train_auc: 0.86646 | valid_auc: 0.8698  |  0:00:04s\n",
      "epoch 8  | loss: 0.4365  | train_auc: 0.8751  | valid_auc: 0.88191 |  0:00:05s\n",
      "epoch 9  | loss: 0.42797 | train_auc: 0.87456 | valid_auc: 0.88009 |  0:00:05s\n",
      "epoch 10 | loss: 0.43419 | train_auc: 0.88078 | valid_auc: 0.88669 |  0:00:06s\n",
      "epoch 11 | loss: 0.42931 | train_auc: 0.88226 | valid_auc: 0.88836 |  0:00:06s\n",
      "epoch 12 | loss: 0.42283 | train_auc: 0.88109 | valid_auc: 0.88768 |  0:00:07s\n",
      "epoch 13 | loss: 0.43173 | train_auc: 0.8821  | valid_auc: 0.88862 |  0:00:07s\n",
      "epoch 14 | loss: 0.42511 | train_auc: 0.88424 | valid_auc: 0.89197 |  0:00:08s\n",
      "epoch 15 | loss: 0.42607 | train_auc: 0.8835  | valid_auc: 0.89055 |  0:00:08s\n",
      "epoch 16 | loss: 0.41706 | train_auc: 0.88468 | valid_auc: 0.89088 |  0:00:09s\n",
      "epoch 17 | loss: 0.42686 | train_auc: 0.88419 | valid_auc: 0.8894  |  0:00:10s\n",
      "epoch 18 | loss: 0.42595 | train_auc: 0.88527 | valid_auc: 0.89107 |  0:00:10s\n",
      "epoch 19 | loss: 0.42166 | train_auc: 0.88513 | valid_auc: 0.89168 |  0:00:11s\n",
      "epoch 20 | loss: 0.43118 | train_auc: 0.88502 | valid_auc: 0.89262 |  0:00:11s\n",
      "epoch 21 | loss: 0.427   | train_auc: 0.88468 | valid_auc: 0.89093 |  0:00:12s\n",
      "epoch 22 | loss: 0.42144 | train_auc: 0.88476 | valid_auc: 0.89113 |  0:00:12s\n",
      "epoch 23 | loss: 0.42884 | train_auc: 0.88548 | valid_auc: 0.8921  |  0:00:13s\n",
      "epoch 24 | loss: 0.42836 | train_auc: 0.88545 | valid_auc: 0.8924  |  0:00:13s\n",
      "epoch 25 | loss: 0.42719 | train_auc: 0.885   | valid_auc: 0.89205 |  0:00:14s\n",
      "epoch 26 | loss: 0.4256  | train_auc: 0.88572 | valid_auc: 0.89232 |  0:00:14s\n",
      "epoch 27 | loss: 0.42086 | train_auc: 0.88496 | valid_auc: 0.89191 |  0:00:15s\n",
      "epoch 28 | loss: 0.43131 | train_auc: 0.88597 | valid_auc: 0.89233 |  0:00:15s\n",
      "epoch 29 | loss: 0.4288  | train_auc: 0.88557 | valid_auc: 0.89195 |  0:00:16s\n",
      "epoch 30 | loss: 0.42061 | train_auc: 0.88532 | valid_auc: 0.89108 |  0:00:17s\n",
      "epoch 31 | loss: 0.42823 | train_auc: 0.88606 | valid_auc: 0.89146 |  0:00:17s\n",
      "epoch 32 | loss: 0.42226 | train_auc: 0.88603 | valid_auc: 0.89165 |  0:00:18s\n",
      "epoch 33 | loss: 0.41965 | train_auc: 0.88524 | valid_auc: 0.89037 |  0:00:18s\n",
      "epoch 34 | loss: 0.42191 | train_auc: 0.88617 | valid_auc: 0.89162 |  0:00:19s\n",
      "epoch 35 | loss: 0.41958 | train_auc: 0.88587 | valid_auc: 0.89096 |  0:00:19s\n",
      "epoch 36 | loss: 0.42645 | train_auc: 0.88628 | valid_auc: 0.89122 |  0:00:20s\n",
      "epoch 37 | loss: 0.42046 | train_auc: 0.88648 | valid_auc: 0.8922  |  0:00:21s\n",
      "epoch 38 | loss: 0.41784 | train_auc: 0.88625 | valid_auc: 0.89235 |  0:00:21s\n",
      "epoch 39 | loss: 0.42205 | train_auc: 0.88617 | valid_auc: 0.89227 |  0:00:22s\n",
      "epoch 40 | loss: 0.41707 | train_auc: 0.8864  | valid_auc: 0.89232 |  0:00:22s\n",
      "epoch 41 | loss: 0.42114 | train_auc: 0.88634 | valid_auc: 0.89214 |  0:00:23s\n",
      "epoch 42 | loss: 0.42147 | train_auc: 0.88641 | valid_auc: 0.89185 |  0:00:23s\n",
      "epoch 43 | loss: 0.4252  | train_auc: 0.88613 | valid_auc: 0.89062 |  0:00:24s\n",
      "epoch 44 | loss: 0.43573 | train_auc: 0.88682 | valid_auc: 0.89257 |  0:00:24s\n",
      "epoch 45 | loss: 0.42868 | train_auc: 0.88636 | valid_auc: 0.8922  |  0:00:25s\n",
      "epoch 46 | loss: 0.42743 | train_auc: 0.88671 | valid_auc: 0.89105 |  0:00:26s\n",
      "epoch 47 | loss: 0.42724 | train_auc: 0.88661 | valid_auc: 0.89171 |  0:00:26s\n",
      "epoch 48 | loss: 0.42578 | train_auc: 0.88664 | valid_auc: 0.89175 |  0:00:27s\n",
      "epoch 49 | loss: 0.42073 | train_auc: 0.88661 | valid_auc: 0.89178 |  0:00:27s\n",
      "epoch 50 | loss: 0.41972 | train_auc: 0.88664 | valid_auc: 0.89272 |  0:00:28s\n",
      "epoch 51 | loss: 0.42257 | train_auc: 0.88665 | valid_auc: 0.89209 |  0:00:28s\n",
      "epoch 52 | loss: 0.42341 | train_auc: 0.88661 | valid_auc: 0.89124 |  0:00:29s\n",
      "epoch 53 | loss: 0.42898 | train_auc: 0.88669 | valid_auc: 0.89136 |  0:00:29s\n",
      "epoch 54 | loss: 0.41653 | train_auc: 0.88662 | valid_auc: 0.8912  |  0:00:30s\n",
      "epoch 55 | loss: 0.41831 | train_auc: 0.88629 | valid_auc: 0.8903  |  0:00:31s\n",
      "epoch 56 | loss: 0.4157  | train_auc: 0.88529 | valid_auc: 0.89152 |  0:00:31s\n",
      "epoch 57 | loss: 0.4237  | train_auc: 0.88577 | valid_auc: 0.89164 |  0:00:32s\n",
      "epoch 58 | loss: 0.42194 | train_auc: 0.88625 | valid_auc: 0.89191 |  0:00:32s\n",
      "epoch 59 | loss: 0.41884 | train_auc: 0.88652 | valid_auc: 0.89242 |  0:00:33s\n",
      "epoch 60 | loss: 0.42211 | train_auc: 0.88662 | valid_auc: 0.89123 |  0:00:33s\n",
      "epoch 61 | loss: 0.42229 | train_auc: 0.88624 | valid_auc: 0.89128 |  0:00:34s\n",
      "epoch 62 | loss: 0.42217 | train_auc: 0.88633 | valid_auc: 0.89115 |  0:00:34s\n",
      "epoch 63 | loss: 0.42806 | train_auc: 0.88637 | valid_auc: 0.89058 |  0:00:35s\n",
      "epoch 64 | loss: 0.42281 | train_auc: 0.88689 | valid_auc: 0.89171 |  0:00:35s\n",
      "epoch 65 | loss: 0.41499 | train_auc: 0.88519 | valid_auc: 0.89031 |  0:00:36s\n",
      "epoch 66 | loss: 0.42995 | train_auc: 0.88678 | valid_auc: 0.89221 |  0:00:37s\n",
      "epoch 67 | loss: 0.42708 | train_auc: 0.88677 | valid_auc: 0.8918  |  0:00:37s\n",
      "epoch 68 | loss: 0.42377 | train_auc: 0.88647 | valid_auc: 0.89045 |  0:00:38s\n",
      "epoch 69 | loss: 0.42327 | train_auc: 0.88669 | valid_auc: 0.89245 |  0:00:38s\n",
      "epoch 70 | loss: 0.42523 | train_auc: 0.88631 | valid_auc: 0.89188 |  0:00:39s\n",
      "epoch 71 | loss: 0.41625 | train_auc: 0.88649 | valid_auc: 0.89204 |  0:00:39s\n",
      "epoch 72 | loss: 0.41491 | train_auc: 0.88692 | valid_auc: 0.89235 |  0:00:40s\n",
      "epoch 73 | loss: 0.41899 | train_auc: 0.88671 | valid_auc: 0.89089 |  0:00:40s\n",
      "epoch 74 | loss: 0.41913 | train_auc: 0.88679 | valid_auc: 0.89163 |  0:00:41s\n",
      "epoch 75 | loss: 0.4076  | train_auc: 0.88688 | valid_auc: 0.89209 |  0:00:42s\n",
      "epoch 76 | loss: 0.40942 | train_auc: 0.88684 | valid_auc: 0.8918  |  0:00:42s\n",
      "epoch 77 | loss: 0.42328 | train_auc: 0.88664 | valid_auc: 0.89163 |  0:00:43s\n",
      "epoch 78 | loss: 0.42055 | train_auc: 0.88682 | valid_auc: 0.89319 |  0:00:43s\n",
      "epoch 79 | loss: 0.42877 | train_auc: 0.88642 | valid_auc: 0.89195 |  0:00:44s\n",
      "epoch 80 | loss: 0.41816 | train_auc: 0.88721 | valid_auc: 0.89272 |  0:00:44s\n",
      "epoch 81 | loss: 0.41935 | train_auc: 0.88657 | valid_auc: 0.89258 |  0:00:45s\n",
      "epoch 82 | loss: 0.41865 | train_auc: 0.88673 | valid_auc: 0.89253 |  0:00:45s\n",
      "epoch 83 | loss: 0.41701 | train_auc: 0.88692 | valid_auc: 0.89229 |  0:00:46s\n",
      "epoch 84 | loss: 0.41677 | train_auc: 0.88596 | valid_auc: 0.89153 |  0:00:46s\n",
      "epoch 85 | loss: 0.43054 | train_auc: 0.88623 | valid_auc: 0.8914  |  0:00:47s\n",
      "epoch 86 | loss: 0.41989 | train_auc: 0.88609 | valid_auc: 0.89157 |  0:00:48s\n",
      "epoch 87 | loss: 0.4198  | train_auc: 0.88699 | valid_auc: 0.8915  |  0:00:48s\n",
      "epoch 88 | loss: 0.42474 | train_auc: 0.88643 | valid_auc: 0.89186 |  0:00:49s\n",
      "epoch 89 | loss: 0.4205  | train_auc: 0.88616 | valid_auc: 0.89185 |  0:00:49s\n",
      "epoch 90 | loss: 0.43194 | train_auc: 0.8865  | valid_auc: 0.89155 |  0:00:50s\n",
      "epoch 91 | loss: 0.42023 | train_auc: 0.88654 | valid_auc: 0.89147 |  0:00:50s\n",
      "epoch 92 | loss: 0.41903 | train_auc: 0.88666 | valid_auc: 0.89218 |  0:00:51s\n",
      "epoch 93 | loss: 0.41455 | train_auc: 0.88693 | valid_auc: 0.89184 |  0:00:51s\n",
      "epoch 94 | loss: 0.42585 | train_auc: 0.88704 | valid_auc: 0.89143 |  0:00:52s\n",
      "epoch 95 | loss: 0.42592 | train_auc: 0.88682 | valid_auc: 0.89298 |  0:00:52s\n",
      "epoch 96 | loss: 0.41998 | train_auc: 0.88701 | valid_auc: 0.89269 |  0:00:53s\n",
      "epoch 97 | loss: 0.42068 | train_auc: 0.88612 | valid_auc: 0.89209 |  0:00:54s\n",
      "epoch 98 | loss: 0.41172 | train_auc: 0.88716 | valid_auc: 0.89323 |  0:00:54s\n",
      "epoch 99 | loss: 0.41949 | train_auc: 0.88703 | valid_auc: 0.89243 |  0:00:55s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_valid_auc = 0.89323\n",
      "sampling:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.70757 | train_auc: 0.59026 | valid_auc: 0.58452 |  0:00:00s\n",
      "epoch 1  | loss: 0.46325 | train_auc: 0.79487 | valid_auc: 0.78511 |  0:00:01s\n",
      "epoch 2  | loss: 0.42753 | train_auc: 0.81281 | valid_auc: 0.80848 |  0:00:01s\n",
      "epoch 3  | loss: 0.42228 | train_auc: 0.85798 | valid_auc: 0.85172 |  0:00:02s\n",
      "epoch 4  | loss: 0.42155 | train_auc: 0.87008 | valid_auc: 0.86482 |  0:00:02s\n",
      "epoch 5  | loss: 0.43134 | train_auc: 0.87311 | valid_auc: 0.86789 |  0:00:03s\n",
      "epoch 6  | loss: 0.42688 | train_auc: 0.87799 | valid_auc: 0.87151 |  0:00:03s\n",
      "epoch 7  | loss: 0.42343 | train_auc: 0.87735 | valid_auc: 0.87226 |  0:00:04s\n",
      "epoch 8  | loss: 0.41525 | train_auc: 0.88111 | valid_auc: 0.87686 |  0:00:04s\n",
      "epoch 9  | loss: 0.41695 | train_auc: 0.88388 | valid_auc: 0.87994 |  0:00:05s\n",
      "epoch 10 | loss: 0.41588 | train_auc: 0.88429 | valid_auc: 0.87952 |  0:00:06s\n",
      "epoch 11 | loss: 0.42285 | train_auc: 0.88593 | valid_auc: 0.88097 |  0:00:06s\n",
      "epoch 12 | loss: 0.41699 | train_auc: 0.88736 | valid_auc: 0.88412 |  0:00:07s\n",
      "epoch 13 | loss: 0.42775 | train_auc: 0.88632 | valid_auc: 0.8826  |  0:00:07s\n",
      "epoch 14 | loss: 0.42345 | train_auc: 0.88724 | valid_auc: 0.88371 |  0:00:08s\n",
      "epoch 15 | loss: 0.42047 | train_auc: 0.88799 | valid_auc: 0.88269 |  0:00:08s\n",
      "epoch 16 | loss: 0.41533 | train_auc: 0.88862 | valid_auc: 0.88303 |  0:00:09s\n",
      "epoch 17 | loss: 0.41088 | train_auc: 0.88875 | valid_auc: 0.88272 |  0:00:09s\n",
      "epoch 18 | loss: 0.41615 | train_auc: 0.88823 | valid_auc: 0.88269 |  0:00:10s\n",
      "epoch 19 | loss: 0.42066 | train_auc: 0.88919 | valid_auc: 0.88275 |  0:00:11s\n",
      "epoch 20 | loss: 0.42352 | train_auc: 0.8888  | valid_auc: 0.88249 |  0:00:11s\n",
      "epoch 21 | loss: 0.41998 | train_auc: 0.88875 | valid_auc: 0.88272 |  0:00:12s\n",
      "epoch 22 | loss: 0.41428 | train_auc: 0.88931 | valid_auc: 0.88329 |  0:00:12s\n",
      "epoch 23 | loss: 0.41786 | train_auc: 0.88951 | valid_auc: 0.88349 |  0:00:13s\n",
      "epoch 24 | loss: 0.42065 | train_auc: 0.88996 | valid_auc: 0.8838  |  0:00:13s\n",
      "epoch 25 | loss: 0.41323 | train_auc: 0.88908 | valid_auc: 0.88366 |  0:00:14s\n",
      "epoch 26 | loss: 0.41256 | train_auc: 0.89005 | valid_auc: 0.88386 |  0:00:14s\n",
      "epoch 27 | loss: 0.40911 | train_auc: 0.88975 | valid_auc: 0.88353 |  0:00:15s\n",
      "epoch 28 | loss: 0.41604 | train_auc: 0.88997 | valid_auc: 0.88407 |  0:00:16s\n",
      "epoch 29 | loss: 0.41667 | train_auc: 0.88979 | valid_auc: 0.88312 |  0:00:16s\n",
      "epoch 30 | loss: 0.41918 | train_auc: 0.89016 | valid_auc: 0.88332 |  0:00:17s\n",
      "epoch 31 | loss: 0.4141  | train_auc: 0.88995 | valid_auc: 0.88292 |  0:00:17s\n",
      "epoch 32 | loss: 0.42124 | train_auc: 0.89001 | valid_auc: 0.88399 |  0:00:18s\n",
      "epoch 33 | loss: 0.42132 | train_auc: 0.88867 | valid_auc: 0.88181 |  0:00:18s\n",
      "epoch 34 | loss: 0.41038 | train_auc: 0.88889 | valid_auc: 0.88121 |  0:00:19s\n",
      "epoch 35 | loss: 0.42051 | train_auc: 0.88934 | valid_auc: 0.88368 |  0:00:19s\n",
      "epoch 36 | loss: 0.41511 | train_auc: 0.89037 | valid_auc: 0.88268 |  0:00:20s\n",
      "epoch 37 | loss: 0.42092 | train_auc: 0.88995 | valid_auc: 0.88312 |  0:00:21s\n",
      "epoch 38 | loss: 0.41889 | train_auc: 0.89014 | valid_auc: 0.88397 |  0:00:21s\n",
      "epoch 39 | loss: 0.41014 | train_auc: 0.8905  | valid_auc: 0.88208 |  0:00:22s\n",
      "epoch 40 | loss: 0.4232  | train_auc: 0.89017 | valid_auc: 0.88163 |  0:00:22s\n",
      "epoch 41 | loss: 0.42042 | train_auc: 0.89013 | valid_auc: 0.88322 |  0:00:23s\n",
      "epoch 42 | loss: 0.41142 | train_auc: 0.88995 | valid_auc: 0.88316 |  0:00:23s\n",
      "epoch 43 | loss: 0.4135  | train_auc: 0.88983 | valid_auc: 0.88243 |  0:00:24s\n",
      "epoch 44 | loss: 0.41394 | train_auc: 0.89058 | valid_auc: 0.88315 |  0:00:24s\n",
      "epoch 45 | loss: 0.42101 | train_auc: 0.89089 | valid_auc: 0.88337 |  0:00:25s\n",
      "epoch 46 | loss: 0.41733 | train_auc: 0.89083 | valid_auc: 0.88376 |  0:00:25s\n",
      "epoch 47 | loss: 0.41786 | train_auc: 0.89003 | valid_auc: 0.88347 |  0:00:26s\n",
      "epoch 48 | loss: 0.41739 | train_auc: 0.89057 | valid_auc: 0.88299 |  0:00:27s\n",
      "epoch 49 | loss: 0.41644 | train_auc: 0.89036 | valid_auc: 0.88273 |  0:00:27s\n",
      "epoch 50 | loss: 0.41005 | train_auc: 0.8889  | valid_auc: 0.88246 |  0:00:28s\n",
      "epoch 51 | loss: 0.41833 | train_auc: 0.88929 | valid_auc: 0.88396 |  0:00:28s\n",
      "epoch 52 | loss: 0.41455 | train_auc: 0.89039 | valid_auc: 0.88383 |  0:00:29s\n",
      "epoch 53 | loss: 0.42621 | train_auc: 0.89049 | valid_auc: 0.88284 |  0:00:29s\n",
      "epoch 54 | loss: 0.4167  | train_auc: 0.89019 | valid_auc: 0.88291 |  0:00:30s\n",
      "epoch 55 | loss: 0.41145 | train_auc: 0.89008 | valid_auc: 0.88337 |  0:00:30s\n",
      "epoch 56 | loss: 0.42048 | train_auc: 0.89073 | valid_auc: 0.88309 |  0:00:31s\n",
      "epoch 57 | loss: 0.42072 | train_auc: 0.89044 | valid_auc: 0.88402 |  0:00:32s\n",
      "epoch 58 | loss: 0.41702 | train_auc: 0.89005 | valid_auc: 0.88325 |  0:00:32s\n",
      "epoch 59 | loss: 0.41196 | train_auc: 0.88942 | valid_auc: 0.88299 |  0:00:33s\n",
      "epoch 60 | loss: 0.40834 | train_auc: 0.88992 | valid_auc: 0.88248 |  0:00:33s\n",
      "epoch 61 | loss: 0.41548 | train_auc: 0.88996 | valid_auc: 0.88299 |  0:00:34s\n",
      "epoch 62 | loss: 0.41551 | train_auc: 0.89025 | valid_auc: 0.88236 |  0:00:34s\n",
      "\n",
      "Early stopping occurred at epoch 62 with best_epoch = 12 and best_valid_auc = 0.88412\n",
      "sampling:  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69834 | train_auc: 0.6212  | valid_auc: 0.61375 |  0:00:00s\n",
      "epoch 1  | loss: 0.47386 | train_auc: 0.71817 | valid_auc: 0.71426 |  0:00:01s\n",
      "epoch 2  | loss: 0.45037 | train_auc: 0.79375 | valid_auc: 0.78612 |  0:00:01s\n",
      "epoch 3  | loss: 0.44604 | train_auc: 0.83069 | valid_auc: 0.82761 |  0:00:02s\n",
      "epoch 4  | loss: 0.42748 | train_auc: 0.86265 | valid_auc: 0.86334 |  0:00:02s\n",
      "epoch 5  | loss: 0.43152 | train_auc: 0.8667  | valid_auc: 0.86508 |  0:00:03s\n",
      "epoch 6  | loss: 0.43177 | train_auc: 0.86267 | valid_auc: 0.86015 |  0:00:03s\n",
      "epoch 7  | loss: 0.43121 | train_auc: 0.8723  | valid_auc: 0.86962 |  0:00:04s\n",
      "epoch 8  | loss: 0.42977 | train_auc: 0.87651 | valid_auc: 0.87623 |  0:00:04s\n",
      "epoch 9  | loss: 0.42653 | train_auc: 0.87293 | valid_auc: 0.87139 |  0:00:05s\n",
      "epoch 10 | loss: 0.42545 | train_auc: 0.88051 | valid_auc: 0.87947 |  0:00:06s\n",
      "epoch 11 | loss: 0.42359 | train_auc: 0.88478 | valid_auc: 0.88451 |  0:00:06s\n",
      "epoch 12 | loss: 0.42911 | train_auc: 0.88604 | valid_auc: 0.88567 |  0:00:07s\n",
      "epoch 13 | loss: 0.42393 | train_auc: 0.88758 | valid_auc: 0.88838 |  0:00:07s\n",
      "epoch 14 | loss: 0.41608 | train_auc: 0.88807 | valid_auc: 0.88872 |  0:00:08s\n",
      "epoch 15 | loss: 0.43043 | train_auc: 0.8872  | valid_auc: 0.88829 |  0:00:08s\n",
      "epoch 16 | loss: 0.41724 | train_auc: 0.88676 | valid_auc: 0.88661 |  0:00:09s\n",
      "epoch 17 | loss: 0.41614 | train_auc: 0.88718 | valid_auc: 0.88719 |  0:00:09s\n",
      "epoch 18 | loss: 0.41704 | train_auc: 0.88837 | valid_auc: 0.88878 |  0:00:10s\n",
      "epoch 19 | loss: 0.41951 | train_auc: 0.88942 | valid_auc: 0.89021 |  0:00:11s\n",
      "epoch 20 | loss: 0.41446 | train_auc: 0.88935 | valid_auc: 0.89037 |  0:00:11s\n",
      "epoch 21 | loss: 0.4233  | train_auc: 0.88901 | valid_auc: 0.89094 |  0:00:12s\n",
      "epoch 22 | loss: 0.4095  | train_auc: 0.88909 | valid_auc: 0.88954 |  0:00:12s\n",
      "epoch 23 | loss: 0.42029 | train_auc: 0.88939 | valid_auc: 0.88993 |  0:00:13s\n",
      "epoch 24 | loss: 0.41847 | train_auc: 0.88959 | valid_auc: 0.88949 |  0:00:13s\n",
      "epoch 25 | loss: 0.41347 | train_auc: 0.89006 | valid_auc: 0.89143 |  0:00:14s\n",
      "epoch 26 | loss: 0.41682 | train_auc: 0.88951 | valid_auc: 0.89023 |  0:00:14s\n",
      "epoch 27 | loss: 0.40625 | train_auc: 0.88971 | valid_auc: 0.89031 |  0:00:15s\n",
      "epoch 28 | loss: 0.41572 | train_auc: 0.8898  | valid_auc: 0.89132 |  0:00:16s\n",
      "epoch 29 | loss: 0.40805 | train_auc: 0.89028 | valid_auc: 0.89136 |  0:00:16s\n",
      "epoch 30 | loss: 0.40947 | train_auc: 0.89048 | valid_auc: 0.89133 |  0:00:17s\n",
      "epoch 31 | loss: 0.42162 | train_auc: 0.89034 | valid_auc: 0.89157 |  0:00:17s\n",
      "epoch 32 | loss: 0.41137 | train_auc: 0.89054 | valid_auc: 0.89086 |  0:00:18s\n",
      "epoch 33 | loss: 0.41607 | train_auc: 0.89042 | valid_auc: 0.89163 |  0:00:18s\n",
      "epoch 34 | loss: 0.41554 | train_auc: 0.89044 | valid_auc: 0.89143 |  0:00:19s\n",
      "epoch 35 | loss: 0.41619 | train_auc: 0.89042 | valid_auc: 0.89183 |  0:00:19s\n",
      "epoch 36 | loss: 0.40587 | train_auc: 0.89034 | valid_auc: 0.89174 |  0:00:20s\n",
      "epoch 37 | loss: 0.41656 | train_auc: 0.89028 | valid_auc: 0.89097 |  0:00:20s\n",
      "epoch 38 | loss: 0.40956 | train_auc: 0.89075 | valid_auc: 0.89139 |  0:00:21s\n",
      "epoch 39 | loss: 0.41965 | train_auc: 0.88998 | valid_auc: 0.89142 |  0:00:22s\n",
      "epoch 40 | loss: 0.41849 | train_auc: 0.89073 | valid_auc: 0.89031 |  0:00:22s\n",
      "epoch 41 | loss: 0.41517 | train_auc: 0.89039 | valid_auc: 0.89114 |  0:00:23s\n",
      "epoch 42 | loss: 0.42097 | train_auc: 0.89031 | valid_auc: 0.89127 |  0:00:23s\n",
      "epoch 43 | loss: 0.41275 | train_auc: 0.89069 | valid_auc: 0.89046 |  0:00:24s\n",
      "epoch 44 | loss: 0.41406 | train_auc: 0.88965 | valid_auc: 0.89175 |  0:00:24s\n",
      "epoch 45 | loss: 0.41517 | train_auc: 0.89089 | valid_auc: 0.8923  |  0:00:25s\n",
      "epoch 46 | loss: 0.41533 | train_auc: 0.89107 | valid_auc: 0.89144 |  0:00:25s\n",
      "epoch 47 | loss: 0.41004 | train_auc: 0.89043 | valid_auc: 0.89144 |  0:00:26s\n",
      "epoch 48 | loss: 0.40995 | train_auc: 0.89086 | valid_auc: 0.89089 |  0:00:26s\n",
      "epoch 49 | loss: 0.4148  | train_auc: 0.89035 | valid_auc: 0.89096 |  0:00:27s\n",
      "epoch 50 | loss: 0.40971 | train_auc: 0.89103 | valid_auc: 0.89139 |  0:00:28s\n",
      "epoch 51 | loss: 0.41735 | train_auc: 0.89074 | valid_auc: 0.89091 |  0:00:28s\n",
      "epoch 52 | loss: 0.41933 | train_auc: 0.89081 | valid_auc: 0.89075 |  0:00:29s\n",
      "epoch 53 | loss: 0.42045 | train_auc: 0.89092 | valid_auc: 0.89121 |  0:00:29s\n",
      "epoch 54 | loss: 0.41659 | train_auc: 0.89073 | valid_auc: 0.8921  |  0:00:30s\n",
      "epoch 55 | loss: 0.40705 | train_auc: 0.891   | valid_auc: 0.89119 |  0:00:30s\n",
      "epoch 56 | loss: 0.42079 | train_auc: 0.89094 | valid_auc: 0.89168 |  0:00:31s\n",
      "epoch 57 | loss: 0.41611 | train_auc: 0.89062 | valid_auc: 0.88992 |  0:00:31s\n",
      "epoch 58 | loss: 0.42198 | train_auc: 0.89061 | valid_auc: 0.89211 |  0:00:32s\n",
      "epoch 59 | loss: 0.41207 | train_auc: 0.89061 | valid_auc: 0.89099 |  0:00:33s\n",
      "epoch 60 | loss: 0.41066 | train_auc: 0.89072 | valid_auc: 0.89186 |  0:00:33s\n",
      "epoch 61 | loss: 0.42034 | train_auc: 0.89048 | valid_auc: 0.89101 |  0:00:34s\n",
      "epoch 62 | loss: 0.4245  | train_auc: 0.89073 | valid_auc: 0.89029 |  0:00:34s\n",
      "epoch 63 | loss: 0.41587 | train_auc: 0.89102 | valid_auc: 0.89097 |  0:00:35s\n",
      "epoch 64 | loss: 0.41624 | train_auc: 0.89072 | valid_auc: 0.8927  |  0:00:35s\n",
      "epoch 65 | loss: 0.41324 | train_auc: 0.89101 | valid_auc: 0.89139 |  0:00:36s\n",
      "epoch 66 | loss: 0.40683 | train_auc: 0.89092 | valid_auc: 0.89196 |  0:00:36s\n",
      "epoch 67 | loss: 0.41669 | train_auc: 0.89106 | valid_auc: 0.89195 |  0:00:37s\n",
      "epoch 68 | loss: 0.42225 | train_auc: 0.89061 | valid_auc: 0.89109 |  0:00:37s\n",
      "epoch 69 | loss: 0.40885 | train_auc: 0.8904  | valid_auc: 0.89113 |  0:00:38s\n",
      "epoch 70 | loss: 0.4137  | train_auc: 0.89105 | valid_auc: 0.89034 |  0:00:39s\n",
      "epoch 71 | loss: 0.41834 | train_auc: 0.89118 | valid_auc: 0.89043 |  0:00:39s\n",
      "epoch 72 | loss: 0.42023 | train_auc: 0.89099 | valid_auc: 0.89126 |  0:00:40s\n",
      "epoch 73 | loss: 0.41945 | train_auc: 0.89144 | valid_auc: 0.89093 |  0:00:40s\n",
      "epoch 74 | loss: 0.41074 | train_auc: 0.89154 | valid_auc: 0.89151 |  0:00:41s\n",
      "epoch 75 | loss: 0.41068 | train_auc: 0.89157 | valid_auc: 0.89162 |  0:00:41s\n",
      "epoch 76 | loss: 0.42353 | train_auc: 0.89148 | valid_auc: 0.89109 |  0:00:42s\n",
      "epoch 77 | loss: 0.41063 | train_auc: 0.8913  | valid_auc: 0.89155 |  0:00:42s\n",
      "epoch 78 | loss: 0.40733 | train_auc: 0.89164 | valid_auc: 0.89138 |  0:00:43s\n",
      "epoch 79 | loss: 0.41283 | train_auc: 0.8914  | valid_auc: 0.8921  |  0:00:44s\n",
      "epoch 80 | loss: 0.41294 | train_auc: 0.89138 | valid_auc: 0.89084 |  0:00:44s\n",
      "epoch 81 | loss: 0.41235 | train_auc: 0.89115 | valid_auc: 0.8913  |  0:00:45s\n",
      "epoch 82 | loss: 0.41473 | train_auc: 0.89007 | valid_auc: 0.88951 |  0:00:45s\n",
      "epoch 83 | loss: 0.41474 | train_auc: 0.89069 | valid_auc: 0.89049 |  0:00:46s\n",
      "epoch 84 | loss: 0.414   | train_auc: 0.89081 | valid_auc: 0.89133 |  0:00:46s\n",
      "epoch 85 | loss: 0.4134  | train_auc: 0.89099 | valid_auc: 0.89171 |  0:00:47s\n",
      "epoch 86 | loss: 0.41801 | train_auc: 0.89082 | valid_auc: 0.89245 |  0:00:47s\n",
      "epoch 87 | loss: 0.42004 | train_auc: 0.89047 | valid_auc: 0.89142 |  0:00:48s\n",
      "epoch 88 | loss: 0.40847 | train_auc: 0.89    | valid_auc: 0.89117 |  0:00:48s\n",
      "epoch 89 | loss: 0.40618 | train_auc: 0.89082 | valid_auc: 0.89153 |  0:00:49s\n",
      "epoch 90 | loss: 0.40753 | train_auc: 0.89124 | valid_auc: 0.89046 |  0:00:50s\n",
      "epoch 91 | loss: 0.4134  | train_auc: 0.8907  | valid_auc: 0.89163 |  0:00:50s\n",
      "epoch 92 | loss: 0.41527 | train_auc: 0.89121 | valid_auc: 0.8902  |  0:00:51s\n",
      "epoch 93 | loss: 0.41904 | train_auc: 0.89067 | valid_auc: 0.8919  |  0:00:51s\n",
      "epoch 94 | loss: 0.40984 | train_auc: 0.89118 | valid_auc: 0.89062 |  0:00:52s\n",
      "epoch 95 | loss: 0.42085 | train_auc: 0.89128 | valid_auc: 0.8919  |  0:00:52s\n",
      "epoch 96 | loss: 0.40996 | train_auc: 0.89154 | valid_auc: 0.89101 |  0:00:53s\n",
      "epoch 97 | loss: 0.41402 | train_auc: 0.89131 | valid_auc: 0.89143 |  0:00:53s\n",
      "epoch 98 | loss: 0.41034 | train_auc: 0.89092 | valid_auc: 0.89082 |  0:00:54s\n",
      "epoch 99 | loss: 0.41196 | train_auc: 0.89011 | valid_auc: 0.89158 |  0:00:55s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 64 and best_valid_auc = 0.8927\n",
      "sampling:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.67918 | train_auc: 0.59221 | valid_auc: 0.59959 |  0:00:00s\n",
      "epoch 1  | loss: 0.46989 | train_auc: 0.72658 | valid_auc: 0.72326 |  0:00:01s\n",
      "epoch 2  | loss: 0.43928 | train_auc: 0.81637 | valid_auc: 0.83263 |  0:00:01s\n",
      "epoch 3  | loss: 0.42924 | train_auc: 0.84549 | valid_auc: 0.85336 |  0:00:02s\n",
      "epoch 4  | loss: 0.43191 | train_auc: 0.85771 | valid_auc: 0.86641 |  0:00:02s\n",
      "epoch 5  | loss: 0.42267 | train_auc: 0.86869 | valid_auc: 0.87651 |  0:00:03s\n",
      "epoch 6  | loss: 0.42261 | train_auc: 0.86061 | valid_auc: 0.86869 |  0:00:03s\n",
      "epoch 7  | loss: 0.42452 | train_auc: 0.87102 | valid_auc: 0.88054 |  0:00:04s\n",
      "epoch 8  | loss: 0.42213 | train_auc: 0.87268 | valid_auc: 0.88267 |  0:00:05s\n",
      "epoch 9  | loss: 0.41894 | train_auc: 0.87333 | valid_auc: 0.88286 |  0:00:05s\n",
      "epoch 10 | loss: 0.41747 | train_auc: 0.8798  | valid_auc: 0.88966 |  0:00:06s\n",
      "epoch 11 | loss: 0.41828 | train_auc: 0.87981 | valid_auc: 0.88872 |  0:00:06s\n",
      "epoch 12 | loss: 0.42494 | train_auc: 0.88244 | valid_auc: 0.88983 |  0:00:07s\n",
      "epoch 13 | loss: 0.42159 | train_auc: 0.88354 | valid_auc: 0.88954 |  0:00:07s\n",
      "epoch 14 | loss: 0.42287 | train_auc: 0.88502 | valid_auc: 0.89079 |  0:00:08s\n",
      "epoch 15 | loss: 0.42721 | train_auc: 0.88534 | valid_auc: 0.89369 |  0:00:09s\n",
      "epoch 16 | loss: 0.42077 | train_auc: 0.88552 | valid_auc: 0.89171 |  0:00:09s\n",
      "epoch 17 | loss: 0.42746 | train_auc: 0.88634 | valid_auc: 0.89329 |  0:00:10s\n",
      "epoch 18 | loss: 0.41337 | train_auc: 0.88616 | valid_auc: 0.89273 |  0:00:10s\n",
      "epoch 19 | loss: 0.41792 | train_auc: 0.88654 | valid_auc: 0.89324 |  0:00:11s\n",
      "epoch 20 | loss: 0.42113 | train_auc: 0.88741 | valid_auc: 0.89544 |  0:00:11s\n",
      "epoch 21 | loss: 0.41868 | train_auc: 0.88694 | valid_auc: 0.89449 |  0:00:12s\n",
      "epoch 22 | loss: 0.4156  | train_auc: 0.88685 | valid_auc: 0.89435 |  0:00:13s\n",
      "epoch 23 | loss: 0.41386 | train_auc: 0.8876  | valid_auc: 0.89459 |  0:00:13s\n",
      "epoch 24 | loss: 0.41857 | train_auc: 0.8864  | valid_auc: 0.89125 |  0:00:14s\n",
      "epoch 25 | loss: 0.42708 | train_auc: 0.88616 | valid_auc: 0.89347 |  0:00:14s\n",
      "epoch 26 | loss: 0.42077 | train_auc: 0.88797 | valid_auc: 0.89385 |  0:00:15s\n",
      "epoch 27 | loss: 0.41701 | train_auc: 0.88761 | valid_auc: 0.89525 |  0:00:15s\n",
      "epoch 28 | loss: 0.41921 | train_auc: 0.8884  | valid_auc: 0.89535 |  0:00:16s\n",
      "epoch 29 | loss: 0.415   | train_auc: 0.88792 | valid_auc: 0.89521 |  0:00:16s\n",
      "epoch 30 | loss: 0.41751 | train_auc: 0.88799 | valid_auc: 0.89567 |  0:00:17s\n",
      "epoch 31 | loss: 0.41988 | train_auc: 0.88773 | valid_auc: 0.89335 |  0:00:18s\n",
      "epoch 32 | loss: 0.42    | train_auc: 0.88825 | valid_auc: 0.89489 |  0:00:18s\n",
      "epoch 33 | loss: 0.42238 | train_auc: 0.88803 | valid_auc: 0.89357 |  0:00:19s\n",
      "epoch 34 | loss: 0.42315 | train_auc: 0.88751 | valid_auc: 0.89359 |  0:00:19s\n",
      "epoch 35 | loss: 0.41345 | train_auc: 0.88775 | valid_auc: 0.89467 |  0:00:20s\n",
      "epoch 36 | loss: 0.42862 | train_auc: 0.88847 | valid_auc: 0.89489 |  0:00:20s\n",
      "epoch 37 | loss: 0.41868 | train_auc: 0.88662 | valid_auc: 0.89483 |  0:00:21s\n",
      "epoch 38 | loss: 0.42235 | train_auc: 0.88799 | valid_auc: 0.89489 |  0:00:22s\n",
      "epoch 39 | loss: 0.41851 | train_auc: 0.88802 | valid_auc: 0.89554 |  0:00:22s\n",
      "epoch 40 | loss: 0.41153 | train_auc: 0.8881  | valid_auc: 0.89512 |  0:00:23s\n",
      "epoch 41 | loss: 0.41219 | train_auc: 0.88859 | valid_auc: 0.89439 |  0:00:23s\n",
      "epoch 42 | loss: 0.41839 | train_auc: 0.88901 | valid_auc: 0.89523 |  0:00:24s\n",
      "epoch 43 | loss: 0.4139  | train_auc: 0.88896 | valid_auc: 0.89601 |  0:00:24s\n",
      "epoch 44 | loss: 0.41581 | train_auc: 0.88836 | valid_auc: 0.89557 |  0:00:25s\n",
      "epoch 45 | loss: 0.42391 | train_auc: 0.88869 | valid_auc: 0.89551 |  0:00:25s\n",
      "epoch 46 | loss: 0.41321 | train_auc: 0.88872 | valid_auc: 0.89507 |  0:00:26s\n",
      "epoch 47 | loss: 0.42117 | train_auc: 0.88871 | valid_auc: 0.89544 |  0:00:27s\n",
      "epoch 48 | loss: 0.4199  | train_auc: 0.88767 | valid_auc: 0.89489 |  0:00:27s\n",
      "epoch 49 | loss: 0.42055 | train_auc: 0.88902 | valid_auc: 0.8958  |  0:00:28s\n",
      "epoch 50 | loss: 0.42158 | train_auc: 0.8891  | valid_auc: 0.89573 |  0:00:28s\n",
      "epoch 51 | loss: 0.41296 | train_auc: 0.8879  | valid_auc: 0.89444 |  0:00:29s\n",
      "epoch 52 | loss: 0.41722 | train_auc: 0.88949 | valid_auc: 0.8951  |  0:00:29s\n",
      "epoch 53 | loss: 0.4101  | train_auc: 0.8884  | valid_auc: 0.89474 |  0:00:30s\n",
      "epoch 54 | loss: 0.40842 | train_auc: 0.8885  | valid_auc: 0.89477 |  0:00:30s\n",
      "epoch 55 | loss: 0.40752 | train_auc: 0.88852 | valid_auc: 0.89538 |  0:00:31s\n",
      "epoch 56 | loss: 0.41492 | train_auc: 0.88888 | valid_auc: 0.89551 |  0:00:32s\n",
      "epoch 57 | loss: 0.41326 | train_auc: 0.88829 | valid_auc: 0.89425 |  0:00:32s\n",
      "epoch 58 | loss: 0.40826 | train_auc: 0.88863 | valid_auc: 0.89451 |  0:00:33s\n",
      "epoch 59 | loss: 0.40826 | train_auc: 0.88904 | valid_auc: 0.89368 |  0:00:33s\n",
      "epoch 60 | loss: 0.42458 | train_auc: 0.88857 | valid_auc: 0.89381 |  0:00:34s\n",
      "epoch 61 | loss: 0.41705 | train_auc: 0.88871 | valid_auc: 0.89523 |  0:00:34s\n",
      "epoch 62 | loss: 0.41959 | train_auc: 0.88872 | valid_auc: 0.89589 |  0:00:35s\n",
      "epoch 63 | loss: 0.41924 | train_auc: 0.88898 | valid_auc: 0.89409 |  0:00:35s\n",
      "epoch 64 | loss: 0.42232 | train_auc: 0.88913 | valid_auc: 0.8948  |  0:00:36s\n",
      "epoch 65 | loss: 0.42179 | train_auc: 0.88918 | valid_auc: 0.89538 |  0:00:37s\n",
      "epoch 66 | loss: 0.41823 | train_auc: 0.88888 | valid_auc: 0.89562 |  0:00:37s\n",
      "epoch 67 | loss: 0.41249 | train_auc: 0.88897 | valid_auc: 0.89518 |  0:00:38s\n",
      "epoch 68 | loss: 0.4137  | train_auc: 0.88925 | valid_auc: 0.89482 |  0:00:38s\n",
      "epoch 69 | loss: 0.41441 | train_auc: 0.88862 | valid_auc: 0.89585 |  0:00:39s\n",
      "epoch 70 | loss: 0.41904 | train_auc: 0.88896 | valid_auc: 0.89583 |  0:00:39s\n",
      "epoch 71 | loss: 0.41208 | train_auc: 0.88845 | valid_auc: 0.89572 |  0:00:40s\n",
      "epoch 72 | loss: 0.41731 | train_auc: 0.88902 | valid_auc: 0.89487 |  0:00:40s\n",
      "epoch 73 | loss: 0.4121  | train_auc: 0.8889  | valid_auc: 0.89478 |  0:00:41s\n",
      "epoch 74 | loss: 0.41    | train_auc: 0.88863 | valid_auc: 0.89605 |  0:00:42s\n",
      "epoch 75 | loss: 0.41685 | train_auc: 0.88906 | valid_auc: 0.89518 |  0:00:42s\n",
      "epoch 76 | loss: 0.41913 | train_auc: 0.88923 | valid_auc: 0.89466 |  0:00:43s\n",
      "epoch 77 | loss: 0.41649 | train_auc: 0.88872 | valid_auc: 0.89486 |  0:00:43s\n",
      "epoch 78 | loss: 0.41343 | train_auc: 0.88895 | valid_auc: 0.89369 |  0:00:44s\n",
      "epoch 79 | loss: 0.41247 | train_auc: 0.88884 | valid_auc: 0.89636 |  0:00:44s\n",
      "epoch 80 | loss: 0.41777 | train_auc: 0.88919 | valid_auc: 0.89426 |  0:00:45s\n",
      "epoch 81 | loss: 0.41633 | train_auc: 0.88805 | valid_auc: 0.89359 |  0:00:45s\n",
      "epoch 82 | loss: 0.42544 | train_auc: 0.8886  | valid_auc: 0.89522 |  0:00:46s\n",
      "epoch 83 | loss: 0.41373 | train_auc: 0.88897 | valid_auc: 0.89455 |  0:00:47s\n",
      "epoch 84 | loss: 0.41148 | train_auc: 0.88949 | valid_auc: 0.89647 |  0:00:47s\n",
      "epoch 85 | loss: 0.40309 | train_auc: 0.88966 | valid_auc: 0.89604 |  0:00:48s\n",
      "epoch 86 | loss: 0.41145 | train_auc: 0.88888 | valid_auc: 0.89685 |  0:00:48s\n",
      "epoch 87 | loss: 0.41414 | train_auc: 0.8887  | valid_auc: 0.89621 |  0:00:49s\n",
      "epoch 88 | loss: 0.41817 | train_auc: 0.88952 | valid_auc: 0.89533 |  0:00:49s\n",
      "epoch 89 | loss: 0.41714 | train_auc: 0.88863 | valid_auc: 0.89388 |  0:00:50s\n",
      "epoch 90 | loss: 0.42026 | train_auc: 0.88903 | valid_auc: 0.8949  |  0:00:51s\n",
      "epoch 91 | loss: 0.40652 | train_auc: 0.88918 | valid_auc: 0.8952  |  0:00:51s\n",
      "epoch 92 | loss: 0.41165 | train_auc: 0.88921 | valid_auc: 0.89541 |  0:00:52s\n",
      "epoch 93 | loss: 0.41484 | train_auc: 0.88904 | valid_auc: 0.89552 |  0:00:52s\n",
      "epoch 94 | loss: 0.4192  | train_auc: 0.88945 | valid_auc: 0.89634 |  0:00:53s\n",
      "epoch 95 | loss: 0.4152  | train_auc: 0.88979 | valid_auc: 0.8957  |  0:00:53s\n",
      "epoch 96 | loss: 0.40901 | train_auc: 0.88915 | valid_auc: 0.89536 |  0:00:54s\n",
      "epoch 97 | loss: 0.41447 | train_auc: 0.8899  | valid_auc: 0.89553 |  0:00:54s\n",
      "epoch 98 | loss: 0.41739 | train_auc: 0.88979 | valid_auc: 0.89602 |  0:00:55s\n",
      "epoch 99 | loss: 0.41183 | train_auc: 0.88948 | valid_auc: 0.8952  |  0:00:56s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 86 and best_valid_auc = 0.89685\n",
      "sampling:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.6788  | train_auc: 0.62982 | valid_auc: 0.61903 |  0:00:00s\n",
      "epoch 1  | loss: 0.48966 | train_auc: 0.74415 | valid_auc: 0.73522 |  0:00:01s\n",
      "epoch 2  | loss: 0.44845 | train_auc: 0.8299  | valid_auc: 0.83322 |  0:00:01s\n",
      "epoch 3  | loss: 0.43053 | train_auc: 0.84335 | valid_auc: 0.84352 |  0:00:02s\n",
      "epoch 4  | loss: 0.4344  | train_auc: 0.85803 | valid_auc: 0.86351 |  0:00:02s\n",
      "epoch 5  | loss: 0.42565 | train_auc: 0.86911 | valid_auc: 0.87303 |  0:00:03s\n",
      "epoch 6  | loss: 0.42364 | train_auc: 0.87205 | valid_auc: 0.87541 |  0:00:04s\n",
      "epoch 7  | loss: 0.43016 | train_auc: 0.87247 | valid_auc: 0.87658 |  0:00:04s\n",
      "epoch 8  | loss: 0.42641 | train_auc: 0.87528 | valid_auc: 0.88015 |  0:00:05s\n",
      "epoch 9  | loss: 0.42127 | train_auc: 0.87905 | valid_auc: 0.88693 |  0:00:05s\n",
      "epoch 10 | loss: 0.42086 | train_auc: 0.8767  | valid_auc: 0.881   |  0:00:06s\n",
      "epoch 11 | loss: 0.4246  | train_auc: 0.88173 | valid_auc: 0.88853 |  0:00:06s\n",
      "epoch 12 | loss: 0.42826 | train_auc: 0.88382 | valid_auc: 0.89138 |  0:00:07s\n",
      "epoch 13 | loss: 0.42155 | train_auc: 0.88416 | valid_auc: 0.89055 |  0:00:08s\n",
      "epoch 14 | loss: 0.43248 | train_auc: 0.88464 | valid_auc: 0.89292 |  0:00:08s\n",
      "epoch 15 | loss: 0.42893 | train_auc: 0.88613 | valid_auc: 0.8938  |  0:00:09s\n",
      "epoch 16 | loss: 0.42875 | train_auc: 0.88733 | valid_auc: 0.89445 |  0:00:09s\n",
      "epoch 17 | loss: 0.41903 | train_auc: 0.88546 | valid_auc: 0.89128 |  0:00:10s\n",
      "epoch 18 | loss: 0.42797 | train_auc: 0.88744 | valid_auc: 0.89571 |  0:00:10s\n",
      "epoch 19 | loss: 0.42139 | train_auc: 0.88673 | valid_auc: 0.89401 |  0:00:11s\n",
      "epoch 20 | loss: 0.41894 | train_auc: 0.88697 | valid_auc: 0.89507 |  0:00:11s\n",
      "epoch 21 | loss: 0.41358 | train_auc: 0.88586 | valid_auc: 0.89297 |  0:00:12s\n",
      "epoch 22 | loss: 0.42626 | train_auc: 0.88692 | valid_auc: 0.89379 |  0:00:13s\n",
      "epoch 23 | loss: 0.41508 | train_auc: 0.88768 | valid_auc: 0.89486 |  0:00:13s\n",
      "epoch 24 | loss: 0.42655 | train_auc: 0.88816 | valid_auc: 0.89501 |  0:00:14s\n",
      "epoch 25 | loss: 0.42049 | train_auc: 0.88787 | valid_auc: 0.89494 |  0:00:14s\n",
      "epoch 26 | loss: 0.41707 | train_auc: 0.88774 | valid_auc: 0.89451 |  0:00:15s\n",
      "epoch 27 | loss: 0.41856 | train_auc: 0.88787 | valid_auc: 0.89443 |  0:00:15s\n",
      "epoch 28 | loss: 0.42705 | train_auc: 0.88824 | valid_auc: 0.89467 |  0:00:16s\n",
      "epoch 29 | loss: 0.42351 | train_auc: 0.88786 | valid_auc: 0.89472 |  0:00:16s\n",
      "epoch 30 | loss: 0.42104 | train_auc: 0.88806 | valid_auc: 0.89524 |  0:00:17s\n",
      "epoch 31 | loss: 0.41887 | train_auc: 0.88794 | valid_auc: 0.89451 |  0:00:17s\n",
      "epoch 32 | loss: 0.42678 | train_auc: 0.88838 | valid_auc: 0.89556 |  0:00:18s\n",
      "epoch 33 | loss: 0.41783 | train_auc: 0.88751 | valid_auc: 0.89499 |  0:00:19s\n",
      "epoch 34 | loss: 0.41744 | train_auc: 0.88851 | valid_auc: 0.89604 |  0:00:19s\n",
      "epoch 35 | loss: 0.41533 | train_auc: 0.88875 | valid_auc: 0.89601 |  0:00:20s\n",
      "epoch 36 | loss: 0.41615 | train_auc: 0.88873 | valid_auc: 0.89586 |  0:00:20s\n",
      "epoch 37 | loss: 0.41611 | train_auc: 0.88832 | valid_auc: 0.89494 |  0:00:21s\n",
      "epoch 38 | loss: 0.4276  | train_auc: 0.88807 | valid_auc: 0.8948  |  0:00:21s\n",
      "epoch 39 | loss: 0.41393 | train_auc: 0.88843 | valid_auc: 0.89489 |  0:00:22s\n",
      "epoch 40 | loss: 0.42459 | train_auc: 0.88801 | valid_auc: 0.89464 |  0:00:22s\n",
      "epoch 41 | loss: 0.42394 | train_auc: 0.88842 | valid_auc: 0.89515 |  0:00:23s\n",
      "epoch 42 | loss: 0.4147  | train_auc: 0.8885  | valid_auc: 0.89597 |  0:00:24s\n",
      "epoch 43 | loss: 0.42496 | train_auc: 0.88865 | valid_auc: 0.89577 |  0:00:24s\n",
      "epoch 44 | loss: 0.41991 | train_auc: 0.88858 | valid_auc: 0.89657 |  0:00:25s\n",
      "epoch 45 | loss: 0.41789 | train_auc: 0.88881 | valid_auc: 0.89605 |  0:00:25s\n",
      "epoch 46 | loss: 0.42271 | train_auc: 0.8883  | valid_auc: 0.89601 |  0:00:26s\n",
      "epoch 47 | loss: 0.41565 | train_auc: 0.88938 | valid_auc: 0.8963  |  0:00:26s\n",
      "epoch 48 | loss: 0.41301 | train_auc: 0.88916 | valid_auc: 0.89597 |  0:00:27s\n",
      "epoch 49 | loss: 0.41752 | train_auc: 0.88907 | valid_auc: 0.89587 |  0:00:27s\n",
      "epoch 50 | loss: 0.41938 | train_auc: 0.88906 | valid_auc: 0.89559 |  0:00:28s\n",
      "epoch 51 | loss: 0.41352 | train_auc: 0.8891  | valid_auc: 0.89519 |  0:00:28s\n",
      "epoch 52 | loss: 0.42233 | train_auc: 0.88931 | valid_auc: 0.89647 |  0:00:29s\n",
      "epoch 53 | loss: 0.41808 | train_auc: 0.88893 | valid_auc: 0.89575 |  0:00:30s\n",
      "epoch 54 | loss: 0.42197 | train_auc: 0.8885  | valid_auc: 0.89569 |  0:00:30s\n",
      "epoch 55 | loss: 0.42568 | train_auc: 0.88914 | valid_auc: 0.89543 |  0:00:31s\n",
      "epoch 56 | loss: 0.41171 | train_auc: 0.88934 | valid_auc: 0.89677 |  0:00:31s\n",
      "epoch 57 | loss: 0.42418 | train_auc: 0.88912 | valid_auc: 0.89665 |  0:00:32s\n",
      "epoch 58 | loss: 0.40976 | train_auc: 0.88922 | valid_auc: 0.89675 |  0:00:32s\n",
      "epoch 59 | loss: 0.43035 | train_auc: 0.889   | valid_auc: 0.89608 |  0:00:33s\n",
      "epoch 60 | loss: 0.4144  | train_auc: 0.88955 | valid_auc: 0.89556 |  0:00:33s\n",
      "epoch 61 | loss: 0.41667 | train_auc: 0.88888 | valid_auc: 0.89467 |  0:00:34s\n",
      "epoch 62 | loss: 0.42347 | train_auc: 0.88947 | valid_auc: 0.89582 |  0:00:35s\n",
      "epoch 63 | loss: 0.417   | train_auc: 0.88941 | valid_auc: 0.89661 |  0:00:35s\n",
      "epoch 64 | loss: 0.41859 | train_auc: 0.88824 | valid_auc: 0.89458 |  0:00:36s\n",
      "epoch 65 | loss: 0.41846 | train_auc: 0.88943 | valid_auc: 0.89486 |  0:00:36s\n",
      "epoch 66 | loss: 0.41773 | train_auc: 0.88919 | valid_auc: 0.89535 |  0:00:37s\n",
      "epoch 67 | loss: 0.41667 | train_auc: 0.88932 | valid_auc: 0.89463 |  0:00:37s\n",
      "epoch 68 | loss: 0.42006 | train_auc: 0.88889 | valid_auc: 0.89526 |  0:00:38s\n",
      "epoch 69 | loss: 0.42001 | train_auc: 0.88945 | valid_auc: 0.89605 |  0:00:38s\n",
      "epoch 70 | loss: 0.41743 | train_auc: 0.88951 | valid_auc: 0.89564 |  0:00:39s\n",
      "epoch 71 | loss: 0.42125 | train_auc: 0.8891  | valid_auc: 0.89442 |  0:00:39s\n",
      "epoch 72 | loss: 0.42022 | train_auc: 0.88903 | valid_auc: 0.89491 |  0:00:40s\n",
      "epoch 73 | loss: 0.41557 | train_auc: 0.88912 | valid_auc: 0.89547 |  0:00:41s\n",
      "epoch 74 | loss: 0.41485 | train_auc: 0.88931 | valid_auc: 0.89544 |  0:00:41s\n",
      "epoch 75 | loss: 0.42054 | train_auc: 0.88894 | valid_auc: 0.89544 |  0:00:42s\n",
      "epoch 76 | loss: 0.40865 | train_auc: 0.88959 | valid_auc: 0.89556 |  0:00:42s\n",
      "epoch 77 | loss: 0.41747 | train_auc: 0.88959 | valid_auc: 0.89568 |  0:00:43s\n",
      "epoch 78 | loss: 0.41717 | train_auc: 0.88918 | valid_auc: 0.89495 |  0:00:43s\n",
      "epoch 79 | loss: 0.41003 | train_auc: 0.88881 | valid_auc: 0.89538 |  0:00:44s\n",
      "epoch 80 | loss: 0.41882 | train_auc: 0.88854 | valid_auc: 0.89503 |  0:00:44s\n",
      "epoch 81 | loss: 0.41742 | train_auc: 0.8884  | valid_auc: 0.89526 |  0:00:45s\n",
      "epoch 82 | loss: 0.41855 | train_auc: 0.889   | valid_auc: 0.89476 |  0:00:46s\n",
      "epoch 83 | loss: 0.41574 | train_auc: 0.88952 | valid_auc: 0.89552 |  0:00:46s\n",
      "epoch 84 | loss: 0.41731 | train_auc: 0.88922 | valid_auc: 0.89561 |  0:00:47s\n",
      "epoch 85 | loss: 0.42219 | train_auc: 0.88881 | valid_auc: 0.89586 |  0:00:47s\n",
      "epoch 86 | loss: 0.42088 | train_auc: 0.88954 | valid_auc: 0.89642 |  0:00:48s\n",
      "epoch 87 | loss: 0.41525 | train_auc: 0.88943 | valid_auc: 0.89672 |  0:00:48s\n",
      "epoch 88 | loss: 0.42562 | train_auc: 0.88946 | valid_auc: 0.89614 |  0:00:49s\n",
      "epoch 89 | loss: 0.41704 | train_auc: 0.8895  | valid_auc: 0.89598 |  0:00:49s\n",
      "epoch 90 | loss: 0.41235 | train_auc: 0.88969 | valid_auc: 0.89649 |  0:00:50s\n",
      "epoch 91 | loss: 0.42325 | train_auc: 0.88923 | valid_auc: 0.89564 |  0:00:50s\n",
      "epoch 92 | loss: 0.41555 | train_auc: 0.8897  | valid_auc: 0.89638 |  0:00:51s\n",
      "epoch 93 | loss: 0.4222  | train_auc: 0.88979 | valid_auc: 0.89702 |  0:00:52s\n",
      "epoch 94 | loss: 0.40591 | train_auc: 0.8893  | valid_auc: 0.89589 |  0:00:52s\n",
      "epoch 95 | loss: 0.41437 | train_auc: 0.88947 | valid_auc: 0.89624 |  0:00:53s\n",
      "epoch 96 | loss: 0.41222 | train_auc: 0.88968 | valid_auc: 0.8955  |  0:00:53s\n",
      "epoch 97 | loss: 0.41835 | train_auc: 0.88947 | valid_auc: 0.89576 |  0:00:54s\n",
      "epoch 98 | loss: 0.4121  | train_auc: 0.88951 | valid_auc: 0.8953  |  0:00:54s\n",
      "epoch 99 | loss: 0.4212  | train_auc: 0.88975 | valid_auc: 0.89675 |  0:00:55s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 93 and best_valid_auc = 0.89702\n",
      "sampling:  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69136 | train_auc: 0.68744 | valid_auc: 0.68325 |  0:00:00s\n",
      "epoch 1  | loss: 0.48    | train_auc: 0.75525 | valid_auc: 0.75734 |  0:00:01s\n",
      "epoch 2  | loss: 0.44969 | train_auc: 0.85369 | valid_auc: 0.8515  |  0:00:01s\n",
      "epoch 3  | loss: 0.4301  | train_auc: 0.86577 | valid_auc: 0.86552 |  0:00:02s\n",
      "epoch 4  | loss: 0.41799 | train_auc: 0.86327 | valid_auc: 0.86228 |  0:00:02s\n",
      "epoch 5  | loss: 0.42811 | train_auc: 0.86894 | valid_auc: 0.86769 |  0:00:03s\n",
      "epoch 6  | loss: 0.42044 | train_auc: 0.87129 | valid_auc: 0.86974 |  0:00:03s\n",
      "epoch 7  | loss: 0.41726 | train_auc: 0.87823 | valid_auc: 0.87629 |  0:00:04s\n",
      "epoch 8  | loss: 0.41718 | train_auc: 0.88073 | valid_auc: 0.87915 |  0:00:04s\n",
      "epoch 9  | loss: 0.41425 | train_auc: 0.88045 | valid_auc: 0.87871 |  0:00:05s\n",
      "epoch 10 | loss: 0.4221  | train_auc: 0.88468 | valid_auc: 0.88284 |  0:00:06s\n",
      "epoch 11 | loss: 0.41885 | train_auc: 0.8851  | valid_auc: 0.8823  |  0:00:06s\n",
      "epoch 12 | loss: 0.42143 | train_auc: 0.88721 | valid_auc: 0.8825  |  0:00:07s\n",
      "epoch 13 | loss: 0.41772 | train_auc: 0.88726 | valid_auc: 0.88374 |  0:00:07s\n",
      "epoch 14 | loss: 0.42004 | train_auc: 0.88587 | valid_auc: 0.88181 |  0:00:08s\n",
      "epoch 15 | loss: 0.41893 | train_auc: 0.88718 | valid_auc: 0.88402 |  0:00:08s\n",
      "epoch 16 | loss: 0.41915 | train_auc: 0.88382 | valid_auc: 0.88283 |  0:00:09s\n",
      "epoch 17 | loss: 0.42249 | train_auc: 0.88774 | valid_auc: 0.88443 |  0:00:09s\n",
      "epoch 18 | loss: 0.42327 | train_auc: 0.88882 | valid_auc: 0.88591 |  0:00:10s\n",
      "epoch 19 | loss: 0.41992 | train_auc: 0.88823 | valid_auc: 0.88603 |  0:00:11s\n",
      "epoch 20 | loss: 0.41326 | train_auc: 0.88881 | valid_auc: 0.88689 |  0:00:11s\n",
      "epoch 21 | loss: 0.41798 | train_auc: 0.88888 | valid_auc: 0.88674 |  0:00:12s\n",
      "epoch 22 | loss: 0.41651 | train_auc: 0.88944 | valid_auc: 0.8874  |  0:00:12s\n",
      "epoch 23 | loss: 0.41718 | train_auc: 0.8891  | valid_auc: 0.88685 |  0:00:13s\n",
      "epoch 24 | loss: 0.41452 | train_auc: 0.8896  | valid_auc: 0.88848 |  0:00:13s\n",
      "epoch 25 | loss: 0.41852 | train_auc: 0.88994 | valid_auc: 0.88778 |  0:00:14s\n",
      "epoch 26 | loss: 0.41382 | train_auc: 0.88882 | valid_auc: 0.88583 |  0:00:14s\n",
      "epoch 27 | loss: 0.41375 | train_auc: 0.88914 | valid_auc: 0.88759 |  0:00:15s\n",
      "epoch 28 | loss: 0.42011 | train_auc: 0.88979 | valid_auc: 0.88757 |  0:00:16s\n",
      "epoch 29 | loss: 0.41409 | train_auc: 0.89013 | valid_auc: 0.88782 |  0:00:16s\n",
      "epoch 30 | loss: 0.42547 | train_auc: 0.88949 | valid_auc: 0.88693 |  0:00:17s\n",
      "epoch 31 | loss: 0.41823 | train_auc: 0.88955 | valid_auc: 0.88844 |  0:00:17s\n",
      "epoch 32 | loss: 0.41744 | train_auc: 0.88948 | valid_auc: 0.88821 |  0:00:18s\n",
      "epoch 33 | loss: 0.42263 | train_auc: 0.89014 | valid_auc: 0.88631 |  0:00:18s\n",
      "epoch 34 | loss: 0.41629 | train_auc: 0.88997 | valid_auc: 0.88809 |  0:00:19s\n",
      "epoch 35 | loss: 0.4251  | train_auc: 0.89002 | valid_auc: 0.88713 |  0:00:19s\n",
      "epoch 36 | loss: 0.42134 | train_auc: 0.89034 | valid_auc: 0.88865 |  0:00:20s\n",
      "epoch 37 | loss: 0.41865 | train_auc: 0.89029 | valid_auc: 0.88886 |  0:00:20s\n",
      "epoch 38 | loss: 0.42423 | train_auc: 0.8905  | valid_auc: 0.88731 |  0:00:21s\n",
      "epoch 39 | loss: 0.42128 | train_auc: 0.8904  | valid_auc: 0.88803 |  0:00:22s\n",
      "epoch 40 | loss: 0.41731 | train_auc: 0.8895  | valid_auc: 0.88858 |  0:00:22s\n",
      "epoch 41 | loss: 0.42441 | train_auc: 0.88937 | valid_auc: 0.88727 |  0:00:23s\n",
      "epoch 42 | loss: 0.42161 | train_auc: 0.89045 | valid_auc: 0.8884  |  0:00:23s\n",
      "epoch 43 | loss: 0.4116  | train_auc: 0.88973 | valid_auc: 0.88816 |  0:00:24s\n",
      "epoch 44 | loss: 0.41146 | train_auc: 0.88966 | valid_auc: 0.88848 |  0:00:24s\n",
      "epoch 45 | loss: 0.4207  | train_auc: 0.88925 | valid_auc: 0.88912 |  0:00:25s\n",
      "epoch 46 | loss: 0.40972 | train_auc: 0.89019 | valid_auc: 0.88953 |  0:00:25s\n",
      "epoch 47 | loss: 0.41259 | train_auc: 0.8905  | valid_auc: 0.8888  |  0:00:26s\n",
      "epoch 48 | loss: 0.41408 | train_auc: 0.88993 | valid_auc: 0.88694 |  0:00:27s\n",
      "epoch 49 | loss: 0.41424 | train_auc: 0.89045 | valid_auc: 0.88795 |  0:00:27s\n",
      "epoch 50 | loss: 0.40984 | train_auc: 0.88952 | valid_auc: 0.88759 |  0:00:28s\n",
      "epoch 51 | loss: 0.42087 | train_auc: 0.89018 | valid_auc: 0.88864 |  0:00:28s\n",
      "epoch 52 | loss: 0.41208 | train_auc: 0.89001 | valid_auc: 0.88801 |  0:00:29s\n",
      "epoch 53 | loss: 0.4154  | train_auc: 0.89028 | valid_auc: 0.88645 |  0:00:29s\n",
      "epoch 54 | loss: 0.41487 | train_auc: 0.8904  | valid_auc: 0.88883 |  0:00:30s\n",
      "epoch 55 | loss: 0.41891 | train_auc: 0.89005 | valid_auc: 0.88627 |  0:00:30s\n",
      "epoch 56 | loss: 0.41134 | train_auc: 0.89052 | valid_auc: 0.88851 |  0:00:31s\n",
      "epoch 57 | loss: 0.4112  | train_auc: 0.89076 | valid_auc: 0.88905 |  0:00:31s\n",
      "epoch 58 | loss: 0.41235 | train_auc: 0.89018 | valid_auc: 0.88846 |  0:00:32s\n",
      "epoch 59 | loss: 0.41939 | train_auc: 0.8906  | valid_auc: 0.88726 |  0:00:33s\n",
      "epoch 60 | loss: 0.41397 | train_auc: 0.88974 | valid_auc: 0.88833 |  0:00:33s\n",
      "epoch 61 | loss: 0.40428 | train_auc: 0.8899  | valid_auc: 0.88778 |  0:00:34s\n",
      "epoch 62 | loss: 0.41009 | train_auc: 0.89001 | valid_auc: 0.88739 |  0:00:34s\n",
      "epoch 63 | loss: 0.42812 | train_auc: 0.88961 | valid_auc: 0.88868 |  0:00:35s\n",
      "epoch 64 | loss: 0.41414 | train_auc: 0.89014 | valid_auc: 0.88918 |  0:00:35s\n",
      "epoch 65 | loss: 0.41132 | train_auc: 0.88994 | valid_auc: 0.88822 |  0:00:36s\n",
      "epoch 66 | loss: 0.41261 | train_auc: 0.89008 | valid_auc: 0.88816 |  0:00:36s\n",
      "epoch 67 | loss: 0.41374 | train_auc: 0.88944 | valid_auc: 0.88777 |  0:00:37s\n",
      "epoch 68 | loss: 0.41245 | train_auc: 0.89061 | valid_auc: 0.88905 |  0:00:37s\n",
      "epoch 69 | loss: 0.41206 | train_auc: 0.89073 | valid_auc: 0.88886 |  0:00:38s\n",
      "epoch 70 | loss: 0.41109 | train_auc: 0.89061 | valid_auc: 0.88835 |  0:00:39s\n",
      "epoch 71 | loss: 0.40584 | train_auc: 0.89003 | valid_auc: 0.88811 |  0:00:39s\n",
      "epoch 72 | loss: 0.41053 | train_auc: 0.89027 | valid_auc: 0.88829 |  0:00:40s\n",
      "epoch 73 | loss: 0.41474 | train_auc: 0.89075 | valid_auc: 0.88842 |  0:00:40s\n",
      "epoch 74 | loss: 0.4203  | train_auc: 0.8903  | valid_auc: 0.88834 |  0:00:41s\n",
      "epoch 75 | loss: 0.40625 | train_auc: 0.89065 | valid_auc: 0.88747 |  0:00:41s\n",
      "epoch 76 | loss: 0.41224 | train_auc: 0.89071 | valid_auc: 0.88861 |  0:00:42s\n",
      "epoch 77 | loss: 0.41708 | train_auc: 0.89037 | valid_auc: 0.88912 |  0:00:43s\n",
      "epoch 78 | loss: 0.40635 | train_auc: 0.89054 | valid_auc: 0.88768 |  0:00:43s\n",
      "epoch 79 | loss: 0.41237 | train_auc: 0.89034 | valid_auc: 0.8889  |  0:00:44s\n",
      "epoch 80 | loss: 0.40907 | train_auc: 0.89084 | valid_auc: 0.88828 |  0:00:44s\n",
      "epoch 81 | loss: 0.41686 | train_auc: 0.89079 | valid_auc: 0.8886  |  0:00:45s\n",
      "epoch 82 | loss: 0.41317 | train_auc: 0.891   | valid_auc: 0.88946 |  0:00:45s\n",
      "epoch 83 | loss: 0.40613 | train_auc: 0.89069 | valid_auc: 0.88928 |  0:00:46s\n",
      "epoch 84 | loss: 0.40731 | train_auc: 0.8911  | valid_auc: 0.88801 |  0:00:46s\n",
      "epoch 85 | loss: 0.41098 | train_auc: 0.89085 | valid_auc: 0.88764 |  0:00:47s\n",
      "epoch 86 | loss: 0.41797 | train_auc: 0.89094 | valid_auc: 0.88859 |  0:00:48s\n",
      "epoch 87 | loss: 0.41987 | train_auc: 0.89058 | valid_auc: 0.88799 |  0:00:48s\n",
      "epoch 88 | loss: 0.42062 | train_auc: 0.89009 | valid_auc: 0.8881  |  0:00:49s\n",
      "epoch 89 | loss: 0.41067 | train_auc: 0.8899  | valid_auc: 0.88817 |  0:00:49s\n",
      "epoch 90 | loss: 0.41651 | train_auc: 0.89003 | valid_auc: 0.88805 |  0:00:50s\n",
      "epoch 91 | loss: 0.41972 | train_auc: 0.88974 | valid_auc: 0.88966 |  0:00:50s\n",
      "epoch 92 | loss: 0.40632 | train_auc: 0.89011 | valid_auc: 0.88821 |  0:00:51s\n",
      "epoch 93 | loss: 0.41867 | train_auc: 0.89057 | valid_auc: 0.88821 |  0:00:51s\n",
      "epoch 94 | loss: 0.42035 | train_auc: 0.89072 | valid_auc: 0.88994 |  0:00:52s\n",
      "epoch 95 | loss: 0.41037 | train_auc: 0.89022 | valid_auc: 0.88946 |  0:00:53s\n",
      "epoch 96 | loss: 0.41038 | train_auc: 0.89029 | valid_auc: 0.88908 |  0:00:53s\n",
      "epoch 97 | loss: 0.41443 | train_auc: 0.891   | valid_auc: 0.88828 |  0:00:54s\n",
      "epoch 98 | loss: 0.40732 | train_auc: 0.89088 | valid_auc: 0.88798 |  0:00:54s\n",
      "epoch 99 | loss: 0.42178 | train_auc: 0.89078 | valid_auc: 0.88892 |  0:00:55s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 94 and best_valid_auc = 0.88994\n",
      "sampling:  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69175 | train_auc: 0.75782 | valid_auc: 0.76668 |  0:00:00s\n",
      "epoch 1  | loss: 0.475   | train_auc: 0.82044 | valid_auc: 0.82025 |  0:00:01s\n",
      "epoch 2  | loss: 0.43522 | train_auc: 0.86558 | valid_auc: 0.86628 |  0:00:01s\n",
      "epoch 3  | loss: 0.4253  | train_auc: 0.85903 | valid_auc: 0.85996 |  0:00:02s\n",
      "epoch 4  | loss: 0.42978 | train_auc: 0.87276 | valid_auc: 0.87244 |  0:00:02s\n",
      "epoch 5  | loss: 0.43081 | train_auc: 0.86907 | valid_auc: 0.86889 |  0:00:03s\n",
      "epoch 6  | loss: 0.42648 | train_auc: 0.87972 | valid_auc: 0.87924 |  0:00:03s\n",
      "epoch 7  | loss: 0.42403 | train_auc: 0.87707 | valid_auc: 0.87419 |  0:00:04s\n",
      "epoch 8  | loss: 0.41817 | train_auc: 0.88087 | valid_auc: 0.87881 |  0:00:05s\n",
      "epoch 9  | loss: 0.4194  | train_auc: 0.88212 | valid_auc: 0.87884 |  0:00:05s\n",
      "epoch 10 | loss: 0.42148 | train_auc: 0.88485 | valid_auc: 0.8828  |  0:00:06s\n",
      "epoch 11 | loss: 0.41712 | train_auc: 0.88486 | valid_auc: 0.88157 |  0:00:06s\n",
      "epoch 12 | loss: 0.42685 | train_auc: 0.88599 | valid_auc: 0.882   |  0:00:07s\n",
      "epoch 13 | loss: 0.4212  | train_auc: 0.88615 | valid_auc: 0.88407 |  0:00:07s\n",
      "epoch 14 | loss: 0.40836 | train_auc: 0.88718 | valid_auc: 0.88576 |  0:00:08s\n",
      "epoch 15 | loss: 0.42159 | train_auc: 0.88765 | valid_auc: 0.8854  |  0:00:08s\n",
      "epoch 16 | loss: 0.4205  | train_auc: 0.88788 | valid_auc: 0.88505 |  0:00:09s\n",
      "epoch 17 | loss: 0.42173 | train_auc: 0.88685 | valid_auc: 0.88514 |  0:00:10s\n",
      "epoch 18 | loss: 0.42523 | train_auc: 0.88716 | valid_auc: 0.88584 |  0:00:10s\n",
      "epoch 19 | loss: 0.42064 | train_auc: 0.887   | valid_auc: 0.88331 |  0:00:11s\n",
      "epoch 20 | loss: 0.42057 | train_auc: 0.88887 | valid_auc: 0.8855  |  0:00:11s\n",
      "epoch 21 | loss: 0.42387 | train_auc: 0.88933 | valid_auc: 0.88657 |  0:00:12s\n",
      "epoch 22 | loss: 0.41835 | train_auc: 0.88905 | valid_auc: 0.88733 |  0:00:12s\n",
      "epoch 23 | loss: 0.42557 | train_auc: 0.88906 | valid_auc: 0.88607 |  0:00:13s\n",
      "epoch 24 | loss: 0.41432 | train_auc: 0.89005 | valid_auc: 0.88777 |  0:00:13s\n",
      "epoch 25 | loss: 0.41985 | train_auc: 0.88922 | valid_auc: 0.88769 |  0:00:14s\n",
      "epoch 26 | loss: 0.41452 | train_auc: 0.88995 | valid_auc: 0.88765 |  0:00:15s\n",
      "epoch 27 | loss: 0.41134 | train_auc: 0.88943 | valid_auc: 0.8888  |  0:00:15s\n",
      "epoch 28 | loss: 0.42618 | train_auc: 0.88958 | valid_auc: 0.8875  |  0:00:16s\n",
      "epoch 29 | loss: 0.41845 | train_auc: 0.88998 | valid_auc: 0.88879 |  0:00:16s\n",
      "epoch 30 | loss: 0.41638 | train_auc: 0.88913 | valid_auc: 0.88764 |  0:00:17s\n",
      "epoch 31 | loss: 0.41903 | train_auc: 0.88913 | valid_auc: 0.8889  |  0:00:17s\n",
      "epoch 32 | loss: 0.41515 | train_auc: 0.88995 | valid_auc: 0.88887 |  0:00:18s\n",
      "epoch 33 | loss: 0.42142 | train_auc: 0.89024 | valid_auc: 0.88962 |  0:00:18s\n",
      "epoch 34 | loss: 0.42695 | train_auc: 0.88987 | valid_auc: 0.88765 |  0:00:19s\n",
      "epoch 35 | loss: 0.41427 | train_auc: 0.89057 | valid_auc: 0.88899 |  0:00:20s\n",
      "epoch 36 | loss: 0.41342 | train_auc: 0.8902  | valid_auc: 0.88762 |  0:00:20s\n",
      "epoch 37 | loss: 0.41124 | train_auc: 0.89071 | valid_auc: 0.88874 |  0:00:21s\n",
      "epoch 38 | loss: 0.41965 | train_auc: 0.89019 | valid_auc: 0.88855 |  0:00:21s\n",
      "epoch 39 | loss: 0.41558 | train_auc: 0.89019 | valid_auc: 0.88865 |  0:00:22s\n",
      "epoch 40 | loss: 0.41711 | train_auc: 0.88956 | valid_auc: 0.88942 |  0:00:22s\n",
      "epoch 41 | loss: 0.42098 | train_auc: 0.89056 | valid_auc: 0.88894 |  0:00:23s\n",
      "epoch 42 | loss: 0.41072 | train_auc: 0.89042 | valid_auc: 0.8877  |  0:00:23s\n",
      "epoch 43 | loss: 0.40912 | train_auc: 0.89034 | valid_auc: 0.88906 |  0:00:24s\n",
      "epoch 44 | loss: 0.42063 | train_auc: 0.89053 | valid_auc: 0.88688 |  0:00:25s\n",
      "epoch 45 | loss: 0.41837 | train_auc: 0.89005 | valid_auc: 0.88734 |  0:00:25s\n",
      "epoch 46 | loss: 0.41428 | train_auc: 0.89093 | valid_auc: 0.88849 |  0:00:26s\n",
      "epoch 47 | loss: 0.41118 | train_auc: 0.8909  | valid_auc: 0.88854 |  0:00:26s\n",
      "epoch 48 | loss: 0.42233 | train_auc: 0.88996 | valid_auc: 0.88849 |  0:00:27s\n",
      "epoch 49 | loss: 0.41529 | train_auc: 0.89092 | valid_auc: 0.88866 |  0:00:27s\n",
      "epoch 50 | loss: 0.41155 | train_auc: 0.88971 | valid_auc: 0.88659 |  0:00:28s\n",
      "epoch 51 | loss: 0.41587 | train_auc: 0.89016 | valid_auc: 0.88666 |  0:00:28s\n",
      "epoch 52 | loss: 0.41291 | train_auc: 0.88966 | valid_auc: 0.88852 |  0:00:29s\n",
      "epoch 53 | loss: 0.41256 | train_auc: 0.89007 | valid_auc: 0.88931 |  0:00:29s\n",
      "epoch 54 | loss: 0.41178 | train_auc: 0.8902  | valid_auc: 0.88858 |  0:00:30s\n",
      "epoch 55 | loss: 0.40996 | train_auc: 0.89013 | valid_auc: 0.88751 |  0:00:31s\n",
      "epoch 56 | loss: 0.42393 | train_auc: 0.88986 | valid_auc: 0.88846 |  0:00:31s\n",
      "epoch 57 | loss: 0.41473 | train_auc: 0.89025 | valid_auc: 0.8882  |  0:00:32s\n",
      "epoch 58 | loss: 0.4165  | train_auc: 0.88982 | valid_auc: 0.88901 |  0:00:32s\n",
      "epoch 59 | loss: 0.41083 | train_auc: 0.89083 | valid_auc: 0.88796 |  0:00:33s\n",
      "epoch 60 | loss: 0.41874 | train_auc: 0.89086 | valid_auc: 0.88879 |  0:00:33s\n",
      "epoch 61 | loss: 0.42171 | train_auc: 0.89114 | valid_auc: 0.88813 |  0:00:34s\n",
      "epoch 62 | loss: 0.4198  | train_auc: 0.89095 | valid_auc: 0.88915 |  0:00:34s\n",
      "epoch 63 | loss: 0.41309 | train_auc: 0.89057 | valid_auc: 0.8885  |  0:00:35s\n",
      "epoch 64 | loss: 0.41779 | train_auc: 0.89097 | valid_auc: 0.88915 |  0:00:36s\n",
      "epoch 65 | loss: 0.41241 | train_auc: 0.89114 | valid_auc: 0.8897  |  0:00:36s\n",
      "epoch 66 | loss: 0.40751 | train_auc: 0.8911  | valid_auc: 0.88916 |  0:00:37s\n",
      "epoch 67 | loss: 0.41783 | train_auc: 0.89004 | valid_auc: 0.88706 |  0:00:37s\n",
      "epoch 68 | loss: 0.41269 | train_auc: 0.89049 | valid_auc: 0.88749 |  0:00:38s\n",
      "epoch 69 | loss: 0.41919 | train_auc: 0.89112 | valid_auc: 0.88914 |  0:00:38s\n",
      "epoch 70 | loss: 0.41387 | train_auc: 0.89124 | valid_auc: 0.88946 |  0:00:39s\n",
      "epoch 71 | loss: 0.41676 | train_auc: 0.89042 | valid_auc: 0.88702 |  0:00:39s\n",
      "epoch 72 | loss: 0.41035 | train_auc: 0.89139 | valid_auc: 0.88886 |  0:00:40s\n",
      "epoch 73 | loss: 0.41539 | train_auc: 0.89069 | valid_auc: 0.88681 |  0:00:40s\n",
      "epoch 74 | loss: 0.41053 | train_auc: 0.89022 | valid_auc: 0.88983 |  0:00:41s\n",
      "epoch 75 | loss: 0.41467 | train_auc: 0.891   | valid_auc: 0.88822 |  0:00:42s\n",
      "epoch 76 | loss: 0.41193 | train_auc: 0.89076 | valid_auc: 0.88781 |  0:00:42s\n",
      "epoch 77 | loss: 0.41545 | train_auc: 0.89079 | valid_auc: 0.88737 |  0:00:43s\n",
      "epoch 78 | loss: 0.41475 | train_auc: 0.89084 | valid_auc: 0.88907 |  0:00:43s\n",
      "epoch 79 | loss: 0.4111  | train_auc: 0.8912  | valid_auc: 0.88818 |  0:00:44s\n",
      "epoch 80 | loss: 0.40898 | train_auc: 0.89059 | valid_auc: 0.88943 |  0:00:44s\n",
      "epoch 81 | loss: 0.41085 | train_auc: 0.89103 | valid_auc: 0.88881 |  0:00:45s\n",
      "epoch 82 | loss: 0.41082 | train_auc: 0.89069 | valid_auc: 0.88936 |  0:00:45s\n",
      "epoch 83 | loss: 0.42384 | train_auc: 0.89139 | valid_auc: 0.88882 |  0:00:46s\n",
      "epoch 84 | loss: 0.41342 | train_auc: 0.89125 | valid_auc: 0.88888 |  0:00:46s\n",
      "epoch 85 | loss: 0.41269 | train_auc: 0.89091 | valid_auc: 0.88863 |  0:00:47s\n",
      "epoch 86 | loss: 0.41912 | train_auc: 0.89037 | valid_auc: 0.8859  |  0:00:48s\n",
      "epoch 87 | loss: 0.40431 | train_auc: 0.89132 | valid_auc: 0.88893 |  0:00:48s\n",
      "epoch 88 | loss: 0.41339 | train_auc: 0.891   | valid_auc: 0.88849 |  0:00:49s\n",
      "epoch 89 | loss: 0.41591 | train_auc: 0.89091 | valid_auc: 0.88882 |  0:00:49s\n",
      "epoch 90 | loss: 0.41301 | train_auc: 0.89051 | valid_auc: 0.88827 |  0:00:50s\n",
      "epoch 91 | loss: 0.41114 | train_auc: 0.89103 | valid_auc: 0.88954 |  0:00:50s\n",
      "epoch 92 | loss: 0.40933 | train_auc: 0.89091 | valid_auc: 0.8897  |  0:00:51s\n",
      "epoch 93 | loss: 0.41647 | train_auc: 0.89087 | valid_auc: 0.88877 |  0:00:51s\n",
      "epoch 94 | loss: 0.41428 | train_auc: 0.89095 | valid_auc: 0.88873 |  0:00:52s\n",
      "epoch 95 | loss: 0.41378 | train_auc: 0.89134 | valid_auc: 0.88814 |  0:00:53s\n",
      "epoch 96 | loss: 0.41847 | train_auc: 0.89117 | valid_auc: 0.88784 |  0:00:53s\n",
      "epoch 97 | loss: 0.41179 | train_auc: 0.89144 | valid_auc: 0.88865 |  0:00:54s\n",
      "epoch 98 | loss: 0.41632 | train_auc: 0.89123 | valid_auc: 0.88902 |  0:00:54s\n",
      "epoch 99 | loss: 0.41368 | train_auc: 0.8908  | valid_auc: 0.88841 |  0:00:55s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 74 and best_valid_auc = 0.88983\n",
      "sampling:  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.68123 | train_auc: 0.63924 | valid_auc: 0.62702 |  0:00:00s\n",
      "epoch 1  | loss: 0.46974 | train_auc: 0.80721 | valid_auc: 0.79093 |  0:00:01s\n",
      "epoch 2  | loss: 0.44599 | train_auc: 0.80958 | valid_auc: 0.80016 |  0:00:01s\n",
      "epoch 3  | loss: 0.43908 | train_auc: 0.83966 | valid_auc: 0.83204 |  0:00:02s\n",
      "epoch 4  | loss: 0.43377 | train_auc: 0.85646 | valid_auc: 0.8433  |  0:00:02s\n",
      "epoch 5  | loss: 0.42594 | train_auc: 0.86345 | valid_auc: 0.85271 |  0:00:03s\n",
      "epoch 6  | loss: 0.42685 | train_auc: 0.87486 | valid_auc: 0.86489 |  0:00:03s\n",
      "epoch 7  | loss: 0.41554 | train_auc: 0.88192 | valid_auc: 0.87303 |  0:00:04s\n",
      "epoch 8  | loss: 0.42934 | train_auc: 0.88298 | valid_auc: 0.87316 |  0:00:04s\n",
      "epoch 9  | loss: 0.42121 | train_auc: 0.88384 | valid_auc: 0.87448 |  0:00:05s\n",
      "epoch 10 | loss: 0.41288 | train_auc: 0.88495 | valid_auc: 0.87525 |  0:00:06s\n",
      "epoch 11 | loss: 0.41438 | train_auc: 0.88968 | valid_auc: 0.88037 |  0:00:06s\n",
      "epoch 12 | loss: 0.4183  | train_auc: 0.88712 | valid_auc: 0.88036 |  0:00:07s\n",
      "epoch 13 | loss: 0.41461 | train_auc: 0.88926 | valid_auc: 0.88033 |  0:00:07s\n",
      "epoch 14 | loss: 0.41887 | train_auc: 0.89006 | valid_auc: 0.88168 |  0:00:08s\n",
      "epoch 15 | loss: 0.41853 | train_auc: 0.8913  | valid_auc: 0.88236 |  0:00:08s\n",
      "epoch 16 | loss: 0.4069  | train_auc: 0.89151 | valid_auc: 0.88261 |  0:00:09s\n",
      "epoch 17 | loss: 0.41775 | train_auc: 0.89148 | valid_auc: 0.88207 |  0:00:09s\n",
      "epoch 18 | loss: 0.40823 | train_auc: 0.8923  | valid_auc: 0.88184 |  0:00:10s\n",
      "epoch 19 | loss: 0.41409 | train_auc: 0.89085 | valid_auc: 0.88168 |  0:00:10s\n",
      "epoch 20 | loss: 0.41222 | train_auc: 0.89106 | valid_auc: 0.88234 |  0:00:11s\n",
      "epoch 21 | loss: 0.41517 | train_auc: 0.89094 | valid_auc: 0.88311 |  0:00:12s\n",
      "epoch 22 | loss: 0.41886 | train_auc: 0.89146 | valid_auc: 0.88224 |  0:00:12s\n",
      "epoch 23 | loss: 0.40923 | train_auc: 0.89151 | valid_auc: 0.88196 |  0:00:13s\n",
      "epoch 24 | loss: 0.41005 | train_auc: 0.89149 | valid_auc: 0.88205 |  0:00:13s\n",
      "epoch 25 | loss: 0.41336 | train_auc: 0.89207 | valid_auc: 0.88332 |  0:00:14s\n",
      "epoch 26 | loss: 0.41048 | train_auc: 0.89225 | valid_auc: 0.88269 |  0:00:14s\n",
      "epoch 27 | loss: 0.40923 | train_auc: 0.89214 | valid_auc: 0.88314 |  0:00:15s\n",
      "epoch 28 | loss: 0.4132  | train_auc: 0.89232 | valid_auc: 0.88298 |  0:00:16s\n",
      "epoch 29 | loss: 0.41156 | train_auc: 0.89223 | valid_auc: 0.88242 |  0:00:16s\n",
      "epoch 30 | loss: 0.41506 | train_auc: 0.89215 | valid_auc: 0.88289 |  0:00:17s\n",
      "epoch 31 | loss: 0.41596 | train_auc: 0.89212 | valid_auc: 0.88426 |  0:00:17s\n",
      "epoch 32 | loss: 0.41392 | train_auc: 0.89244 | valid_auc: 0.88416 |  0:00:18s\n",
      "epoch 33 | loss: 0.42301 | train_auc: 0.89191 | valid_auc: 0.88435 |  0:00:18s\n",
      "epoch 34 | loss: 0.41062 | train_auc: 0.89249 | valid_auc: 0.88368 |  0:00:19s\n",
      "epoch 35 | loss: 0.41657 | train_auc: 0.8929  | valid_auc: 0.88393 |  0:00:19s\n",
      "epoch 36 | loss: 0.41505 | train_auc: 0.89282 | valid_auc: 0.88491 |  0:00:20s\n",
      "epoch 37 | loss: 0.41579 | train_auc: 0.89213 | valid_auc: 0.88459 |  0:00:20s\n",
      "epoch 38 | loss: 0.41882 | train_auc: 0.89208 | valid_auc: 0.88368 |  0:00:21s\n",
      "epoch 39 | loss: 0.416   | train_auc: 0.89223 | valid_auc: 0.88377 |  0:00:22s\n",
      "epoch 40 | loss: 0.41484 | train_auc: 0.89278 | valid_auc: 0.88339 |  0:00:22s\n",
      "epoch 41 | loss: 0.41207 | train_auc: 0.89249 | valid_auc: 0.88365 |  0:00:23s\n",
      "epoch 42 | loss: 0.40817 | train_auc: 0.89277 | valid_auc: 0.88345 |  0:00:23s\n",
      "epoch 43 | loss: 0.40153 | train_auc: 0.89257 | valid_auc: 0.88474 |  0:00:24s\n",
      "epoch 44 | loss: 0.41866 | train_auc: 0.89299 | valid_auc: 0.88398 |  0:00:24s\n",
      "epoch 45 | loss: 0.41398 | train_auc: 0.89307 | valid_auc: 0.88322 |  0:00:25s\n",
      "epoch 46 | loss: 0.4081  | train_auc: 0.89308 | valid_auc: 0.88372 |  0:00:25s\n",
      "epoch 47 | loss: 0.40749 | train_auc: 0.89319 | valid_auc: 0.8842  |  0:00:26s\n",
      "epoch 48 | loss: 0.40942 | train_auc: 0.8929  | valid_auc: 0.88304 |  0:00:27s\n",
      "epoch 49 | loss: 0.41663 | train_auc: 0.89276 | valid_auc: 0.8835  |  0:00:27s\n",
      "epoch 50 | loss: 0.41825 | train_auc: 0.89273 | valid_auc: 0.88392 |  0:00:28s\n",
      "epoch 51 | loss: 0.41477 | train_auc: 0.89296 | valid_auc: 0.8846  |  0:00:28s\n",
      "epoch 52 | loss: 0.41346 | train_auc: 0.8922  | valid_auc: 0.88481 |  0:00:29s\n",
      "epoch 53 | loss: 0.40316 | train_auc: 0.89222 | valid_auc: 0.8851  |  0:00:29s\n",
      "epoch 54 | loss: 0.41406 | train_auc: 0.89286 | valid_auc: 0.88434 |  0:00:30s\n",
      "epoch 55 | loss: 0.39858 | train_auc: 0.89295 | valid_auc: 0.88486 |  0:00:30s\n",
      "epoch 56 | loss: 0.41228 | train_auc: 0.89278 | valid_auc: 0.8848  |  0:00:31s\n",
      "epoch 57 | loss: 0.42062 | train_auc: 0.893   | valid_auc: 0.88457 |  0:00:32s\n",
      "epoch 58 | loss: 0.41687 | train_auc: 0.89224 | valid_auc: 0.88426 |  0:00:32s\n",
      "epoch 59 | loss: 0.40538 | train_auc: 0.89303 | valid_auc: 0.88459 |  0:00:33s\n",
      "epoch 60 | loss: 0.4056  | train_auc: 0.89333 | valid_auc: 0.88357 |  0:00:33s\n",
      "epoch 61 | loss: 0.41624 | train_auc: 0.89337 | valid_auc: 0.88417 |  0:00:34s\n",
      "epoch 62 | loss: 0.40522 | train_auc: 0.89316 | valid_auc: 0.88263 |  0:00:34s\n",
      "epoch 63 | loss: 0.40053 | train_auc: 0.89314 | valid_auc: 0.88384 |  0:00:35s\n",
      "epoch 64 | loss: 0.40618 | train_auc: 0.89348 | valid_auc: 0.88482 |  0:00:35s\n",
      "epoch 65 | loss: 0.40876 | train_auc: 0.89318 | valid_auc: 0.88281 |  0:00:36s\n",
      "epoch 66 | loss: 0.40953 | train_auc: 0.89228 | valid_auc: 0.88425 |  0:00:37s\n",
      "epoch 67 | loss: 0.41475 | train_auc: 0.89329 | valid_auc: 0.8844  |  0:00:37s\n",
      "epoch 68 | loss: 0.41416 | train_auc: 0.89312 | valid_auc: 0.88339 |  0:00:38s\n",
      "epoch 69 | loss: 0.4142  | train_auc: 0.89321 | valid_auc: 0.88444 |  0:00:38s\n",
      "epoch 70 | loss: 0.41142 | train_auc: 0.89341 | valid_auc: 0.88473 |  0:00:39s\n",
      "epoch 71 | loss: 0.40709 | train_auc: 0.89362 | valid_auc: 0.88494 |  0:00:39s\n",
      "epoch 72 | loss: 0.41311 | train_auc: 0.89269 | valid_auc: 0.88419 |  0:00:40s\n",
      "epoch 73 | loss: 0.40979 | train_auc: 0.89335 | valid_auc: 0.88509 |  0:00:41s\n",
      "epoch 74 | loss: 0.40944 | train_auc: 0.89352 | valid_auc: 0.88403 |  0:00:41s\n",
      "epoch 75 | loss: 0.41563 | train_auc: 0.89359 | valid_auc: 0.88437 |  0:00:42s\n",
      "epoch 76 | loss: 0.40903 | train_auc: 0.89343 | valid_auc: 0.88522 |  0:00:42s\n",
      "epoch 77 | loss: 0.40736 | train_auc: 0.89331 | valid_auc: 0.88362 |  0:00:43s\n",
      "epoch 78 | loss: 0.40391 | train_auc: 0.89375 | valid_auc: 0.88433 |  0:00:43s\n",
      "epoch 79 | loss: 0.41238 | train_auc: 0.8934  | valid_auc: 0.88326 |  0:00:44s\n",
      "epoch 80 | loss: 0.41477 | train_auc: 0.89365 | valid_auc: 0.88325 |  0:00:44s\n",
      "epoch 81 | loss: 0.41347 | train_auc: 0.8934  | valid_auc: 0.88393 |  0:00:45s\n",
      "epoch 82 | loss: 0.40948 | train_auc: 0.89354 | valid_auc: 0.884   |  0:00:46s\n",
      "epoch 83 | loss: 0.41721 | train_auc: 0.89349 | valid_auc: 0.88463 |  0:00:46s\n",
      "epoch 84 | loss: 0.40676 | train_auc: 0.89348 | valid_auc: 0.88365 |  0:00:47s\n",
      "epoch 85 | loss: 0.40801 | train_auc: 0.89362 | valid_auc: 0.88408 |  0:00:47s\n",
      "epoch 86 | loss: 0.41094 | train_auc: 0.89371 | valid_auc: 0.88332 |  0:00:48s\n",
      "epoch 87 | loss: 0.40957 | train_auc: 0.89356 | valid_auc: 0.88464 |  0:00:48s\n",
      "epoch 88 | loss: 0.41    | train_auc: 0.89409 | valid_auc: 0.88412 |  0:00:49s\n",
      "epoch 89 | loss: 0.41176 | train_auc: 0.89354 | valid_auc: 0.8843  |  0:00:49s\n",
      "epoch 90 | loss: 0.40949 | train_auc: 0.89322 | valid_auc: 0.88451 |  0:00:50s\n",
      "epoch 91 | loss: 0.41248 | train_auc: 0.89329 | valid_auc: 0.88399 |  0:00:51s\n",
      "epoch 92 | loss: 0.41156 | train_auc: 0.89382 | valid_auc: 0.88385 |  0:00:51s\n",
      "epoch 93 | loss: 0.41153 | train_auc: 0.8938  | valid_auc: 0.88554 |  0:00:52s\n",
      "epoch 94 | loss: 0.41356 | train_auc: 0.89362 | valid_auc: 0.88511 |  0:00:52s\n",
      "epoch 95 | loss: 0.40437 | train_auc: 0.8938  | valid_auc: 0.88526 |  0:00:53s\n",
      "epoch 96 | loss: 0.4139  | train_auc: 0.8942  | valid_auc: 0.88493 |  0:00:53s\n",
      "epoch 97 | loss: 0.41762 | train_auc: 0.89411 | valid_auc: 0.88492 |  0:00:54s\n",
      "epoch 98 | loss: 0.40839 | train_auc: 0.89389 | valid_auc: 0.88451 |  0:00:54s\n",
      "epoch 99 | loss: 0.40709 | train_auc: 0.89376 | valid_auc: 0.88418 |  0:00:55s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 93 and best_valid_auc = 0.88554\n",
      "sampling:  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69442 | train_auc: 0.74544 | valid_auc: 0.7314  |  0:00:00s\n",
      "epoch 1  | loss: 0.46872 | train_auc: 0.8442  | valid_auc: 0.83197 |  0:00:01s\n",
      "epoch 2  | loss: 0.43395 | train_auc: 0.82151 | valid_auc: 0.8133  |  0:00:01s\n",
      "epoch 3  | loss: 0.43143 | train_auc: 0.85409 | valid_auc: 0.84331 |  0:00:02s\n",
      "epoch 4  | loss: 0.42429 | train_auc: 0.86749 | valid_auc: 0.85917 |  0:00:02s\n",
      "epoch 5  | loss: 0.42188 | train_auc: 0.87267 | valid_auc: 0.86339 |  0:00:03s\n",
      "epoch 6  | loss: 0.419   | train_auc: 0.87429 | valid_auc: 0.86273 |  0:00:03s\n",
      "epoch 7  | loss: 0.42038 | train_auc: 0.8763  | valid_auc: 0.8654  |  0:00:04s\n",
      "epoch 8  | loss: 0.41839 | train_auc: 0.88008 | valid_auc: 0.86903 |  0:00:05s\n",
      "epoch 9  | loss: 0.41482 | train_auc: 0.88193 | valid_auc: 0.8705  |  0:00:05s\n",
      "epoch 10 | loss: 0.41479 | train_auc: 0.88077 | valid_auc: 0.87132 |  0:00:06s\n",
      "epoch 11 | loss: 0.41333 | train_auc: 0.88557 | valid_auc: 0.87602 |  0:00:06s\n",
      "epoch 12 | loss: 0.40635 | train_auc: 0.8874  | valid_auc: 0.87842 |  0:00:07s\n",
      "epoch 13 | loss: 0.41809 | train_auc: 0.88672 | valid_auc: 0.87659 |  0:00:07s\n",
      "epoch 14 | loss: 0.41372 | train_auc: 0.8881  | valid_auc: 0.87796 |  0:00:08s\n",
      "epoch 15 | loss: 0.4136  | train_auc: 0.88894 | valid_auc: 0.87875 |  0:00:08s\n",
      "epoch 16 | loss: 0.41929 | train_auc: 0.88942 | valid_auc: 0.8795  |  0:00:09s\n",
      "epoch 17 | loss: 0.41137 | train_auc: 0.89001 | valid_auc: 0.8804  |  0:00:10s\n",
      "epoch 18 | loss: 0.4215  | train_auc: 0.89042 | valid_auc: 0.87964 |  0:00:10s\n",
      "epoch 19 | loss: 0.4117  | train_auc: 0.89021 | valid_auc: 0.88039 |  0:00:11s\n",
      "epoch 20 | loss: 0.42081 | train_auc: 0.89136 | valid_auc: 0.88197 |  0:00:11s\n",
      "epoch 21 | loss: 0.41828 | train_auc: 0.8899  | valid_auc: 0.87947 |  0:00:12s\n",
      "epoch 22 | loss: 0.41193 | train_auc: 0.8911  | valid_auc: 0.88041 |  0:00:12s\n",
      "epoch 23 | loss: 0.41169 | train_auc: 0.89116 | valid_auc: 0.88105 |  0:00:13s\n",
      "epoch 24 | loss: 0.40975 | train_auc: 0.89087 | valid_auc: 0.8807  |  0:00:13s\n",
      "epoch 25 | loss: 0.41275 | train_auc: 0.89115 | valid_auc: 0.88092 |  0:00:14s\n",
      "epoch 26 | loss: 0.41309 | train_auc: 0.89142 | valid_auc: 0.88074 |  0:00:15s\n",
      "epoch 27 | loss: 0.41777 | train_auc: 0.89156 | valid_auc: 0.88113 |  0:00:15s\n",
      "epoch 28 | loss: 0.4079  | train_auc: 0.8913  | valid_auc: 0.88217 |  0:00:16s\n",
      "epoch 29 | loss: 0.40862 | train_auc: 0.8907  | valid_auc: 0.88268 |  0:00:16s\n",
      "epoch 30 | loss: 0.41271 | train_auc: 0.89102 | valid_auc: 0.88219 |  0:00:17s\n",
      "epoch 31 | loss: 0.41943 | train_auc: 0.89065 | valid_auc: 0.88114 |  0:00:17s\n",
      "epoch 32 | loss: 0.41373 | train_auc: 0.8909  | valid_auc: 0.88125 |  0:00:18s\n",
      "epoch 33 | loss: 0.41849 | train_auc: 0.89082 | valid_auc: 0.88078 |  0:00:18s\n",
      "epoch 34 | loss: 0.41244 | train_auc: 0.88978 | valid_auc: 0.88073 |  0:00:19s\n",
      "epoch 35 | loss: 0.42046 | train_auc: 0.89087 | valid_auc: 0.8828  |  0:00:19s\n",
      "epoch 36 | loss: 0.41239 | train_auc: 0.89161 | valid_auc: 0.88184 |  0:00:20s\n",
      "epoch 37 | loss: 0.41354 | train_auc: 0.89189 | valid_auc: 0.88144 |  0:00:21s\n",
      "epoch 38 | loss: 0.40658 | train_auc: 0.892   | valid_auc: 0.87993 |  0:00:21s\n",
      "epoch 39 | loss: 0.42324 | train_auc: 0.89181 | valid_auc: 0.8809  |  0:00:22s\n",
      "epoch 40 | loss: 0.40672 | train_auc: 0.89224 | valid_auc: 0.88121 |  0:00:22s\n",
      "epoch 41 | loss: 0.41079 | train_auc: 0.8915  | valid_auc: 0.88198 |  0:00:23s\n",
      "epoch 42 | loss: 0.40482 | train_auc: 0.8915  | valid_auc: 0.88115 |  0:00:24s\n",
      "epoch 43 | loss: 0.41005 | train_auc: 0.89151 | valid_auc: 0.88211 |  0:00:24s\n",
      "epoch 44 | loss: 0.40681 | train_auc: 0.8912  | valid_auc: 0.8825  |  0:00:25s\n",
      "epoch 45 | loss: 0.41089 | train_auc: 0.89173 | valid_auc: 0.88121 |  0:00:25s\n",
      "epoch 46 | loss: 0.40795 | train_auc: 0.89079 | valid_auc: 0.88072 |  0:00:26s\n",
      "epoch 47 | loss: 0.40987 | train_auc: 0.89106 | valid_auc: 0.88052 |  0:00:26s\n",
      "epoch 48 | loss: 0.41216 | train_auc: 0.89114 | valid_auc: 0.88229 |  0:00:27s\n",
      "epoch 49 | loss: 0.40141 | train_auc: 0.89158 | valid_auc: 0.88061 |  0:00:27s\n",
      "epoch 50 | loss: 0.41944 | train_auc: 0.89032 | valid_auc: 0.87932 |  0:00:28s\n",
      "epoch 51 | loss: 0.41531 | train_auc: 0.89148 | valid_auc: 0.88038 |  0:00:28s\n",
      "epoch 52 | loss: 0.41365 | train_auc: 0.89105 | valid_auc: 0.87988 |  0:00:29s\n",
      "epoch 53 | loss: 0.41265 | train_auc: 0.89149 | valid_auc: 0.88067 |  0:00:30s\n",
      "epoch 54 | loss: 0.41555 | train_auc: 0.89152 | valid_auc: 0.88025 |  0:00:30s\n",
      "epoch 55 | loss: 0.42083 | train_auc: 0.8911  | valid_auc: 0.88173 |  0:00:31s\n",
      "epoch 56 | loss: 0.41794 | train_auc: 0.89153 | valid_auc: 0.8808  |  0:00:31s\n",
      "epoch 57 | loss: 0.41195 | train_auc: 0.89122 | valid_auc: 0.88003 |  0:00:32s\n",
      "epoch 58 | loss: 0.40288 | train_auc: 0.89165 | valid_auc: 0.88145 |  0:00:32s\n",
      "epoch 59 | loss: 0.40608 | train_auc: 0.89038 | valid_auc: 0.87803 |  0:00:33s\n",
      "epoch 60 | loss: 0.40967 | train_auc: 0.89169 | valid_auc: 0.87958 |  0:00:34s\n",
      "epoch 61 | loss: 0.4178  | train_auc: 0.89108 | valid_auc: 0.88074 |  0:00:34s\n",
      "epoch 62 | loss: 0.40925 | train_auc: 0.89182 | valid_auc: 0.88041 |  0:00:35s\n",
      "epoch 63 | loss: 0.40637 | train_auc: 0.89193 | valid_auc: 0.88079 |  0:00:35s\n",
      "epoch 64 | loss: 0.39858 | train_auc: 0.89186 | valid_auc: 0.8814  |  0:00:36s\n",
      "epoch 65 | loss: 0.41837 | train_auc: 0.89182 | valid_auc: 0.88176 |  0:00:36s\n",
      "epoch 66 | loss: 0.40559 | train_auc: 0.89177 | valid_auc: 0.88103 |  0:00:37s\n",
      "epoch 67 | loss: 0.41275 | train_auc: 0.89148 | valid_auc: 0.88101 |  0:00:37s\n",
      "epoch 68 | loss: 0.40903 | train_auc: 0.89205 | valid_auc: 0.88156 |  0:00:38s\n",
      "epoch 69 | loss: 0.41758 | train_auc: 0.89178 | valid_auc: 0.88087 |  0:00:38s\n",
      "epoch 70 | loss: 0.4143  | train_auc: 0.89217 | valid_auc: 0.8807  |  0:00:39s\n",
      "epoch 71 | loss: 0.42179 | train_auc: 0.89195 | valid_auc: 0.87983 |  0:00:40s\n",
      "epoch 72 | loss: 0.41152 | train_auc: 0.89121 | valid_auc: 0.88025 |  0:00:40s\n",
      "epoch 73 | loss: 0.41541 | train_auc: 0.8923  | valid_auc: 0.88106 |  0:00:41s\n",
      "epoch 74 | loss: 0.41274 | train_auc: 0.89247 | valid_auc: 0.88003 |  0:00:41s\n",
      "epoch 75 | loss: 0.41693 | train_auc: 0.89228 | valid_auc: 0.87973 |  0:00:42s\n",
      "epoch 76 | loss: 0.41683 | train_auc: 0.89225 | valid_auc: 0.88092 |  0:00:42s\n",
      "epoch 77 | loss: 0.41067 | train_auc: 0.89161 | valid_auc: 0.8808  |  0:00:43s\n",
      "epoch 78 | loss: 0.40611 | train_auc: 0.89207 | valid_auc: 0.88146 |  0:00:43s\n",
      "epoch 79 | loss: 0.41558 | train_auc: 0.89222 | valid_auc: 0.8818  |  0:00:44s\n",
      "epoch 80 | loss: 0.40817 | train_auc: 0.89202 | valid_auc: 0.88051 |  0:00:45s\n",
      "epoch 81 | loss: 0.40078 | train_auc: 0.89241 | valid_auc: 0.88067 |  0:00:45s\n",
      "epoch 82 | loss: 0.4105  | train_auc: 0.89216 | valid_auc: 0.88092 |  0:00:46s\n",
      "epoch 83 | loss: 0.41272 | train_auc: 0.89222 | valid_auc: 0.88069 |  0:00:46s\n",
      "epoch 84 | loss: 0.41133 | train_auc: 0.89198 | valid_auc: 0.88036 |  0:00:47s\n",
      "epoch 85 | loss: 0.4082  | train_auc: 0.89155 | valid_auc: 0.88074 |  0:00:47s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 35 and best_valid_auc = 0.8828\n",
      "sampling:  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69419 | train_auc: 0.48899 | valid_auc: 0.50816 |  0:00:00s\n",
      "epoch 1  | loss: 0.49249 | train_auc: 0.49932 | valid_auc: 0.48605 |  0:00:01s\n",
      "epoch 2  | loss: 0.45937 | train_auc: 0.75382 | valid_auc: 0.76285 |  0:00:01s\n",
      "epoch 3  | loss: 0.43828 | train_auc: 0.81499 | valid_auc: 0.81414 |  0:00:02s\n",
      "epoch 4  | loss: 0.43824 | train_auc: 0.85022 | valid_auc: 0.86162 |  0:00:02s\n",
      "epoch 5  | loss: 0.4378  | train_auc: 0.85109 | valid_auc: 0.85992 |  0:00:03s\n",
      "epoch 6  | loss: 0.43185 | train_auc: 0.86883 | valid_auc: 0.87834 |  0:00:03s\n",
      "epoch 7  | loss: 0.4263  | train_auc: 0.86785 | valid_auc: 0.87884 |  0:00:04s\n",
      "epoch 8  | loss: 0.4292  | train_auc: 0.87644 | valid_auc: 0.88597 |  0:00:04s\n",
      "epoch 9  | loss: 0.42786 | train_auc: 0.87633 | valid_auc: 0.88528 |  0:00:05s\n",
      "epoch 10 | loss: 0.4268  | train_auc: 0.88124 | valid_auc: 0.89074 |  0:00:06s\n",
      "epoch 11 | loss: 0.41912 | train_auc: 0.88148 | valid_auc: 0.89208 |  0:00:06s\n",
      "epoch 12 | loss: 0.42457 | train_auc: 0.88248 | valid_auc: 0.89211 |  0:00:07s\n",
      "epoch 13 | loss: 0.42443 | train_auc: 0.88151 | valid_auc: 0.89126 |  0:00:07s\n",
      "epoch 14 | loss: 0.41928 | train_auc: 0.88442 | valid_auc: 0.89223 |  0:00:08s\n",
      "epoch 15 | loss: 0.4188  | train_auc: 0.88581 | valid_auc: 0.89319 |  0:00:08s\n",
      "epoch 16 | loss: 0.41912 | train_auc: 0.88453 | valid_auc: 0.89238 |  0:00:09s\n",
      "epoch 17 | loss: 0.42209 | train_auc: 0.88478 | valid_auc: 0.89401 |  0:00:09s\n",
      "epoch 18 | loss: 0.41473 | train_auc: 0.88486 | valid_auc: 0.89442 |  0:00:10s\n",
      "epoch 19 | loss: 0.43006 | train_auc: 0.88579 | valid_auc: 0.89579 |  0:00:10s\n",
      "epoch 20 | loss: 0.42444 | train_auc: 0.88544 | valid_auc: 0.89456 |  0:00:11s\n",
      "epoch 21 | loss: 0.42608 | train_auc: 0.88548 | valid_auc: 0.89572 |  0:00:12s\n",
      "epoch 22 | loss: 0.42124 | train_auc: 0.88561 | valid_auc: 0.89543 |  0:00:12s\n",
      "epoch 23 | loss: 0.41779 | train_auc: 0.88579 | valid_auc: 0.89634 |  0:00:13s\n",
      "epoch 24 | loss: 0.41725 | train_auc: 0.88699 | valid_auc: 0.89565 |  0:00:13s\n",
      "epoch 25 | loss: 0.4137  | train_auc: 0.88633 | valid_auc: 0.89477 |  0:00:14s\n",
      "epoch 26 | loss: 0.41001 | train_auc: 0.88714 | valid_auc: 0.89633 |  0:00:14s\n",
      "epoch 27 | loss: 0.42601 | train_auc: 0.88628 | valid_auc: 0.89605 |  0:00:15s\n",
      "epoch 28 | loss: 0.42082 | train_auc: 0.88682 | valid_auc: 0.89586 |  0:00:15s\n",
      "epoch 29 | loss: 0.41589 | train_auc: 0.88691 | valid_auc: 0.89556 |  0:00:16s\n",
      "epoch 30 | loss: 0.41897 | train_auc: 0.88675 | valid_auc: 0.89547 |  0:00:16s\n",
      "epoch 31 | loss: 0.42285 | train_auc: 0.88699 | valid_auc: 0.89414 |  0:00:17s\n",
      "epoch 32 | loss: 0.41867 | train_auc: 0.88718 | valid_auc: 0.89589 |  0:00:18s\n",
      "epoch 33 | loss: 0.42391 | train_auc: 0.88731 | valid_auc: 0.89552 |  0:00:18s\n",
      "epoch 34 | loss: 0.42784 | train_auc: 0.8877  | valid_auc: 0.89604 |  0:00:19s\n",
      "epoch 35 | loss: 0.42363 | train_auc: 0.88716 | valid_auc: 0.89538 |  0:00:19s\n",
      "epoch 36 | loss: 0.41798 | train_auc: 0.88725 | valid_auc: 0.89634 |  0:00:20s\n",
      "epoch 37 | loss: 0.42098 | train_auc: 0.88758 | valid_auc: 0.89643 |  0:00:20s\n",
      "epoch 38 | loss: 0.41256 | train_auc: 0.88718 | valid_auc: 0.89541 |  0:00:21s\n",
      "epoch 39 | loss: 0.41504 | train_auc: 0.88755 | valid_auc: 0.89564 |  0:00:21s\n",
      "epoch 40 | loss: 0.42143 | train_auc: 0.88763 | valid_auc: 0.89588 |  0:00:22s\n",
      "epoch 41 | loss: 0.41771 | train_auc: 0.88714 | valid_auc: 0.89619 |  0:00:23s\n",
      "epoch 42 | loss: 0.41738 | train_auc: 0.8872  | valid_auc: 0.8957  |  0:00:23s\n",
      "epoch 43 | loss: 0.41948 | train_auc: 0.88781 | valid_auc: 0.89575 |  0:00:24s\n",
      "epoch 44 | loss: 0.42719 | train_auc: 0.88732 | valid_auc: 0.89692 |  0:00:24s\n",
      "epoch 45 | loss: 0.4213  | train_auc: 0.88742 | valid_auc: 0.89611 |  0:00:25s\n",
      "epoch 46 | loss: 0.41855 | train_auc: 0.88679 | valid_auc: 0.89678 |  0:00:25s\n",
      "epoch 47 | loss: 0.41423 | train_auc: 0.88775 | valid_auc: 0.89624 |  0:00:26s\n",
      "epoch 48 | loss: 0.41557 | train_auc: 0.88773 | valid_auc: 0.89522 |  0:00:26s\n",
      "epoch 49 | loss: 0.40636 | train_auc: 0.88788 | valid_auc: 0.8964  |  0:00:27s\n",
      "epoch 50 | loss: 0.42363 | train_auc: 0.88714 | valid_auc: 0.89561 |  0:00:27s\n",
      "epoch 51 | loss: 0.41331 | train_auc: 0.88717 | valid_auc: 0.89611 |  0:00:28s\n",
      "epoch 52 | loss: 0.41133 | train_auc: 0.88699 | valid_auc: 0.89652 |  0:00:28s\n",
      "epoch 53 | loss: 0.4218  | train_auc: 0.88694 | valid_auc: 0.8961  |  0:00:29s\n",
      "epoch 54 | loss: 0.42291 | train_auc: 0.88646 | valid_auc: 0.89636 |  0:00:30s\n",
      "epoch 55 | loss: 0.42279 | train_auc: 0.88693 | valid_auc: 0.89636 |  0:00:30s\n",
      "epoch 56 | loss: 0.42262 | train_auc: 0.88716 | valid_auc: 0.89469 |  0:00:31s\n",
      "epoch 57 | loss: 0.418   | train_auc: 0.88706 | valid_auc: 0.89544 |  0:00:31s\n",
      "epoch 58 | loss: 0.4277  | train_auc: 0.88634 | valid_auc: 0.89319 |  0:00:32s\n",
      "epoch 59 | loss: 0.41948 | train_auc: 0.88727 | valid_auc: 0.89577 |  0:00:32s\n",
      "epoch 60 | loss: 0.41667 | train_auc: 0.88672 | valid_auc: 0.89428 |  0:00:33s\n",
      "epoch 61 | loss: 0.42055 | train_auc: 0.88503 | valid_auc: 0.89355 |  0:00:33s\n",
      "epoch 62 | loss: 0.42887 | train_auc: 0.88571 | valid_auc: 0.89506 |  0:00:34s\n",
      "epoch 63 | loss: 0.42845 | train_auc: 0.88517 | valid_auc: 0.89428 |  0:00:35s\n",
      "epoch 64 | loss: 0.41884 | train_auc: 0.88638 | valid_auc: 0.89593 |  0:00:35s\n",
      "epoch 65 | loss: 0.42068 | train_auc: 0.88605 | valid_auc: 0.89497 |  0:00:36s\n",
      "epoch 66 | loss: 0.42644 | train_auc: 0.88473 | valid_auc: 0.89223 |  0:00:36s\n",
      "epoch 67 | loss: 0.42007 | train_auc: 0.88665 | valid_auc: 0.89549 |  0:00:37s\n",
      "epoch 68 | loss: 0.42185 | train_auc: 0.88602 | valid_auc: 0.8955  |  0:00:37s\n",
      "epoch 69 | loss: 0.42106 | train_auc: 0.88448 | valid_auc: 0.89349 |  0:00:38s\n",
      "epoch 70 | loss: 0.41843 | train_auc: 0.8838  | valid_auc: 0.89472 |  0:00:38s\n",
      "epoch 71 | loss: 0.42768 | train_auc: 0.8854  | valid_auc: 0.89574 |  0:00:39s\n",
      "epoch 72 | loss: 0.41897 | train_auc: 0.88569 | valid_auc: 0.89457 |  0:00:40s\n",
      "epoch 73 | loss: 0.41892 | train_auc: 0.88592 | valid_auc: 0.89483 |  0:00:40s\n",
      "epoch 74 | loss: 0.41797 | train_auc: 0.88658 | valid_auc: 0.89677 |  0:00:41s\n",
      "epoch 75 | loss: 0.41798 | train_auc: 0.88634 | valid_auc: 0.89563 |  0:00:41s\n",
      "epoch 76 | loss: 0.42518 | train_auc: 0.88598 | valid_auc: 0.89461 |  0:00:42s\n",
      "epoch 77 | loss: 0.42518 | train_auc: 0.8865  | valid_auc: 0.89567 |  0:00:42s\n",
      "epoch 78 | loss: 0.42326 | train_auc: 0.8864  | valid_auc: 0.89546 |  0:00:43s\n",
      "epoch 79 | loss: 0.4166  | train_auc: 0.8869  | valid_auc: 0.89687 |  0:00:43s\n",
      "epoch 80 | loss: 0.41844 | train_auc: 0.8867  | valid_auc: 0.89678 |  0:00:44s\n",
      "epoch 81 | loss: 0.41744 | train_auc: 0.88592 | valid_auc: 0.89662 |  0:00:44s\n",
      "epoch 82 | loss: 0.41981 | train_auc: 0.88648 | valid_auc: 0.89617 |  0:00:45s\n",
      "epoch 83 | loss: 0.41979 | train_auc: 0.88717 | valid_auc: 0.89626 |  0:00:45s\n",
      "epoch 84 | loss: 0.40972 | train_auc: 0.88635 | valid_auc: 0.89624 |  0:00:46s\n",
      "epoch 85 | loss: 0.42619 | train_auc: 0.88607 | valid_auc: 0.89618 |  0:00:46s\n",
      "epoch 86 | loss: 0.417   | train_auc: 0.88678 | valid_auc: 0.8964  |  0:00:47s\n",
      "epoch 87 | loss: 0.41546 | train_auc: 0.88692 | valid_auc: 0.89621 |  0:00:47s\n",
      "epoch 88 | loss: 0.41481 | train_auc: 0.88537 | valid_auc: 0.89346 |  0:00:48s\n",
      "epoch 89 | loss: 0.41811 | train_auc: 0.88692 | valid_auc: 0.89591 |  0:00:49s\n",
      "epoch 90 | loss: 0.42541 | train_auc: 0.88681 | valid_auc: 0.89553 |  0:00:49s\n",
      "epoch 91 | loss: 0.41855 | train_auc: 0.88639 | valid_auc: 0.89501 |  0:00:50s\n",
      "epoch 92 | loss: 0.42465 | train_auc: 0.88701 | valid_auc: 0.89698 |  0:00:50s\n",
      "epoch 93 | loss: 0.41593 | train_auc: 0.88663 | valid_auc: 0.89681 |  0:00:51s\n",
      "epoch 94 | loss: 0.42245 | train_auc: 0.88706 | valid_auc: 0.89724 |  0:00:51s\n",
      "epoch 95 | loss: 0.41312 | train_auc: 0.88716 | valid_auc: 0.89655 |  0:00:52s\n",
      "epoch 96 | loss: 0.42377 | train_auc: 0.88751 | valid_auc: 0.89743 |  0:00:52s\n",
      "epoch 97 | loss: 0.42757 | train_auc: 0.88606 | valid_auc: 0.89442 |  0:00:53s\n",
      "epoch 98 | loss: 0.42055 | train_auc: 0.88656 | valid_auc: 0.89484 |  0:00:53s\n",
      "epoch 99 | loss: 0.42104 | train_auc: 0.88667 | valid_auc: 0.89456 |  0:00:54s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 96 and best_valid_auc = 0.89743\n",
      "sampling:  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.70509 | train_auc: 0.6087  | valid_auc: 0.62351 |  0:00:00s\n",
      "epoch 1  | loss: 0.4933  | train_auc: 0.70469 | valid_auc: 0.71153 |  0:00:01s\n",
      "epoch 2  | loss: 0.45458 | train_auc: 0.79012 | valid_auc: 0.80207 |  0:00:01s\n",
      "epoch 3  | loss: 0.43705 | train_auc: 0.84009 | valid_auc: 0.8516  |  0:00:02s\n",
      "epoch 4  | loss: 0.43193 | train_auc: 0.84345 | valid_auc: 0.85797 |  0:00:02s\n",
      "epoch 5  | loss: 0.42999 | train_auc: 0.85477 | valid_auc: 0.868   |  0:00:03s\n",
      "epoch 6  | loss: 0.42996 | train_auc: 0.8634  | valid_auc: 0.87744 |  0:00:03s\n",
      "epoch 7  | loss: 0.42589 | train_auc: 0.86932 | valid_auc: 0.88318 |  0:00:04s\n",
      "epoch 8  | loss: 0.42368 | train_auc: 0.87271 | valid_auc: 0.88616 |  0:00:05s\n",
      "epoch 9  | loss: 0.42657 | train_auc: 0.87721 | valid_auc: 0.88944 |  0:00:05s\n",
      "epoch 10 | loss: 0.43361 | train_auc: 0.87948 | valid_auc: 0.89214 |  0:00:06s\n",
      "epoch 11 | loss: 0.42995 | train_auc: 0.88229 | valid_auc: 0.89438 |  0:00:06s\n",
      "epoch 12 | loss: 0.42574 | train_auc: 0.88328 | valid_auc: 0.89567 |  0:00:07s\n",
      "epoch 13 | loss: 0.42666 | train_auc: 0.88112 | valid_auc: 0.89273 |  0:00:07s\n",
      "epoch 14 | loss: 0.42791 | train_auc: 0.88213 | valid_auc: 0.89468 |  0:00:08s\n",
      "epoch 15 | loss: 0.42278 | train_auc: 0.88442 | valid_auc: 0.89752 |  0:00:08s\n",
      "epoch 16 | loss: 0.42954 | train_auc: 0.88469 | valid_auc: 0.89627 |  0:00:09s\n",
      "epoch 17 | loss: 0.43041 | train_auc: 0.88373 | valid_auc: 0.89633 |  0:00:09s\n",
      "epoch 18 | loss: 0.42093 | train_auc: 0.88421 | valid_auc: 0.89622 |  0:00:10s\n",
      "epoch 19 | loss: 0.41829 | train_auc: 0.88401 | valid_auc: 0.89433 |  0:00:11s\n",
      "epoch 20 | loss: 0.43121 | train_auc: 0.88473 | valid_auc: 0.89605 |  0:00:11s\n",
      "epoch 21 | loss: 0.42103 | train_auc: 0.88399 | valid_auc: 0.89742 |  0:00:12s\n",
      "epoch 22 | loss: 0.4215  | train_auc: 0.88503 | valid_auc: 0.89606 |  0:00:12s\n",
      "epoch 23 | loss: 0.43352 | train_auc: 0.88301 | valid_auc: 0.89327 |  0:00:13s\n",
      "epoch 24 | loss: 0.4232  | train_auc: 0.88472 | valid_auc: 0.89543 |  0:00:13s\n",
      "epoch 25 | loss: 0.41897 | train_auc: 0.88535 | valid_auc: 0.89817 |  0:00:14s\n",
      "epoch 26 | loss: 0.42048 | train_auc: 0.88483 | valid_auc: 0.89773 |  0:00:14s\n",
      "epoch 27 | loss: 0.42314 | train_auc: 0.88432 | valid_auc: 0.89514 |  0:00:15s\n",
      "epoch 28 | loss: 0.43356 | train_auc: 0.88541 | valid_auc: 0.89586 |  0:00:15s\n",
      "epoch 29 | loss: 0.42841 | train_auc: 0.88497 | valid_auc: 0.89707 |  0:00:16s\n",
      "epoch 30 | loss: 0.43362 | train_auc: 0.88559 | valid_auc: 0.89687 |  0:00:17s\n",
      "epoch 31 | loss: 0.4241  | train_auc: 0.88537 | valid_auc: 0.89621 |  0:00:17s\n",
      "epoch 32 | loss: 0.42525 | train_auc: 0.88547 | valid_auc: 0.89826 |  0:00:18s\n",
      "epoch 33 | loss: 0.42522 | train_auc: 0.88531 | valid_auc: 0.89664 |  0:00:18s\n",
      "epoch 34 | loss: 0.42852 | train_auc: 0.88556 | valid_auc: 0.89742 |  0:00:19s\n",
      "epoch 35 | loss: 0.43176 | train_auc: 0.88569 | valid_auc: 0.89562 |  0:00:19s\n",
      "epoch 36 | loss: 0.42622 | train_auc: 0.88533 | valid_auc: 0.89697 |  0:00:20s\n",
      "epoch 37 | loss: 0.41838 | train_auc: 0.88515 | valid_auc: 0.89775 |  0:00:20s\n",
      "epoch 38 | loss: 0.41956 | train_auc: 0.88509 | valid_auc: 0.89627 |  0:00:21s\n",
      "epoch 39 | loss: 0.42114 | train_auc: 0.88521 | valid_auc: 0.8979  |  0:00:22s\n",
      "epoch 40 | loss: 0.42274 | train_auc: 0.8851  | valid_auc: 0.89564 |  0:00:22s\n",
      "epoch 41 | loss: 0.42711 | train_auc: 0.88531 | valid_auc: 0.89864 |  0:00:23s\n",
      "epoch 42 | loss: 0.42317 | train_auc: 0.88523 | valid_auc: 0.8985  |  0:00:23s\n",
      "epoch 43 | loss: 0.4195  | train_auc: 0.88601 | valid_auc: 0.8977  |  0:00:24s\n",
      "epoch 44 | loss: 0.42501 | train_auc: 0.88603 | valid_auc: 0.89765 |  0:00:24s\n",
      "epoch 45 | loss: 0.42316 | train_auc: 0.88586 | valid_auc: 0.89817 |  0:00:25s\n",
      "epoch 46 | loss: 0.41894 | train_auc: 0.88573 | valid_auc: 0.89621 |  0:00:25s\n",
      "epoch 47 | loss: 0.42742 | train_auc: 0.8858  | valid_auc: 0.89546 |  0:00:26s\n",
      "epoch 48 | loss: 0.42855 | train_auc: 0.88467 | valid_auc: 0.89775 |  0:00:27s\n",
      "epoch 49 | loss: 0.42285 | train_auc: 0.8852  | valid_auc: 0.89662 |  0:00:27s\n",
      "epoch 50 | loss: 0.42332 | train_auc: 0.88579 | valid_auc: 0.89783 |  0:00:28s\n",
      "epoch 51 | loss: 0.42074 | train_auc: 0.88587 | valid_auc: 0.89703 |  0:00:28s\n",
      "epoch 52 | loss: 0.42328 | train_auc: 0.88605 | valid_auc: 0.89714 |  0:00:29s\n",
      "epoch 53 | loss: 0.42299 | train_auc: 0.88519 | valid_auc: 0.89717 |  0:00:29s\n",
      "epoch 54 | loss: 0.41928 | train_auc: 0.88376 | valid_auc: 0.89669 |  0:00:30s\n",
      "epoch 55 | loss: 0.42561 | train_auc: 0.88554 | valid_auc: 0.89771 |  0:00:30s\n",
      "epoch 56 | loss: 0.42689 | train_auc: 0.88518 | valid_auc: 0.8972  |  0:00:31s\n",
      "epoch 57 | loss: 0.41638 | train_auc: 0.88528 | valid_auc: 0.89601 |  0:00:31s\n",
      "epoch 58 | loss: 0.41483 | train_auc: 0.88539 | valid_auc: 0.89804 |  0:00:32s\n",
      "epoch 59 | loss: 0.42594 | train_auc: 0.8851  | valid_auc: 0.89662 |  0:00:33s\n",
      "epoch 60 | loss: 0.42764 | train_auc: 0.88545 | valid_auc: 0.89448 |  0:00:33s\n",
      "epoch 61 | loss: 0.42462 | train_auc: 0.8856  | valid_auc: 0.89769 |  0:00:34s\n",
      "epoch 62 | loss: 0.42285 | train_auc: 0.88513 | valid_auc: 0.89533 |  0:00:34s\n",
      "epoch 63 | loss: 0.42504 | train_auc: 0.88515 | valid_auc: 0.89825 |  0:00:35s\n",
      "epoch 64 | loss: 0.42221 | train_auc: 0.8853  | valid_auc: 0.89508 |  0:00:35s\n",
      "epoch 65 | loss: 0.41796 | train_auc: 0.88523 | valid_auc: 0.89561 |  0:00:36s\n",
      "epoch 66 | loss: 0.41917 | train_auc: 0.88527 | valid_auc: 0.89737 |  0:00:36s\n",
      "epoch 67 | loss: 0.42556 | train_auc: 0.88528 | valid_auc: 0.89675 |  0:00:37s\n",
      "epoch 68 | loss: 0.42524 | train_auc: 0.88577 | valid_auc: 0.8963  |  0:00:37s\n",
      "epoch 69 | loss: 0.41623 | train_auc: 0.88516 | valid_auc: 0.8962  |  0:00:38s\n",
      "epoch 70 | loss: 0.42213 | train_auc: 0.88558 | valid_auc: 0.89813 |  0:00:39s\n",
      "epoch 71 | loss: 0.41727 | train_auc: 0.88529 | valid_auc: 0.89781 |  0:00:39s\n",
      "epoch 72 | loss: 0.42225 | train_auc: 0.88535 | valid_auc: 0.89705 |  0:00:40s\n",
      "epoch 73 | loss: 0.42021 | train_auc: 0.88571 | valid_auc: 0.8975  |  0:00:40s\n",
      "epoch 74 | loss: 0.42977 | train_auc: 0.88558 | valid_auc: 0.89818 |  0:00:41s\n",
      "epoch 75 | loss: 0.42493 | train_auc: 0.88573 | valid_auc: 0.89692 |  0:00:41s\n",
      "epoch 76 | loss: 0.41352 | train_auc: 0.88574 | valid_auc: 0.89767 |  0:00:42s\n",
      "epoch 77 | loss: 0.42281 | train_auc: 0.88577 | valid_auc: 0.89804 |  0:00:42s\n",
      "epoch 78 | loss: 0.41667 | train_auc: 0.88553 | valid_auc: 0.89561 |  0:00:43s\n",
      "epoch 79 | loss: 0.42183 | train_auc: 0.88336 | valid_auc: 0.89608 |  0:00:44s\n",
      "epoch 80 | loss: 0.4237  | train_auc: 0.88527 | valid_auc: 0.89587 |  0:00:44s\n",
      "epoch 81 | loss: 0.42488 | train_auc: 0.88546 | valid_auc: 0.89694 |  0:00:45s\n",
      "epoch 82 | loss: 0.42896 | train_auc: 0.88592 | valid_auc: 0.89607 |  0:00:45s\n",
      "epoch 83 | loss: 0.42367 | train_auc: 0.88557 | valid_auc: 0.89679 |  0:00:46s\n",
      "epoch 84 | loss: 0.42213 | train_auc: 0.88557 | valid_auc: 0.89663 |  0:00:46s\n",
      "epoch 85 | loss: 0.41976 | train_auc: 0.88595 | valid_auc: 0.89661 |  0:00:47s\n",
      "epoch 86 | loss: 0.41844 | train_auc: 0.88413 | valid_auc: 0.89728 |  0:00:47s\n",
      "epoch 87 | loss: 0.42063 | train_auc: 0.88599 | valid_auc: 0.89733 |  0:00:48s\n",
      "epoch 88 | loss: 0.42477 | train_auc: 0.8864  | valid_auc: 0.89698 |  0:00:48s\n",
      "epoch 89 | loss: 0.42111 | train_auc: 0.88581 | valid_auc: 0.89662 |  0:00:49s\n",
      "epoch 90 | loss: 0.41203 | train_auc: 0.88557 | valid_auc: 0.89871 |  0:00:50s\n",
      "epoch 91 | loss: 0.41594 | train_auc: 0.88508 | valid_auc: 0.89799 |  0:00:50s\n",
      "epoch 92 | loss: 0.41399 | train_auc: 0.88566 | valid_auc: 0.8993  |  0:00:51s\n",
      "epoch 93 | loss: 0.42206 | train_auc: 0.88594 | valid_auc: 0.89711 |  0:00:51s\n",
      "epoch 94 | loss: 0.41594 | train_auc: 0.88578 | valid_auc: 0.89637 |  0:00:52s\n",
      "epoch 95 | loss: 0.41801 | train_auc: 0.88494 | valid_auc: 0.89692 |  0:00:52s\n",
      "epoch 96 | loss: 0.42186 | train_auc: 0.88573 | valid_auc: 0.89812 |  0:00:53s\n",
      "epoch 97 | loss: 0.41786 | train_auc: 0.88547 | valid_auc: 0.89746 |  0:00:53s\n",
      "epoch 98 | loss: 0.41865 | train_auc: 0.88621 | valid_auc: 0.89752 |  0:00:54s\n",
      "epoch 99 | loss: 0.41651 | train_auc: 0.88589 | valid_auc: 0.89737 |  0:00:54s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 92 and best_valid_auc = 0.8993\n",
      "sampling:  23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.7056  | train_auc: 0.56987 | valid_auc: 0.55601 |  0:00:00s\n",
      "epoch 1  | loss: 0.47256 | train_auc: 0.75682 | valid_auc: 0.75318 |  0:00:01s\n",
      "epoch 2  | loss: 0.44069 | train_auc: 0.77771 | valid_auc: 0.76996 |  0:00:01s\n",
      "epoch 3  | loss: 0.42963 | train_auc: 0.8379  | valid_auc: 0.83975 |  0:00:02s\n",
      "epoch 4  | loss: 0.42151 | train_auc: 0.87139 | valid_auc: 0.87064 |  0:00:02s\n",
      "epoch 5  | loss: 0.42397 | train_auc: 0.8672  | valid_auc: 0.86543 |  0:00:03s\n",
      "epoch 6  | loss: 0.41859 | train_auc: 0.87843 | valid_auc: 0.87776 |  0:00:03s\n",
      "epoch 7  | loss: 0.42508 | train_auc: 0.87997 | valid_auc: 0.88021 |  0:00:04s\n",
      "epoch 8  | loss: 0.41231 | train_auc: 0.88318 | valid_auc: 0.88243 |  0:00:04s\n",
      "epoch 9  | loss: 0.41639 | train_auc: 0.88449 | valid_auc: 0.88444 |  0:00:05s\n",
      "epoch 10 | loss: 0.41853 | train_auc: 0.88665 | valid_auc: 0.88635 |  0:00:06s\n",
      "epoch 11 | loss: 0.41354 | train_auc: 0.88748 | valid_auc: 0.88652 |  0:00:06s\n",
      "epoch 12 | loss: 0.41704 | train_auc: 0.88782 | valid_auc: 0.88712 |  0:00:07s\n",
      "epoch 13 | loss: 0.41487 | train_auc: 0.8885  | valid_auc: 0.88791 |  0:00:07s\n",
      "epoch 14 | loss: 0.41558 | train_auc: 0.8898  | valid_auc: 0.88888 |  0:00:08s\n",
      "epoch 15 | loss: 0.41577 | train_auc: 0.89092 | valid_auc: 0.88994 |  0:00:08s\n",
      "epoch 16 | loss: 0.41777 | train_auc: 0.89109 | valid_auc: 0.88963 |  0:00:09s\n",
      "epoch 17 | loss: 0.41597 | train_auc: 0.89182 | valid_auc: 0.89078 |  0:00:09s\n",
      "epoch 18 | loss: 0.42445 | train_auc: 0.89131 | valid_auc: 0.8901  |  0:00:10s\n",
      "epoch 19 | loss: 0.41506 | train_auc: 0.89068 | valid_auc: 0.88987 |  0:00:11s\n",
      "epoch 20 | loss: 0.40704 | train_auc: 0.8914  | valid_auc: 0.89105 |  0:00:11s\n",
      "epoch 21 | loss: 0.41204 | train_auc: 0.89223 | valid_auc: 0.89106 |  0:00:12s\n",
      "epoch 22 | loss: 0.41818 | train_auc: 0.89179 | valid_auc: 0.89057 |  0:00:12s\n",
      "epoch 23 | loss: 0.42062 | train_auc: 0.8922  | valid_auc: 0.89145 |  0:00:13s\n",
      "epoch 24 | loss: 0.41159 | train_auc: 0.89288 | valid_auc: 0.89148 |  0:00:13s\n",
      "epoch 25 | loss: 0.41595 | train_auc: 0.8927  | valid_auc: 0.89076 |  0:00:14s\n",
      "epoch 26 | loss: 0.4096  | train_auc: 0.89249 | valid_auc: 0.89114 |  0:00:14s\n",
      "epoch 27 | loss: 0.4189  | train_auc: 0.892   | valid_auc: 0.89103 |  0:00:15s\n",
      "epoch 28 | loss: 0.41396 | train_auc: 0.89263 | valid_auc: 0.89179 |  0:00:15s\n",
      "epoch 29 | loss: 0.41247 | train_auc: 0.89291 | valid_auc: 0.89139 |  0:00:16s\n",
      "epoch 30 | loss: 0.41077 | train_auc: 0.89238 | valid_auc: 0.8918  |  0:00:17s\n",
      "epoch 31 | loss: 0.41117 | train_auc: 0.89254 | valid_auc: 0.89155 |  0:00:17s\n",
      "epoch 32 | loss: 0.40983 | train_auc: 0.89255 | valid_auc: 0.89214 |  0:00:18s\n",
      "epoch 33 | loss: 0.41114 | train_auc: 0.89188 | valid_auc: 0.89113 |  0:00:18s\n",
      "epoch 34 | loss: 0.41196 | train_auc: 0.89296 | valid_auc: 0.89165 |  0:00:19s\n",
      "epoch 35 | loss: 0.4203  | train_auc: 0.89279 | valid_auc: 0.89189 |  0:00:19s\n",
      "epoch 36 | loss: 0.40681 | train_auc: 0.89235 | valid_auc: 0.8907  |  0:00:20s\n",
      "epoch 37 | loss: 0.42275 | train_auc: 0.89262 | valid_auc: 0.89148 |  0:00:21s\n",
      "epoch 38 | loss: 0.40952 | train_auc: 0.89275 | valid_auc: 0.89176 |  0:00:21s\n",
      "epoch 39 | loss: 0.40772 | train_auc: 0.89281 | valid_auc: 0.89068 |  0:00:22s\n",
      "epoch 40 | loss: 0.41066 | train_auc: 0.89262 | valid_auc: 0.89096 |  0:00:22s\n",
      "epoch 41 | loss: 0.40939 | train_auc: 0.89295 | valid_auc: 0.8912  |  0:00:23s\n",
      "epoch 42 | loss: 0.41152 | train_auc: 0.8926  | valid_auc: 0.89183 |  0:00:23s\n",
      "epoch 43 | loss: 0.41926 | train_auc: 0.89234 | valid_auc: 0.8914  |  0:00:24s\n",
      "epoch 44 | loss: 0.41777 | train_auc: 0.89256 | valid_auc: 0.89238 |  0:00:24s\n",
      "epoch 45 | loss: 0.41902 | train_auc: 0.89304 | valid_auc: 0.89114 |  0:00:25s\n",
      "epoch 46 | loss: 0.4098  | train_auc: 0.89273 | valid_auc: 0.89205 |  0:00:25s\n",
      "epoch 47 | loss: 0.40781 | train_auc: 0.89262 | valid_auc: 0.8923  |  0:00:26s\n",
      "epoch 48 | loss: 0.41606 | train_auc: 0.89316 | valid_auc: 0.89135 |  0:00:27s\n",
      "epoch 49 | loss: 0.41009 | train_auc: 0.89293 | valid_auc: 0.89225 |  0:00:27s\n",
      "epoch 50 | loss: 0.41698 | train_auc: 0.89243 | valid_auc: 0.89193 |  0:00:28s\n",
      "epoch 51 | loss: 0.40547 | train_auc: 0.89289 | valid_auc: 0.89263 |  0:00:28s\n",
      "epoch 52 | loss: 0.41025 | train_auc: 0.893   | valid_auc: 0.89284 |  0:00:29s\n",
      "epoch 53 | loss: 0.40994 | train_auc: 0.89294 | valid_auc: 0.89198 |  0:00:29s\n",
      "epoch 54 | loss: 0.41756 | train_auc: 0.89258 | valid_auc: 0.89104 |  0:00:30s\n",
      "epoch 55 | loss: 0.4214  | train_auc: 0.89244 | valid_auc: 0.89215 |  0:00:30s\n",
      "epoch 56 | loss: 0.41407 | train_auc: 0.89284 | valid_auc: 0.8906  |  0:00:31s\n",
      "epoch 57 | loss: 0.41459 | train_auc: 0.89292 | valid_auc: 0.89212 |  0:00:32s\n",
      "epoch 58 | loss: 0.39808 | train_auc: 0.89287 | valid_auc: 0.89218 |  0:00:32s\n",
      "epoch 59 | loss: 0.40784 | train_auc: 0.89189 | valid_auc: 0.89172 |  0:00:33s\n",
      "epoch 60 | loss: 0.41913 | train_auc: 0.89201 | valid_auc: 0.89017 |  0:00:33s\n",
      "epoch 61 | loss: 0.41348 | train_auc: 0.8927  | valid_auc: 0.89217 |  0:00:34s\n",
      "epoch 62 | loss: 0.40513 | train_auc: 0.89285 | valid_auc: 0.89162 |  0:00:34s\n",
      "epoch 63 | loss: 0.40092 | train_auc: 0.89288 | valid_auc: 0.89186 |  0:00:35s\n",
      "epoch 64 | loss: 0.40717 | train_auc: 0.89331 | valid_auc: 0.89084 |  0:00:35s\n",
      "epoch 65 | loss: 0.41347 | train_auc: 0.89287 | valid_auc: 0.89108 |  0:00:36s\n",
      "epoch 66 | loss: 0.41472 | train_auc: 0.89271 | valid_auc: 0.89038 |  0:00:36s\n",
      "epoch 67 | loss: 0.4075  | train_auc: 0.89296 | valid_auc: 0.89106 |  0:00:37s\n",
      "epoch 68 | loss: 0.40408 | train_auc: 0.89309 | valid_auc: 0.89156 |  0:00:37s\n",
      "epoch 69 | loss: 0.40542 | train_auc: 0.89332 | valid_auc: 0.89147 |  0:00:38s\n",
      "epoch 70 | loss: 0.40937 | train_auc: 0.893   | valid_auc: 0.88995 |  0:00:39s\n",
      "epoch 71 | loss: 0.41288 | train_auc: 0.89232 | valid_auc: 0.89046 |  0:00:39s\n",
      "epoch 72 | loss: 0.41454 | train_auc: 0.89288 | valid_auc: 0.8907  |  0:00:40s\n",
      "epoch 73 | loss: 0.41113 | train_auc: 0.89236 | valid_auc: 0.89089 |  0:00:40s\n",
      "epoch 74 | loss: 0.41095 | train_auc: 0.89356 | valid_auc: 0.89118 |  0:00:41s\n",
      "epoch 75 | loss: 0.41352 | train_auc: 0.89345 | valid_auc: 0.89121 |  0:00:41s\n",
      "epoch 76 | loss: 0.41567 | train_auc: 0.89325 | valid_auc: 0.8915  |  0:00:42s\n",
      "epoch 77 | loss: 0.40808 | train_auc: 0.89364 | valid_auc: 0.89078 |  0:00:42s\n",
      "epoch 78 | loss: 0.40937 | train_auc: 0.89271 | valid_auc: 0.89173 |  0:00:43s\n",
      "epoch 79 | loss: 0.41102 | train_auc: 0.89359 | valid_auc: 0.89146 |  0:00:44s\n",
      "epoch 80 | loss: 0.40986 | train_auc: 0.89321 | valid_auc: 0.89147 |  0:00:44s\n",
      "epoch 81 | loss: 0.40581 | train_auc: 0.89377 | valid_auc: 0.89109 |  0:00:45s\n",
      "epoch 82 | loss: 0.40957 | train_auc: 0.89292 | valid_auc: 0.89021 |  0:00:45s\n",
      "epoch 83 | loss: 0.41443 | train_auc: 0.89374 | valid_auc: 0.89262 |  0:00:46s\n",
      "epoch 84 | loss: 0.41019 | train_auc: 0.89359 | valid_auc: 0.89185 |  0:00:46s\n",
      "epoch 85 | loss: 0.41875 | train_auc: 0.89329 | valid_auc: 0.89131 |  0:00:47s\n",
      "epoch 86 | loss: 0.40763 | train_auc: 0.89291 | valid_auc: 0.89147 |  0:00:47s\n",
      "epoch 87 | loss: 0.40881 | train_auc: 0.89353 | valid_auc: 0.89174 |  0:00:48s\n",
      "epoch 88 | loss: 0.41484 | train_auc: 0.89269 | valid_auc: 0.89128 |  0:00:49s\n",
      "epoch 89 | loss: 0.4161  | train_auc: 0.89266 | valid_auc: 0.89209 |  0:00:49s\n",
      "epoch 90 | loss: 0.41342 | train_auc: 0.89304 | valid_auc: 0.89032 |  0:00:50s\n",
      "epoch 91 | loss: 0.41625 | train_auc: 0.89334 | valid_auc: 0.89179 |  0:00:50s\n",
      "epoch 92 | loss: 0.41378 | train_auc: 0.89298 | valid_auc: 0.89217 |  0:00:51s\n",
      "epoch 93 | loss: 0.40761 | train_auc: 0.89355 | valid_auc: 0.89264 |  0:00:51s\n",
      "epoch 94 | loss: 0.40621 | train_auc: 0.89402 | valid_auc: 0.89105 |  0:00:52s\n",
      "epoch 95 | loss: 0.4065  | train_auc: 0.89442 | valid_auc: 0.89175 |  0:00:52s\n",
      "epoch 96 | loss: 0.40816 | train_auc: 0.89402 | valid_auc: 0.89081 |  0:00:53s\n",
      "epoch 97 | loss: 0.40952 | train_auc: 0.89271 | valid_auc: 0.88972 |  0:00:53s\n",
      "epoch 98 | loss: 0.41118 | train_auc: 0.89382 | valid_auc: 0.89157 |  0:00:54s\n",
      "epoch 99 | loss: 0.40538 | train_auc: 0.893   | valid_auc: 0.88974 |  0:00:54s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 52 and best_valid_auc = 0.89284\n",
      "sampling:  24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.67726 | train_auc: 0.76177 | valid_auc: 0.76273 |  0:00:00s\n",
      "epoch 1  | loss: 0.4597  | train_auc: 0.83363 | valid_auc: 0.82139 |  0:00:01s\n",
      "epoch 2  | loss: 0.42504 | train_auc: 0.86342 | valid_auc: 0.85102 |  0:00:01s\n",
      "epoch 3  | loss: 0.42235 | train_auc: 0.86503 | valid_auc: 0.85878 |  0:00:02s\n",
      "epoch 4  | loss: 0.42599 | train_auc: 0.86819 | valid_auc: 0.86076 |  0:00:02s\n",
      "epoch 5  | loss: 0.4225  | train_auc: 0.87213 | valid_auc: 0.86221 |  0:00:03s\n",
      "epoch 6  | loss: 0.41522 | train_auc: 0.8759  | valid_auc: 0.86469 |  0:00:03s\n",
      "epoch 7  | loss: 0.41547 | train_auc: 0.87974 | valid_auc: 0.8701  |  0:00:04s\n",
      "epoch 8  | loss: 0.42346 | train_auc: 0.87981 | valid_auc: 0.86999 |  0:00:05s\n",
      "epoch 9  | loss: 0.42017 | train_auc: 0.88311 | valid_auc: 0.87128 |  0:00:05s\n",
      "epoch 10 | loss: 0.4185  | train_auc: 0.88343 | valid_auc: 0.87244 |  0:00:06s\n",
      "epoch 11 | loss: 0.42214 | train_auc: 0.88536 | valid_auc: 0.87485 |  0:00:06s\n",
      "epoch 12 | loss: 0.4165  | train_auc: 0.8882  | valid_auc: 0.87709 |  0:00:07s\n",
      "epoch 13 | loss: 0.41396 | train_auc: 0.88998 | valid_auc: 0.87939 |  0:00:07s\n",
      "epoch 14 | loss: 0.41729 | train_auc: 0.89064 | valid_auc: 0.87924 |  0:00:08s\n",
      "epoch 15 | loss: 0.41666 | train_auc: 0.89044 | valid_auc: 0.87878 |  0:00:08s\n",
      "epoch 16 | loss: 0.41084 | train_auc: 0.89167 | valid_auc: 0.87886 |  0:00:09s\n",
      "epoch 17 | loss: 0.40822 | train_auc: 0.89144 | valid_auc: 0.87843 |  0:00:09s\n",
      "epoch 18 | loss: 0.41634 | train_auc: 0.89215 | valid_auc: 0.87853 |  0:00:10s\n",
      "epoch 19 | loss: 0.41674 | train_auc: 0.89146 | valid_auc: 0.87883 |  0:00:11s\n",
      "epoch 20 | loss: 0.4079  | train_auc: 0.89258 | valid_auc: 0.87906 |  0:00:11s\n",
      "epoch 21 | loss: 0.41408 | train_auc: 0.89156 | valid_auc: 0.8786  |  0:00:12s\n",
      "epoch 22 | loss: 0.41212 | train_auc: 0.89327 | valid_auc: 0.87937 |  0:00:12s\n",
      "epoch 23 | loss: 0.40326 | train_auc: 0.89318 | valid_auc: 0.87925 |  0:00:13s\n",
      "epoch 24 | loss: 0.40688 | train_auc: 0.89361 | valid_auc: 0.87977 |  0:00:13s\n",
      "epoch 25 | loss: 0.40663 | train_auc: 0.89302 | valid_auc: 0.87993 |  0:00:14s\n",
      "epoch 26 | loss: 0.41591 | train_auc: 0.8928  | valid_auc: 0.87878 |  0:00:14s\n",
      "epoch 27 | loss: 0.41535 | train_auc: 0.89261 | valid_auc: 0.87942 |  0:00:15s\n",
      "epoch 28 | loss: 0.41052 | train_auc: 0.89298 | valid_auc: 0.87915 |  0:00:15s\n",
      "epoch 29 | loss: 0.42076 | train_auc: 0.89318 | valid_auc: 0.87906 |  0:00:16s\n",
      "epoch 30 | loss: 0.41179 | train_auc: 0.8924  | valid_auc: 0.87975 |  0:00:17s\n",
      "epoch 31 | loss: 0.40724 | train_auc: 0.89292 | valid_auc: 0.87766 |  0:00:17s\n",
      "epoch 32 | loss: 0.40492 | train_auc: 0.89211 | valid_auc: 0.87929 |  0:00:18s\n",
      "epoch 33 | loss: 0.41472 | train_auc: 0.89345 | valid_auc: 0.87932 |  0:00:18s\n",
      "epoch 34 | loss: 0.41211 | train_auc: 0.89296 | valid_auc: 0.87876 |  0:00:19s\n",
      "epoch 35 | loss: 0.41701 | train_auc: 0.892   | valid_auc: 0.88007 |  0:00:19s\n",
      "epoch 36 | loss: 0.41103 | train_auc: 0.89208 | valid_auc: 0.87705 |  0:00:20s\n",
      "epoch 37 | loss: 0.40827 | train_auc: 0.89243 | valid_auc: 0.87995 |  0:00:20s\n",
      "epoch 38 | loss: 0.4174  | train_auc: 0.89282 | valid_auc: 0.87862 |  0:00:21s\n",
      "epoch 39 | loss: 0.41928 | train_auc: 0.89309 | valid_auc: 0.87942 |  0:00:22s\n",
      "epoch 40 | loss: 0.41125 | train_auc: 0.89286 | valid_auc: 0.8783  |  0:00:22s\n",
      "epoch 41 | loss: 0.40898 | train_auc: 0.89268 | valid_auc: 0.87885 |  0:00:23s\n",
      "epoch 42 | loss: 0.40511 | train_auc: 0.89342 | valid_auc: 0.87883 |  0:00:23s\n",
      "epoch 43 | loss: 0.41314 | train_auc: 0.89322 | valid_auc: 0.88006 |  0:00:24s\n",
      "epoch 44 | loss: 0.41071 | train_auc: 0.89341 | valid_auc: 0.87983 |  0:00:24s\n",
      "epoch 45 | loss: 0.4121  | train_auc: 0.8936  | valid_auc: 0.87973 |  0:00:25s\n",
      "epoch 46 | loss: 0.4223  | train_auc: 0.89357 | valid_auc: 0.87962 |  0:00:25s\n",
      "epoch 47 | loss: 0.41262 | train_auc: 0.89328 | valid_auc: 0.87811 |  0:00:26s\n",
      "epoch 48 | loss: 0.41795 | train_auc: 0.8931  | valid_auc: 0.87961 |  0:00:26s\n",
      "epoch 49 | loss: 0.4084  | train_auc: 0.89281 | valid_auc: 0.87802 |  0:00:27s\n",
      "epoch 50 | loss: 0.41176 | train_auc: 0.89256 | valid_auc: 0.88068 |  0:00:28s\n",
      "epoch 51 | loss: 0.41355 | train_auc: 0.89323 | valid_auc: 0.88107 |  0:00:28s\n",
      "epoch 52 | loss: 0.4126  | train_auc: 0.89403 | valid_auc: 0.8806  |  0:00:29s\n",
      "epoch 53 | loss: 0.41093 | train_auc: 0.89309 | valid_auc: 0.87802 |  0:00:29s\n",
      "epoch 54 | loss: 0.40093 | train_auc: 0.89379 | valid_auc: 0.88071 |  0:00:30s\n",
      "epoch 55 | loss: 0.39763 | train_auc: 0.89384 | valid_auc: 0.87952 |  0:00:30s\n",
      "epoch 56 | loss: 0.41899 | train_auc: 0.89351 | valid_auc: 0.8794  |  0:00:31s\n",
      "epoch 57 | loss: 0.41322 | train_auc: 0.8937  | valid_auc: 0.88054 |  0:00:31s\n",
      "epoch 58 | loss: 0.41079 | train_auc: 0.8941  | valid_auc: 0.87972 |  0:00:32s\n",
      "epoch 59 | loss: 0.41827 | train_auc: 0.89382 | valid_auc: 0.8802  |  0:00:33s\n",
      "epoch 60 | loss: 0.41546 | train_auc: 0.89339 | valid_auc: 0.8793  |  0:00:33s\n",
      "epoch 61 | loss: 0.40764 | train_auc: 0.8942  | valid_auc: 0.87839 |  0:00:34s\n",
      "epoch 62 | loss: 0.39889 | train_auc: 0.89414 | valid_auc: 0.87892 |  0:00:34s\n",
      "epoch 63 | loss: 0.41275 | train_auc: 0.89361 | valid_auc: 0.87982 |  0:00:35s\n",
      "epoch 64 | loss: 0.40687 | train_auc: 0.89379 | valid_auc: 0.88009 |  0:00:35s\n",
      "epoch 65 | loss: 0.41071 | train_auc: 0.89389 | valid_auc: 0.88005 |  0:00:36s\n",
      "epoch 66 | loss: 0.39908 | train_auc: 0.89349 | valid_auc: 0.87809 |  0:00:36s\n",
      "epoch 67 | loss: 0.41366 | train_auc: 0.89301 | valid_auc: 0.87849 |  0:00:37s\n",
      "epoch 68 | loss: 0.41222 | train_auc: 0.89411 | valid_auc: 0.88084 |  0:00:37s\n",
      "epoch 69 | loss: 0.41395 | train_auc: 0.89413 | valid_auc: 0.87926 |  0:00:38s\n",
      "epoch 70 | loss: 0.3985  | train_auc: 0.89369 | valid_auc: 0.87969 |  0:00:39s\n",
      "epoch 71 | loss: 0.41671 | train_auc: 0.89363 | valid_auc: 0.87748 |  0:00:39s\n",
      "epoch 72 | loss: 0.4042  | train_auc: 0.8946  | valid_auc: 0.88062 |  0:00:40s\n",
      "epoch 73 | loss: 0.40846 | train_auc: 0.89437 | valid_auc: 0.88018 |  0:00:40s\n",
      "epoch 74 | loss: 0.40704 | train_auc: 0.89356 | valid_auc: 0.87914 |  0:00:41s\n",
      "epoch 75 | loss: 0.40732 | train_auc: 0.89402 | valid_auc: 0.87979 |  0:00:41s\n",
      "epoch 76 | loss: 0.40444 | train_auc: 0.89413 | valid_auc: 0.87969 |  0:00:42s\n",
      "epoch 77 | loss: 0.40525 | train_auc: 0.89391 | valid_auc: 0.87971 |  0:00:42s\n",
      "epoch 78 | loss: 0.41573 | train_auc: 0.894   | valid_auc: 0.87862 |  0:00:43s\n",
      "epoch 79 | loss: 0.40936 | train_auc: 0.89296 | valid_auc: 0.88018 |  0:00:43s\n",
      "epoch 80 | loss: 0.40822 | train_auc: 0.89355 | valid_auc: 0.88108 |  0:00:44s\n",
      "epoch 81 | loss: 0.41111 | train_auc: 0.89324 | valid_auc: 0.88089 |  0:00:45s\n",
      "epoch 82 | loss: 0.41608 | train_auc: 0.8934  | valid_auc: 0.88132 |  0:00:45s\n",
      "epoch 83 | loss: 0.41479 | train_auc: 0.89361 | valid_auc: 0.88075 |  0:00:46s\n",
      "epoch 84 | loss: 0.41473 | train_auc: 0.89356 | valid_auc: 0.87954 |  0:00:46s\n",
      "epoch 85 | loss: 0.40912 | train_auc: 0.89376 | valid_auc: 0.88092 |  0:00:47s\n",
      "epoch 86 | loss: 0.41366 | train_auc: 0.8938  | valid_auc: 0.87872 |  0:00:47s\n",
      "epoch 87 | loss: 0.4073  | train_auc: 0.89403 | valid_auc: 0.8807  |  0:00:48s\n",
      "epoch 88 | loss: 0.40532 | train_auc: 0.89385 | valid_auc: 0.88058 |  0:00:48s\n",
      "epoch 89 | loss: 0.4087  | train_auc: 0.89405 | valid_auc: 0.88077 |  0:00:49s\n",
      "epoch 90 | loss: 0.40636 | train_auc: 0.89363 | valid_auc: 0.88041 |  0:00:49s\n",
      "epoch 91 | loss: 0.4033  | train_auc: 0.89415 | valid_auc: 0.88059 |  0:00:50s\n",
      "epoch 92 | loss: 0.4113  | train_auc: 0.89402 | valid_auc: 0.88076 |  0:00:51s\n",
      "epoch 93 | loss: 0.40063 | train_auc: 0.89411 | valid_auc: 0.88031 |  0:00:51s\n",
      "epoch 94 | loss: 0.40908 | train_auc: 0.89408 | valid_auc: 0.88024 |  0:00:52s\n",
      "epoch 95 | loss: 0.40731 | train_auc: 0.89419 | valid_auc: 0.88103 |  0:00:52s\n",
      "epoch 96 | loss: 0.40834 | train_auc: 0.89417 | valid_auc: 0.87992 |  0:00:53s\n",
      "epoch 97 | loss: 0.40858 | train_auc: 0.89461 | valid_auc: 0.87921 |  0:00:53s\n",
      "epoch 98 | loss: 0.41366 | train_auc: 0.89429 | valid_auc: 0.88019 |  0:00:54s\n",
      "epoch 99 | loss: 0.39781 | train_auc: 0.8941  | valid_auc: 0.88046 |  0:00:54s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 82 and best_valid_auc = 0.88132\n",
      "sampling:  25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.68823 | train_auc: 0.62631 | valid_auc: 0.63519 |  0:00:00s\n",
      "epoch 1  | loss: 0.48896 | train_auc: 0.71402 | valid_auc: 0.70516 |  0:00:01s\n",
      "epoch 2  | loss: 0.44925 | train_auc: 0.79606 | valid_auc: 0.79058 |  0:00:01s\n",
      "epoch 3  | loss: 0.43226 | train_auc: 0.82719 | valid_auc: 0.8242  |  0:00:02s\n",
      "epoch 4  | loss: 0.41955 | train_auc: 0.82431 | valid_auc: 0.82739 |  0:00:02s\n",
      "epoch 5  | loss: 0.42893 | train_auc: 0.8689  | valid_auc: 0.86532 |  0:00:03s\n",
      "epoch 6  | loss: 0.42609 | train_auc: 0.87273 | valid_auc: 0.87045 |  0:00:03s\n",
      "epoch 7  | loss: 0.41921 | train_auc: 0.87823 | valid_auc: 0.8753  |  0:00:04s\n",
      "epoch 8  | loss: 0.41122 | train_auc: 0.87825 | valid_auc: 0.87492 |  0:00:04s\n",
      "epoch 9  | loss: 0.41892 | train_auc: 0.88269 | valid_auc: 0.87698 |  0:00:05s\n",
      "epoch 10 | loss: 0.42255 | train_auc: 0.88578 | valid_auc: 0.88024 |  0:00:06s\n",
      "epoch 11 | loss: 0.41611 | train_auc: 0.88751 | valid_auc: 0.88391 |  0:00:06s\n",
      "epoch 12 | loss: 0.41484 | train_auc: 0.8885  | valid_auc: 0.8838  |  0:00:07s\n",
      "epoch 13 | loss: 0.41519 | train_auc: 0.88757 | valid_auc: 0.88371 |  0:00:07s\n",
      "epoch 14 | loss: 0.41621 | train_auc: 0.88859 | valid_auc: 0.88424 |  0:00:08s\n",
      "epoch 15 | loss: 0.41479 | train_auc: 0.88947 | valid_auc: 0.88694 |  0:00:08s\n",
      "epoch 16 | loss: 0.42239 | train_auc: 0.88974 | valid_auc: 0.88585 |  0:00:09s\n",
      "epoch 17 | loss: 0.41038 | train_auc: 0.88981 | valid_auc: 0.8845  |  0:00:09s\n",
      "epoch 18 | loss: 0.41251 | train_auc: 0.89088 | valid_auc: 0.88643 |  0:00:10s\n",
      "epoch 19 | loss: 0.41417 | train_auc: 0.8899  | valid_auc: 0.886   |  0:00:10s\n",
      "epoch 20 | loss: 0.41879 | train_auc: 0.89008 | valid_auc: 0.88571 |  0:00:11s\n",
      "epoch 21 | loss: 0.42102 | train_auc: 0.89053 | valid_auc: 0.88635 |  0:00:11s\n",
      "epoch 22 | loss: 0.41793 | train_auc: 0.89001 | valid_auc: 0.88735 |  0:00:12s\n",
      "epoch 23 | loss: 0.41486 | train_auc: 0.89083 | valid_auc: 0.8872  |  0:00:13s\n",
      "epoch 24 | loss: 0.42224 | train_auc: 0.88984 | valid_auc: 0.88466 |  0:00:13s\n",
      "epoch 25 | loss: 0.4076  | train_auc: 0.89045 | valid_auc: 0.88752 |  0:00:14s\n",
      "epoch 26 | loss: 0.4141  | train_auc: 0.89047 | valid_auc: 0.88688 |  0:00:14s\n",
      "epoch 27 | loss: 0.4246  | train_auc: 0.89069 | valid_auc: 0.88655 |  0:00:15s\n",
      "epoch 28 | loss: 0.41496 | train_auc: 0.89039 | valid_auc: 0.88655 |  0:00:15s\n",
      "epoch 29 | loss: 0.41437 | train_auc: 0.89042 | valid_auc: 0.88759 |  0:00:16s\n",
      "epoch 30 | loss: 0.41224 | train_auc: 0.89077 | valid_auc: 0.88612 |  0:00:16s\n",
      "epoch 31 | loss: 0.41214 | train_auc: 0.89029 | valid_auc: 0.88542 |  0:00:17s\n",
      "epoch 32 | loss: 0.41269 | train_auc: 0.8911  | valid_auc: 0.88772 |  0:00:17s\n",
      "epoch 33 | loss: 0.41414 | train_auc: 0.89127 | valid_auc: 0.88747 |  0:00:18s\n",
      "epoch 34 | loss: 0.41539 | train_auc: 0.89134 | valid_auc: 0.88791 |  0:00:18s\n",
      "epoch 35 | loss: 0.41502 | train_auc: 0.89087 | valid_auc: 0.88639 |  0:00:19s\n",
      "epoch 36 | loss: 0.4098  | train_auc: 0.89087 | valid_auc: 0.88705 |  0:00:20s\n",
      "epoch 37 | loss: 0.41402 | train_auc: 0.89017 | valid_auc: 0.8881  |  0:00:20s\n",
      "epoch 38 | loss: 0.41285 | train_auc: 0.89068 | valid_auc: 0.88641 |  0:00:21s\n",
      "epoch 39 | loss: 0.41893 | train_auc: 0.8917  | valid_auc: 0.88821 |  0:00:21s\n",
      "epoch 40 | loss: 0.4076  | train_auc: 0.89101 | valid_auc: 0.88861 |  0:00:22s\n",
      "epoch 41 | loss: 0.40698 | train_auc: 0.89114 | valid_auc: 0.88681 |  0:00:22s\n",
      "epoch 42 | loss: 0.41232 | train_auc: 0.89109 | valid_auc: 0.88717 |  0:00:23s\n",
      "epoch 43 | loss: 0.41116 | train_auc: 0.89157 | valid_auc: 0.88803 |  0:00:23s\n",
      "epoch 44 | loss: 0.40998 | train_auc: 0.8918  | valid_auc: 0.88861 |  0:00:24s\n",
      "epoch 45 | loss: 0.41414 | train_auc: 0.89124 | valid_auc: 0.88586 |  0:00:24s\n",
      "epoch 46 | loss: 0.41957 | train_auc: 0.89119 | valid_auc: 0.88558 |  0:00:25s\n",
      "epoch 47 | loss: 0.40859 | train_auc: 0.89097 | valid_auc: 0.88782 |  0:00:26s\n",
      "epoch 48 | loss: 0.40938 | train_auc: 0.89212 | valid_auc: 0.88709 |  0:00:26s\n",
      "epoch 49 | loss: 0.41681 | train_auc: 0.89183 | valid_auc: 0.88588 |  0:00:27s\n",
      "epoch 50 | loss: 0.41387 | train_auc: 0.89187 | valid_auc: 0.8883  |  0:00:27s\n",
      "epoch 51 | loss: 0.41388 | train_auc: 0.89156 | valid_auc: 0.88702 |  0:00:28s\n",
      "epoch 52 | loss: 0.40583 | train_auc: 0.89152 | valid_auc: 0.88846 |  0:00:28s\n",
      "epoch 53 | loss: 0.4192  | train_auc: 0.89104 | valid_auc: 0.8874  |  0:00:29s\n",
      "epoch 54 | loss: 0.4097  | train_auc: 0.89149 | valid_auc: 0.88691 |  0:00:29s\n",
      "epoch 55 | loss: 0.40924 | train_auc: 0.89183 | valid_auc: 0.88717 |  0:00:30s\n",
      "epoch 56 | loss: 0.41094 | train_auc: 0.89205 | valid_auc: 0.88657 |  0:00:30s\n",
      "epoch 57 | loss: 0.4146  | train_auc: 0.89225 | valid_auc: 0.88637 |  0:00:31s\n",
      "epoch 58 | loss: 0.41398 | train_auc: 0.89189 | valid_auc: 0.88793 |  0:00:32s\n",
      "epoch 59 | loss: 0.41532 | train_auc: 0.8916  | valid_auc: 0.88586 |  0:00:32s\n",
      "epoch 60 | loss: 0.41146 | train_auc: 0.89227 | valid_auc: 0.88832 |  0:00:33s\n",
      "epoch 61 | loss: 0.40561 | train_auc: 0.89201 | valid_auc: 0.888   |  0:00:33s\n",
      "epoch 62 | loss: 0.41694 | train_auc: 0.892   | valid_auc: 0.88797 |  0:00:34s\n",
      "epoch 63 | loss: 0.4001  | train_auc: 0.89142 | valid_auc: 0.88638 |  0:00:34s\n",
      "epoch 64 | loss: 0.40846 | train_auc: 0.89194 | valid_auc: 0.8871  |  0:00:35s\n",
      "epoch 65 | loss: 0.42025 | train_auc: 0.89218 | valid_auc: 0.8872  |  0:00:35s\n",
      "epoch 66 | loss: 0.41214 | train_auc: 0.89225 | valid_auc: 0.88746 |  0:00:36s\n",
      "epoch 67 | loss: 0.41604 | train_auc: 0.8921  | valid_auc: 0.88737 |  0:00:36s\n",
      "epoch 68 | loss: 0.40514 | train_auc: 0.89196 | valid_auc: 0.88813 |  0:00:37s\n",
      "epoch 69 | loss: 0.41231 | train_auc: 0.89206 | valid_auc: 0.88693 |  0:00:37s\n",
      "epoch 70 | loss: 0.40587 | train_auc: 0.89156 | valid_auc: 0.88559 |  0:00:38s\n",
      "epoch 71 | loss: 0.41578 | train_auc: 0.89141 | valid_auc: 0.88586 |  0:00:39s\n",
      "epoch 72 | loss: 0.40745 | train_auc: 0.89069 | valid_auc: 0.88705 |  0:00:39s\n",
      "epoch 73 | loss: 0.41426 | train_auc: 0.89061 | valid_auc: 0.88687 |  0:00:40s\n",
      "epoch 74 | loss: 0.4064  | train_auc: 0.89174 | valid_auc: 0.88688 |  0:00:40s\n",
      "epoch 75 | loss: 0.41757 | train_auc: 0.89158 | valid_auc: 0.88675 |  0:00:41s\n",
      "epoch 76 | loss: 0.40736 | train_auc: 0.89236 | valid_auc: 0.88721 |  0:00:41s\n",
      "epoch 77 | loss: 0.40632 | train_auc: 0.89211 | valid_auc: 0.88748 |  0:00:42s\n",
      "epoch 78 | loss: 0.41351 | train_auc: 0.89158 | valid_auc: 0.88744 |  0:00:42s\n",
      "epoch 79 | loss: 0.40446 | train_auc: 0.89185 | valid_auc: 0.88911 |  0:00:43s\n",
      "epoch 80 | loss: 0.4144  | train_auc: 0.89175 | valid_auc: 0.88697 |  0:00:43s\n",
      "epoch 81 | loss: 0.40993 | train_auc: 0.89209 | valid_auc: 0.88704 |  0:00:44s\n",
      "epoch 82 | loss: 0.42397 | train_auc: 0.89054 | valid_auc: 0.88668 |  0:00:44s\n",
      "epoch 83 | loss: 0.40502 | train_auc: 0.89102 | valid_auc: 0.88758 |  0:00:45s\n",
      "epoch 84 | loss: 0.41173 | train_auc: 0.89135 | valid_auc: 0.88699 |  0:00:46s\n",
      "epoch 85 | loss: 0.41711 | train_auc: 0.89154 | valid_auc: 0.88646 |  0:00:46s\n",
      "epoch 86 | loss: 0.41268 | train_auc: 0.89212 | valid_auc: 0.88765 |  0:00:47s\n",
      "epoch 87 | loss: 0.41438 | train_auc: 0.8922  | valid_auc: 0.88912 |  0:00:47s\n",
      "epoch 88 | loss: 0.41322 | train_auc: 0.89167 | valid_auc: 0.88773 |  0:00:48s\n",
      "epoch 89 | loss: 0.41074 | train_auc: 0.89211 | valid_auc: 0.8881  |  0:00:48s\n",
      "epoch 90 | loss: 0.41164 | train_auc: 0.89198 | valid_auc: 0.8879  |  0:00:49s\n",
      "epoch 91 | loss: 0.41361 | train_auc: 0.89198 | valid_auc: 0.88802 |  0:00:49s\n",
      "epoch 92 | loss: 0.40904 | train_auc: 0.89215 | valid_auc: 0.88744 |  0:00:50s\n",
      "epoch 93 | loss: 0.41042 | train_auc: 0.89237 | valid_auc: 0.88977 |  0:00:50s\n",
      "epoch 94 | loss: 0.41217 | train_auc: 0.89242 | valid_auc: 0.89009 |  0:00:51s\n",
      "epoch 95 | loss: 0.41648 | train_auc: 0.89257 | valid_auc: 0.88933 |  0:00:52s\n",
      "epoch 96 | loss: 0.40904 | train_auc: 0.89241 | valid_auc: 0.88939 |  0:00:52s\n",
      "epoch 97 | loss: 0.41787 | train_auc: 0.89239 | valid_auc: 0.88715 |  0:00:53s\n",
      "epoch 98 | loss: 0.41222 | train_auc: 0.89229 | valid_auc: 0.88765 |  0:00:53s\n",
      "epoch 99 | loss: 0.41307 | train_auc: 0.89234 | valid_auc: 0.88808 |  0:00:54s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 94 and best_valid_auc = 0.89009\n",
      "sampling:  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69782 | train_auc: 0.57978 | valid_auc: 0.57868 |  0:00:00s\n",
      "epoch 1  | loss: 0.47078 | train_auc: 0.751   | valid_auc: 0.74759 |  0:00:01s\n",
      "epoch 2  | loss: 0.44591 | train_auc: 0.68058 | valid_auc: 0.68107 |  0:00:01s\n",
      "epoch 3  | loss: 0.42491 | train_auc: 0.81749 | valid_auc: 0.8115  |  0:00:02s\n",
      "epoch 4  | loss: 0.42424 | train_auc: 0.80693 | valid_auc: 0.80067 |  0:00:02s\n",
      "epoch 5  | loss: 0.41621 | train_auc: 0.85808 | valid_auc: 0.85273 |  0:00:03s\n",
      "epoch 6  | loss: 0.42768 | train_auc: 0.87258 | valid_auc: 0.8689  |  0:00:03s\n",
      "epoch 7  | loss: 0.43013 | train_auc: 0.87535 | valid_auc: 0.8705  |  0:00:04s\n",
      "epoch 8  | loss: 0.42191 | train_auc: 0.8793  | valid_auc: 0.87535 |  0:00:04s\n",
      "epoch 9  | loss: 0.42493 | train_auc: 0.88279 | valid_auc: 0.8799  |  0:00:05s\n",
      "epoch 10 | loss: 0.41744 | train_auc: 0.88434 | valid_auc: 0.88239 |  0:00:06s\n",
      "epoch 11 | loss: 0.4261  | train_auc: 0.88408 | valid_auc: 0.88347 |  0:00:06s\n",
      "epoch 12 | loss: 0.41716 | train_auc: 0.88486 | valid_auc: 0.88491 |  0:00:07s\n",
      "epoch 13 | loss: 0.42667 | train_auc: 0.88458 | valid_auc: 0.8852  |  0:00:07s\n",
      "epoch 14 | loss: 0.42173 | train_auc: 0.88659 | valid_auc: 0.88452 |  0:00:08s\n",
      "epoch 15 | loss: 0.41475 | train_auc: 0.88721 | valid_auc: 0.88662 |  0:00:08s\n",
      "epoch 16 | loss: 0.42299 | train_auc: 0.88798 | valid_auc: 0.88735 |  0:00:09s\n",
      "epoch 17 | loss: 0.40955 | train_auc: 0.88794 | valid_auc: 0.88725 |  0:00:09s\n",
      "epoch 18 | loss: 0.42607 | train_auc: 0.88819 | valid_auc: 0.88824 |  0:00:10s\n",
      "epoch 19 | loss: 0.42114 | train_auc: 0.88915 | valid_auc: 0.88856 |  0:00:10s\n",
      "epoch 20 | loss: 0.41883 | train_auc: 0.88929 | valid_auc: 0.88811 |  0:00:11s\n",
      "epoch 21 | loss: 0.41832 | train_auc: 0.88877 | valid_auc: 0.88893 |  0:00:12s\n",
      "epoch 22 | loss: 0.40793 | train_auc: 0.88957 | valid_auc: 0.88818 |  0:00:12s\n",
      "epoch 23 | loss: 0.41619 | train_auc: 0.88883 | valid_auc: 0.8871  |  0:00:13s\n",
      "epoch 24 | loss: 0.42628 | train_auc: 0.88917 | valid_auc: 0.88963 |  0:00:13s\n",
      "epoch 25 | loss: 0.42123 | train_auc: 0.88959 | valid_auc: 0.88818 |  0:00:14s\n",
      "epoch 26 | loss: 0.41307 | train_auc: 0.88975 | valid_auc: 0.88822 |  0:00:14s\n",
      "epoch 27 | loss: 0.42104 | train_auc: 0.8889  | valid_auc: 0.88923 |  0:00:15s\n",
      "epoch 28 | loss: 0.41844 | train_auc: 0.89021 | valid_auc: 0.88885 |  0:00:15s\n",
      "epoch 29 | loss: 0.42353 | train_auc: 0.88999 | valid_auc: 0.88905 |  0:00:16s\n",
      "epoch 30 | loss: 0.4119  | train_auc: 0.88963 | valid_auc: 0.88793 |  0:00:16s\n",
      "epoch 31 | loss: 0.42436 | train_auc: 0.88903 | valid_auc: 0.88927 |  0:00:17s\n",
      "epoch 32 | loss: 0.41696 | train_auc: 0.88973 | valid_auc: 0.88819 |  0:00:17s\n",
      "epoch 33 | loss: 0.41404 | train_auc: 0.88939 | valid_auc: 0.88833 |  0:00:18s\n",
      "epoch 34 | loss: 0.41096 | train_auc: 0.89023 | valid_auc: 0.88759 |  0:00:19s\n",
      "epoch 35 | loss: 0.41089 | train_auc: 0.88993 | valid_auc: 0.88753 |  0:00:19s\n",
      "epoch 36 | loss: 0.41024 | train_auc: 0.89022 | valid_auc: 0.88875 |  0:00:20s\n",
      "epoch 37 | loss: 0.41275 | train_auc: 0.88986 | valid_auc: 0.88793 |  0:00:20s\n",
      "epoch 38 | loss: 0.42304 | train_auc: 0.88972 | valid_auc: 0.8881  |  0:00:21s\n",
      "epoch 39 | loss: 0.41737 | train_auc: 0.89042 | valid_auc: 0.8881  |  0:00:21s\n",
      "epoch 40 | loss: 0.41835 | train_auc: 0.89003 | valid_auc: 0.88916 |  0:00:22s\n",
      "epoch 41 | loss: 0.41463 | train_auc: 0.89    | valid_auc: 0.88931 |  0:00:22s\n",
      "epoch 42 | loss: 0.41351 | train_auc: 0.89053 | valid_auc: 0.88909 |  0:00:23s\n",
      "epoch 43 | loss: 0.41609 | train_auc: 0.89024 | valid_auc: 0.88793 |  0:00:23s\n",
      "epoch 44 | loss: 0.41897 | train_auc: 0.89005 | valid_auc: 0.88855 |  0:00:24s\n",
      "epoch 45 | loss: 0.42257 | train_auc: 0.89001 | valid_auc: 0.88832 |  0:00:25s\n",
      "epoch 46 | loss: 0.41776 | train_auc: 0.89043 | valid_auc: 0.88876 |  0:00:25s\n",
      "epoch 47 | loss: 0.42203 | train_auc: 0.89051 | valid_auc: 0.88887 |  0:00:26s\n",
      "epoch 48 | loss: 0.41775 | train_auc: 0.89022 | valid_auc: 0.88828 |  0:00:26s\n",
      "epoch 49 | loss: 0.41151 | train_auc: 0.88964 | valid_auc: 0.88844 |  0:00:27s\n",
      "epoch 50 | loss: 0.41316 | train_auc: 0.89    | valid_auc: 0.88832 |  0:00:27s\n",
      "epoch 51 | loss: 0.42406 | train_auc: 0.88968 | valid_auc: 0.88875 |  0:00:28s\n",
      "epoch 52 | loss: 0.42492 | train_auc: 0.88964 | valid_auc: 0.88793 |  0:00:28s\n",
      "epoch 53 | loss: 0.4174  | train_auc: 0.89014 | valid_auc: 0.88971 |  0:00:29s\n",
      "epoch 54 | loss: 0.41753 | train_auc: 0.89054 | valid_auc: 0.88897 |  0:00:30s\n",
      "epoch 55 | loss: 0.41056 | train_auc: 0.88978 | valid_auc: 0.88792 |  0:00:30s\n",
      "epoch 56 | loss: 0.41378 | train_auc: 0.8897  | valid_auc: 0.88842 |  0:00:31s\n",
      "epoch 57 | loss: 0.41625 | train_auc: 0.89077 | valid_auc: 0.88818 |  0:00:31s\n",
      "epoch 58 | loss: 0.41916 | train_auc: 0.89064 | valid_auc: 0.8888  |  0:00:32s\n",
      "epoch 59 | loss: 0.4218  | train_auc: 0.89055 | valid_auc: 0.88924 |  0:00:32s\n",
      "epoch 60 | loss: 0.41156 | train_auc: 0.89041 | valid_auc: 0.88914 |  0:00:33s\n",
      "epoch 61 | loss: 0.41436 | train_auc: 0.89039 | valid_auc: 0.88927 |  0:00:33s\n",
      "epoch 62 | loss: 0.41581 | train_auc: 0.88997 | valid_auc: 0.88699 |  0:00:34s\n",
      "epoch 63 | loss: 0.41732 | train_auc: 0.89065 | valid_auc: 0.88876 |  0:00:34s\n",
      "epoch 64 | loss: 0.41574 | train_auc: 0.8909  | valid_auc: 0.88763 |  0:00:35s\n",
      "epoch 65 | loss: 0.41718 | train_auc: 0.89064 | valid_auc: 0.8894  |  0:00:35s\n",
      "epoch 66 | loss: 0.4052  | train_auc: 0.89058 | valid_auc: 0.88856 |  0:00:36s\n",
      "epoch 67 | loss: 0.41991 | train_auc: 0.88883 | valid_auc: 0.8879  |  0:00:36s\n",
      "epoch 68 | loss: 0.41084 | train_auc: 0.89054 | valid_auc: 0.88858 |  0:00:37s\n",
      "epoch 69 | loss: 0.41147 | train_auc: 0.8904  | valid_auc: 0.88849 |  0:00:38s\n",
      "epoch 70 | loss: 0.4108  | train_auc: 0.89002 | valid_auc: 0.88919 |  0:00:38s\n",
      "epoch 71 | loss: 0.42348 | train_auc: 0.89042 | valid_auc: 0.88866 |  0:00:39s\n",
      "epoch 72 | loss: 0.41802 | train_auc: 0.89063 | valid_auc: 0.88866 |  0:00:39s\n",
      "epoch 73 | loss: 0.40621 | train_auc: 0.89081 | valid_auc: 0.88836 |  0:00:40s\n",
      "epoch 74 | loss: 0.40803 | train_auc: 0.89032 | valid_auc: 0.88863 |  0:00:40s\n",
      "epoch 75 | loss: 0.40519 | train_auc: 0.89034 | valid_auc: 0.88708 |  0:00:41s\n",
      "epoch 76 | loss: 0.41709 | train_auc: 0.89086 | valid_auc: 0.88904 |  0:00:41s\n",
      "epoch 77 | loss: 0.41409 | train_auc: 0.89117 | valid_auc: 0.88843 |  0:00:42s\n",
      "epoch 78 | loss: 0.41605 | train_auc: 0.89103 | valid_auc: 0.88893 |  0:00:42s\n",
      "epoch 79 | loss: 0.4143  | train_auc: 0.89076 | valid_auc: 0.88934 |  0:00:43s\n",
      "epoch 80 | loss: 0.42067 | train_auc: 0.89059 | valid_auc: 0.88766 |  0:00:43s\n",
      "epoch 81 | loss: 0.40628 | train_auc: 0.89073 | valid_auc: 0.88811 |  0:00:44s\n",
      "epoch 82 | loss: 0.41455 | train_auc: 0.88971 | valid_auc: 0.88819 |  0:00:45s\n",
      "epoch 83 | loss: 0.41541 | train_auc: 0.89101 | valid_auc: 0.88852 |  0:00:45s\n",
      "epoch 84 | loss: 0.41317 | train_auc: 0.89105 | valid_auc: 0.88919 |  0:00:46s\n",
      "epoch 85 | loss: 0.41649 | train_auc: 0.88939 | valid_auc: 0.88829 |  0:00:46s\n",
      "epoch 86 | loss: 0.4071  | train_auc: 0.89104 | valid_auc: 0.88916 |  0:00:47s\n",
      "epoch 87 | loss: 0.41452 | train_auc: 0.89093 | valid_auc: 0.88747 |  0:00:47s\n",
      "epoch 88 | loss: 0.40962 | train_auc: 0.88977 | valid_auc: 0.88943 |  0:00:48s\n",
      "epoch 89 | loss: 0.41443 | train_auc: 0.89044 | valid_auc: 0.8884  |  0:00:48s\n",
      "epoch 90 | loss: 0.41275 | train_auc: 0.89005 | valid_auc: 0.88759 |  0:00:49s\n",
      "epoch 91 | loss: 0.41637 | train_auc: 0.89078 | valid_auc: 0.88983 |  0:00:50s\n",
      "epoch 92 | loss: 0.41081 | train_auc: 0.89103 | valid_auc: 0.88804 |  0:00:50s\n",
      "epoch 93 | loss: 0.41945 | train_auc: 0.8906  | valid_auc: 0.88752 |  0:00:51s\n",
      "epoch 94 | loss: 0.41171 | train_auc: 0.89099 | valid_auc: 0.88879 |  0:00:51s\n",
      "epoch 95 | loss: 0.40826 | train_auc: 0.89019 | valid_auc: 0.88932 |  0:00:52s\n",
      "epoch 96 | loss: 0.41852 | train_auc: 0.89137 | valid_auc: 0.88866 |  0:00:52s\n",
      "epoch 97 | loss: 0.4125  | train_auc: 0.89012 | valid_auc: 0.88925 |  0:00:53s\n",
      "epoch 98 | loss: 0.41423 | train_auc: 0.89103 | valid_auc: 0.88908 |  0:00:53s\n",
      "epoch 99 | loss: 0.41729 | train_auc: 0.89094 | valid_auc: 0.88917 |  0:00:54s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 91 and best_valid_auc = 0.88983\n",
      "sampling:  27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69208 | train_auc: 0.71665 | valid_auc: 0.72953 |  0:00:00s\n",
      "epoch 1  | loss: 0.4898  | train_auc: 0.77311 | valid_auc: 0.77425 |  0:00:01s\n",
      "epoch 2  | loss: 0.44878 | train_auc: 0.75776 | valid_auc: 0.74975 |  0:00:01s\n",
      "epoch 3  | loss: 0.43531 | train_auc: 0.83877 | valid_auc: 0.83553 |  0:00:02s\n",
      "epoch 4  | loss: 0.44271 | train_auc: 0.85961 | valid_auc: 0.86162 |  0:00:02s\n",
      "epoch 5  | loss: 0.42543 | train_auc: 0.86815 | valid_auc: 0.87238 |  0:00:03s\n",
      "epoch 6  | loss: 0.43535 | train_auc: 0.87139 | valid_auc: 0.87425 |  0:00:03s\n",
      "epoch 7  | loss: 0.42054 | train_auc: 0.87212 | valid_auc: 0.87412 |  0:00:04s\n",
      "epoch 8  | loss: 0.42545 | train_auc: 0.88129 | valid_auc: 0.88297 |  0:00:04s\n",
      "epoch 9  | loss: 0.41988 | train_auc: 0.87899 | valid_auc: 0.88251 |  0:00:05s\n",
      "epoch 10 | loss: 0.41942 | train_auc: 0.88193 | valid_auc: 0.88615 |  0:00:06s\n",
      "epoch 11 | loss: 0.42173 | train_auc: 0.88266 | valid_auc: 0.88645 |  0:00:06s\n",
      "epoch 12 | loss: 0.42694 | train_auc: 0.8839  | valid_auc: 0.88746 |  0:00:07s\n",
      "epoch 13 | loss: 0.4216  | train_auc: 0.88366 | valid_auc: 0.88718 |  0:00:07s\n",
      "epoch 14 | loss: 0.41672 | train_auc: 0.88604 | valid_auc: 0.88856 |  0:00:08s\n",
      "epoch 15 | loss: 0.42297 | train_auc: 0.88547 | valid_auc: 0.88858 |  0:00:08s\n",
      "epoch 16 | loss: 0.42491 | train_auc: 0.88471 | valid_auc: 0.88833 |  0:00:09s\n",
      "epoch 17 | loss: 0.4246  | train_auc: 0.88776 | valid_auc: 0.88862 |  0:00:09s\n",
      "epoch 18 | loss: 0.42252 | train_auc: 0.88489 | valid_auc: 0.88663 |  0:00:10s\n",
      "epoch 19 | loss: 0.423   | train_auc: 0.88649 | valid_auc: 0.88772 |  0:00:11s\n",
      "epoch 20 | loss: 0.41814 | train_auc: 0.88655 | valid_auc: 0.88771 |  0:00:11s\n",
      "epoch 21 | loss: 0.43249 | train_auc: 0.88744 | valid_auc: 0.88843 |  0:00:12s\n",
      "epoch 22 | loss: 0.42092 | train_auc: 0.88811 | valid_auc: 0.88879 |  0:00:12s\n",
      "epoch 23 | loss: 0.42296 | train_auc: 0.88694 | valid_auc: 0.88833 |  0:00:13s\n",
      "epoch 24 | loss: 0.42553 | train_auc: 0.88843 | valid_auc: 0.88931 |  0:00:13s\n",
      "epoch 25 | loss: 0.43093 | train_auc: 0.88796 | valid_auc: 0.88829 |  0:00:14s\n",
      "epoch 26 | loss: 0.4182  | train_auc: 0.88807 | valid_auc: 0.8876  |  0:00:14s\n",
      "epoch 27 | loss: 0.415   | train_auc: 0.88829 | valid_auc: 0.88824 |  0:00:15s\n",
      "epoch 28 | loss: 0.42178 | train_auc: 0.88872 | valid_auc: 0.88818 |  0:00:15s\n",
      "epoch 29 | loss: 0.41514 | train_auc: 0.88831 | valid_auc: 0.88905 |  0:00:16s\n",
      "epoch 30 | loss: 0.41606 | train_auc: 0.88879 | valid_auc: 0.88875 |  0:00:16s\n",
      "epoch 31 | loss: 0.41613 | train_auc: 0.88869 | valid_auc: 0.88776 |  0:00:17s\n",
      "epoch 32 | loss: 0.4242  | train_auc: 0.8892  | valid_auc: 0.88901 |  0:00:18s\n",
      "epoch 33 | loss: 0.41268 | train_auc: 0.88956 | valid_auc: 0.88906 |  0:00:18s\n",
      "epoch 34 | loss: 0.41511 | train_auc: 0.88915 | valid_auc: 0.8894  |  0:00:19s\n",
      "epoch 35 | loss: 0.41479 | train_auc: 0.88898 | valid_auc: 0.88881 |  0:00:19s\n",
      "epoch 36 | loss: 0.42247 | train_auc: 0.88917 | valid_auc: 0.88805 |  0:00:20s\n",
      "epoch 37 | loss: 0.41637 | train_auc: 0.88983 | valid_auc: 0.88973 |  0:00:20s\n",
      "epoch 38 | loss: 0.41703 | train_auc: 0.88991 | valid_auc: 0.88856 |  0:00:21s\n",
      "epoch 39 | loss: 0.41859 | train_auc: 0.88981 | valid_auc: 0.88946 |  0:00:21s\n",
      "epoch 40 | loss: 0.41649 | train_auc: 0.88988 | valid_auc: 0.88977 |  0:00:22s\n",
      "epoch 41 | loss: 0.41802 | train_auc: 0.89003 | valid_auc: 0.88973 |  0:00:22s\n",
      "epoch 42 | loss: 0.41904 | train_auc: 0.88919 | valid_auc: 0.88848 |  0:00:23s\n",
      "epoch 43 | loss: 0.42178 | train_auc: 0.88924 | valid_auc: 0.88944 |  0:00:24s\n",
      "epoch 44 | loss: 0.4187  | train_auc: 0.88799 | valid_auc: 0.88811 |  0:00:24s\n",
      "epoch 45 | loss: 0.42667 | train_auc: 0.88907 | valid_auc: 0.88923 |  0:00:25s\n",
      "epoch 46 | loss: 0.4194  | train_auc: 0.88937 | valid_auc: 0.88864 |  0:00:25s\n",
      "epoch 47 | loss: 0.41055 | train_auc: 0.8898  | valid_auc: 0.88905 |  0:00:26s\n",
      "epoch 48 | loss: 0.41697 | train_auc: 0.89027 | valid_auc: 0.8883  |  0:00:26s\n",
      "epoch 49 | loss: 0.41138 | train_auc: 0.88945 | valid_auc: 0.88773 |  0:00:27s\n",
      "epoch 50 | loss: 0.41257 | train_auc: 0.8894  | valid_auc: 0.88697 |  0:00:27s\n",
      "epoch 51 | loss: 0.41485 | train_auc: 0.88939 | valid_auc: 0.88795 |  0:00:28s\n",
      "epoch 52 | loss: 0.41828 | train_auc: 0.89035 | valid_auc: 0.88796 |  0:00:28s\n",
      "epoch 53 | loss: 0.42177 | train_auc: 0.88915 | valid_auc: 0.887   |  0:00:29s\n",
      "epoch 54 | loss: 0.42048 | train_auc: 0.88919 | valid_auc: 0.88813 |  0:00:30s\n",
      "epoch 55 | loss: 0.4199  | train_auc: 0.8887  | valid_auc: 0.88958 |  0:00:30s\n",
      "epoch 56 | loss: 0.41765 | train_auc: 0.88909 | valid_auc: 0.88826 |  0:00:31s\n",
      "epoch 57 | loss: 0.4322  | train_auc: 0.88957 | valid_auc: 0.88865 |  0:00:31s\n",
      "epoch 58 | loss: 0.41825 | train_auc: 0.88838 | valid_auc: 0.88604 |  0:00:32s\n",
      "epoch 59 | loss: 0.4143  | train_auc: 0.88942 | valid_auc: 0.88805 |  0:00:32s\n",
      "epoch 60 | loss: 0.41767 | train_auc: 0.88834 | valid_auc: 0.88774 |  0:00:33s\n",
      "epoch 61 | loss: 0.41935 | train_auc: 0.88946 | valid_auc: 0.88828 |  0:00:33s\n",
      "epoch 62 | loss: 0.43077 | train_auc: 0.88797 | valid_auc: 0.88888 |  0:00:34s\n",
      "epoch 63 | loss: 0.4257  | train_auc: 0.88803 | valid_auc: 0.89021 |  0:00:35s\n",
      "epoch 64 | loss: 0.4157  | train_auc: 0.88806 | valid_auc: 0.88667 |  0:00:35s\n",
      "epoch 65 | loss: 0.42468 | train_auc: 0.88774 | valid_auc: 0.88776 |  0:00:36s\n",
      "epoch 66 | loss: 0.42104 | train_auc: 0.88932 | valid_auc: 0.88916 |  0:00:36s\n",
      "epoch 67 | loss: 0.41779 | train_auc: 0.88942 | valid_auc: 0.88894 |  0:00:37s\n",
      "epoch 68 | loss: 0.41377 | train_auc: 0.88925 | valid_auc: 0.88829 |  0:00:37s\n",
      "epoch 69 | loss: 0.41525 | train_auc: 0.88935 | valid_auc: 0.88898 |  0:00:38s\n",
      "epoch 70 | loss: 0.4286  | train_auc: 0.88926 | valid_auc: 0.88909 |  0:00:38s\n",
      "epoch 71 | loss: 0.41568 | train_auc: 0.88931 | valid_auc: 0.88996 |  0:00:39s\n",
      "epoch 72 | loss: 0.41375 | train_auc: 0.88865 | valid_auc: 0.88945 |  0:00:39s\n",
      "epoch 73 | loss: 0.41816 | train_auc: 0.88903 | valid_auc: 0.88816 |  0:00:40s\n",
      "epoch 74 | loss: 0.41975 | train_auc: 0.88934 | valid_auc: 0.88906 |  0:00:41s\n",
      "epoch 75 | loss: 0.41438 | train_auc: 0.88957 | valid_auc: 0.88877 |  0:00:41s\n",
      "epoch 76 | loss: 0.42048 | train_auc: 0.88962 | valid_auc: 0.88831 |  0:00:42s\n",
      "epoch 77 | loss: 0.41259 | train_auc: 0.88945 | valid_auc: 0.88906 |  0:00:42s\n",
      "epoch 78 | loss: 0.40852 | train_auc: 0.8895  | valid_auc: 0.88832 |  0:00:43s\n",
      "epoch 79 | loss: 0.42007 | train_auc: 0.88978 | valid_auc: 0.88852 |  0:00:43s\n",
      "epoch 80 | loss: 0.41796 | train_auc: 0.88959 | valid_auc: 0.88845 |  0:00:44s\n",
      "epoch 81 | loss: 0.41703 | train_auc: 0.88972 | valid_auc: 0.88912 |  0:00:44s\n",
      "epoch 82 | loss: 0.4153  | train_auc: 0.88982 | valid_auc: 0.88843 |  0:00:45s\n",
      "epoch 83 | loss: 0.41987 | train_auc: 0.89013 | valid_auc: 0.88801 |  0:00:45s\n",
      "epoch 84 | loss: 0.41837 | train_auc: 0.88968 | valid_auc: 0.88877 |  0:00:46s\n",
      "epoch 85 | loss: 0.41039 | train_auc: 0.88941 | valid_auc: 0.88854 |  0:00:47s\n",
      "epoch 86 | loss: 0.41797 | train_auc: 0.88976 | valid_auc: 0.88921 |  0:00:47s\n",
      "epoch 87 | loss: 0.41753 | train_auc: 0.88994 | valid_auc: 0.8883  |  0:00:48s\n",
      "epoch 88 | loss: 0.41757 | train_auc: 0.89014 | valid_auc: 0.888   |  0:00:48s\n",
      "epoch 89 | loss: 0.41995 | train_auc: 0.88986 | valid_auc: 0.88804 |  0:00:49s\n",
      "epoch 90 | loss: 0.41848 | train_auc: 0.89    | valid_auc: 0.88733 |  0:00:49s\n",
      "epoch 91 | loss: 0.42124 | train_auc: 0.88985 | valid_auc: 0.88813 |  0:00:50s\n",
      "epoch 92 | loss: 0.40906 | train_auc: 0.88997 | valid_auc: 0.88847 |  0:00:50s\n",
      "epoch 93 | loss: 0.41553 | train_auc: 0.88924 | valid_auc: 0.88988 |  0:00:51s\n",
      "epoch 94 | loss: 0.41492 | train_auc: 0.88927 | valid_auc: 0.88945 |  0:00:52s\n",
      "epoch 95 | loss: 0.42228 | train_auc: 0.88971 | valid_auc: 0.88885 |  0:00:52s\n",
      "epoch 96 | loss: 0.41835 | train_auc: 0.8899  | valid_auc: 0.88886 |  0:00:53s\n",
      "epoch 97 | loss: 0.41545 | train_auc: 0.89044 | valid_auc: 0.88865 |  0:00:53s\n",
      "epoch 98 | loss: 0.41769 | train_auc: 0.89028 | valid_auc: 0.88857 |  0:00:54s\n",
      "epoch 99 | loss: 0.41417 | train_auc: 0.88994 | valid_auc: 0.88881 |  0:00:54s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 63 and best_valid_auc = 0.89021\n",
      "sampling:  28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.67844 | train_auc: 0.65625 | valid_auc: 0.65475 |  0:00:00s\n",
      "epoch 1  | loss: 0.4651  | train_auc: 0.7225  | valid_auc: 0.71145 |  0:00:01s\n",
      "epoch 2  | loss: 0.43594 | train_auc: 0.76167 | valid_auc: 0.7448  |  0:00:01s\n",
      "epoch 3  | loss: 0.43132 | train_auc: 0.82755 | valid_auc: 0.8173  |  0:00:02s\n",
      "epoch 4  | loss: 0.42955 | train_auc: 0.85587 | valid_auc: 0.84641 |  0:00:02s\n",
      "epoch 5  | loss: 0.43567 | train_auc: 0.856   | valid_auc: 0.84581 |  0:00:03s\n",
      "epoch 6  | loss: 0.42058 | train_auc: 0.86097 | valid_auc: 0.85578 |  0:00:03s\n",
      "epoch 7  | loss: 0.41946 | train_auc: 0.87427 | valid_auc: 0.86681 |  0:00:04s\n",
      "epoch 8  | loss: 0.42042 | train_auc: 0.87547 | valid_auc: 0.86718 |  0:00:04s\n",
      "epoch 9  | loss: 0.42127 | train_auc: 0.8798  | valid_auc: 0.87209 |  0:00:05s\n",
      "epoch 10 | loss: 0.4195  | train_auc: 0.88263 | valid_auc: 0.87451 |  0:00:05s\n",
      "epoch 11 | loss: 0.42339 | train_auc: 0.88398 | valid_auc: 0.87722 |  0:00:06s\n",
      "epoch 12 | loss: 0.42356 | train_auc: 0.88541 | valid_auc: 0.87786 |  0:00:06s\n",
      "epoch 13 | loss: 0.41264 | train_auc: 0.88604 | valid_auc: 0.87908 |  0:00:07s\n",
      "epoch 14 | loss: 0.42129 | train_auc: 0.88765 | valid_auc: 0.87952 |  0:00:07s\n",
      "epoch 15 | loss: 0.41695 | train_auc: 0.88843 | valid_auc: 0.88043 |  0:00:08s\n",
      "epoch 16 | loss: 0.41566 | train_auc: 0.88775 | valid_auc: 0.88021 |  0:00:08s\n",
      "epoch 17 | loss: 0.43066 | train_auc: 0.88827 | valid_auc: 0.88052 |  0:00:09s\n",
      "epoch 18 | loss: 0.42337 | train_auc: 0.88969 | valid_auc: 0.88158 |  0:00:09s\n",
      "epoch 19 | loss: 0.42653 | train_auc: 0.88996 | valid_auc: 0.8804  |  0:00:10s\n",
      "epoch 20 | loss: 0.41853 | train_auc: 0.8897  | valid_auc: 0.88042 |  0:00:10s\n",
      "epoch 21 | loss: 0.42294 | train_auc: 0.89038 | valid_auc: 0.88051 |  0:00:11s\n",
      "epoch 22 | loss: 0.41299 | train_auc: 0.89054 | valid_auc: 0.88188 |  0:00:11s\n",
      "epoch 23 | loss: 0.41679 | train_auc: 0.89055 | valid_auc: 0.88195 |  0:00:12s\n",
      "epoch 24 | loss: 0.41883 | train_auc: 0.89051 | valid_auc: 0.88261 |  0:00:13s\n",
      "epoch 25 | loss: 0.41894 | train_auc: 0.88985 | valid_auc: 0.88229 |  0:00:13s\n",
      "epoch 26 | loss: 0.41982 | train_auc: 0.8897  | valid_auc: 0.88161 |  0:00:14s\n",
      "epoch 27 | loss: 0.40564 | train_auc: 0.89018 | valid_auc: 0.88203 |  0:00:14s\n",
      "epoch 28 | loss: 0.41264 | train_auc: 0.89046 | valid_auc: 0.88259 |  0:00:15s\n",
      "epoch 29 | loss: 0.42403 | train_auc: 0.89032 | valid_auc: 0.88169 |  0:00:15s\n",
      "epoch 30 | loss: 0.40715 | train_auc: 0.89088 | valid_auc: 0.88188 |  0:00:16s\n",
      "epoch 31 | loss: 0.41924 | train_auc: 0.89099 | valid_auc: 0.88253 |  0:00:16s\n",
      "epoch 32 | loss: 0.41463 | train_auc: 0.89103 | valid_auc: 0.88142 |  0:00:17s\n",
      "epoch 33 | loss: 0.41272 | train_auc: 0.89121 | valid_auc: 0.88258 |  0:00:17s\n",
      "epoch 34 | loss: 0.41103 | train_auc: 0.89106 | valid_auc: 0.88185 |  0:00:18s\n",
      "epoch 35 | loss: 0.41685 | train_auc: 0.88975 | valid_auc: 0.88173 |  0:00:19s\n",
      "epoch 36 | loss: 0.42348 | train_auc: 0.89017 | valid_auc: 0.88195 |  0:00:19s\n",
      "epoch 37 | loss: 0.41191 | train_auc: 0.89024 | valid_auc: 0.8823  |  0:00:20s\n",
      "epoch 38 | loss: 0.41231 | train_auc: 0.89088 | valid_auc: 0.88352 |  0:00:20s\n",
      "epoch 39 | loss: 0.40386 | train_auc: 0.8909  | valid_auc: 0.88185 |  0:00:21s\n",
      "epoch 40 | loss: 0.42126 | train_auc: 0.8902  | valid_auc: 0.88266 |  0:00:21s\n",
      "epoch 41 | loss: 0.41621 | train_auc: 0.89075 | valid_auc: 0.88263 |  0:00:22s\n",
      "epoch 42 | loss: 0.41794 | train_auc: 0.89082 | valid_auc: 0.88271 |  0:00:22s\n",
      "epoch 43 | loss: 0.40777 | train_auc: 0.89117 | valid_auc: 0.88301 |  0:00:23s\n",
      "epoch 44 | loss: 0.41927 | train_auc: 0.89113 | valid_auc: 0.88339 |  0:00:24s\n",
      "epoch 45 | loss: 0.41637 | train_auc: 0.89074 | valid_auc: 0.88137 |  0:00:24s\n",
      "epoch 46 | loss: 0.42065 | train_auc: 0.88999 | valid_auc: 0.88205 |  0:00:25s\n",
      "epoch 47 | loss: 0.41516 | train_auc: 0.8906  | valid_auc: 0.88263 |  0:00:25s\n",
      "epoch 48 | loss: 0.41267 | train_auc: 0.89104 | valid_auc: 0.88216 |  0:00:26s\n",
      "epoch 49 | loss: 0.41344 | train_auc: 0.89105 | valid_auc: 0.88319 |  0:00:26s\n",
      "epoch 50 | loss: 0.41504 | train_auc: 0.8913  | valid_auc: 0.88261 |  0:00:27s\n",
      "epoch 51 | loss: 0.4175  | train_auc: 0.89131 | valid_auc: 0.88242 |  0:00:27s\n",
      "epoch 52 | loss: 0.41985 | train_auc: 0.89132 | valid_auc: 0.88267 |  0:00:28s\n",
      "epoch 53 | loss: 0.4088  | train_auc: 0.89085 | valid_auc: 0.88257 |  0:00:28s\n",
      "epoch 54 | loss: 0.41893 | train_auc: 0.89147 | valid_auc: 0.88293 |  0:00:29s\n",
      "epoch 55 | loss: 0.41607 | train_auc: 0.89134 | valid_auc: 0.88309 |  0:00:30s\n",
      "epoch 56 | loss: 0.40884 | train_auc: 0.89165 | valid_auc: 0.88287 |  0:00:30s\n",
      "epoch 57 | loss: 0.41215 | train_auc: 0.89078 | valid_auc: 0.8829  |  0:00:31s\n",
      "epoch 58 | loss: 0.42391 | train_auc: 0.89123 | valid_auc: 0.88364 |  0:00:31s\n",
      "epoch 59 | loss: 0.415   | train_auc: 0.89083 | valid_auc: 0.88271 |  0:00:32s\n",
      "epoch 60 | loss: 0.41895 | train_auc: 0.89172 | valid_auc: 0.88253 |  0:00:32s\n",
      "epoch 61 | loss: 0.41572 | train_auc: 0.8918  | valid_auc: 0.88276 |  0:00:33s\n",
      "epoch 62 | loss: 0.41959 | train_auc: 0.89161 | valid_auc: 0.88267 |  0:00:33s\n",
      "epoch 63 | loss: 0.40994 | train_auc: 0.89092 | valid_auc: 0.88246 |  0:00:34s\n",
      "epoch 64 | loss: 0.40984 | train_auc: 0.89138 | valid_auc: 0.88254 |  0:00:34s\n",
      "epoch 65 | loss: 0.42076 | train_auc: 0.89149 | valid_auc: 0.88239 |  0:00:35s\n",
      "epoch 66 | loss: 0.41875 | train_auc: 0.89115 | valid_auc: 0.88291 |  0:00:35s\n",
      "epoch 67 | loss: 0.41311 | train_auc: 0.89218 | valid_auc: 0.88379 |  0:00:36s\n",
      "epoch 68 | loss: 0.41077 | train_auc: 0.89185 | valid_auc: 0.88312 |  0:00:37s\n",
      "epoch 69 | loss: 0.4126  | train_auc: 0.89131 | valid_auc: 0.88281 |  0:00:37s\n",
      "epoch 70 | loss: 0.41603 | train_auc: 0.89092 | valid_auc: 0.88211 |  0:00:38s\n",
      "epoch 71 | loss: 0.41866 | train_auc: 0.89181 | valid_auc: 0.88281 |  0:00:38s\n",
      "epoch 72 | loss: 0.40608 | train_auc: 0.89185 | valid_auc: 0.88269 |  0:00:39s\n",
      "epoch 73 | loss: 0.41264 | train_auc: 0.89199 | valid_auc: 0.88254 |  0:00:39s\n",
      "epoch 74 | loss: 0.40494 | train_auc: 0.89137 | valid_auc: 0.88274 |  0:00:40s\n",
      "epoch 75 | loss: 0.41081 | train_auc: 0.89154 | valid_auc: 0.88325 |  0:00:40s\n",
      "epoch 76 | loss: 0.41301 | train_auc: 0.89166 | valid_auc: 0.88214 |  0:00:41s\n",
      "epoch 77 | loss: 0.40881 | train_auc: 0.89219 | valid_auc: 0.88279 |  0:00:41s\n",
      "epoch 78 | loss: 0.41514 | train_auc: 0.89164 | valid_auc: 0.88233 |  0:00:42s\n",
      "epoch 79 | loss: 0.40743 | train_auc: 0.89206 | valid_auc: 0.88245 |  0:00:43s\n",
      "epoch 80 | loss: 0.41608 | train_auc: 0.89159 | valid_auc: 0.88251 |  0:00:43s\n",
      "epoch 81 | loss: 0.40751 | train_auc: 0.89016 | valid_auc: 0.88086 |  0:00:44s\n",
      "epoch 82 | loss: 0.41127 | train_auc: 0.89034 | valid_auc: 0.88166 |  0:00:44s\n",
      "epoch 83 | loss: 0.41579 | train_auc: 0.89114 | valid_auc: 0.88246 |  0:00:45s\n",
      "epoch 84 | loss: 0.41293 | train_auc: 0.89099 | valid_auc: 0.88172 |  0:00:45s\n",
      "epoch 85 | loss: 0.40623 | train_auc: 0.89208 | valid_auc: 0.88251 |  0:00:46s\n",
      "epoch 86 | loss: 0.41591 | train_auc: 0.89172 | valid_auc: 0.88236 |  0:00:46s\n",
      "epoch 87 | loss: 0.41355 | train_auc: 0.89227 | valid_auc: 0.88257 |  0:00:47s\n",
      "epoch 88 | loss: 0.40918 | train_auc: 0.89213 | valid_auc: 0.88235 |  0:00:48s\n",
      "epoch 89 | loss: 0.40031 | train_auc: 0.89172 | valid_auc: 0.88196 |  0:00:48s\n",
      "epoch 90 | loss: 0.40569 | train_auc: 0.89172 | valid_auc: 0.88266 |  0:00:49s\n",
      "epoch 91 | loss: 0.41038 | train_auc: 0.89167 | valid_auc: 0.88299 |  0:00:49s\n",
      "epoch 92 | loss: 0.41996 | train_auc: 0.89197 | valid_auc: 0.88208 |  0:00:50s\n",
      "epoch 93 | loss: 0.41208 | train_auc: 0.8919  | valid_auc: 0.88209 |  0:00:50s\n",
      "epoch 94 | loss: 0.40742 | train_auc: 0.89176 | valid_auc: 0.88327 |  0:00:51s\n",
      "epoch 95 | loss: 0.40388 | train_auc: 0.89149 | valid_auc: 0.88287 |  0:00:51s\n",
      "epoch 96 | loss: 0.4128  | train_auc: 0.89108 | valid_auc: 0.88239 |  0:00:52s\n",
      "epoch 97 | loss: 0.41563 | train_auc: 0.89194 | valid_auc: 0.88299 |  0:00:52s\n",
      "epoch 98 | loss: 0.40618 | train_auc: 0.89183 | valid_auc: 0.88106 |  0:00:53s\n",
      "epoch 99 | loss: 0.40246 | train_auc: 0.89195 | valid_auc: 0.8823  |  0:00:53s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 67 and best_valid_auc = 0.88379\n",
      "sampling:  29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.71089 | train_auc: 0.58354 | valid_auc: 0.60805 |  0:00:00s\n",
      "epoch 1  | loss: 0.4708  | train_auc: 0.78845 | valid_auc: 0.78827 |  0:00:01s\n",
      "epoch 2  | loss: 0.43632 | train_auc: 0.82196 | valid_auc: 0.81516 |  0:00:01s\n",
      "epoch 3  | loss: 0.42823 | train_auc: 0.85676 | valid_auc: 0.84966 |  0:00:02s\n",
      "epoch 4  | loss: 0.42449 | train_auc: 0.86224 | valid_auc: 0.858   |  0:00:02s\n",
      "epoch 5  | loss: 0.42197 | train_auc: 0.87756 | valid_auc: 0.8753  |  0:00:03s\n",
      "epoch 6  | loss: 0.4218  | train_auc: 0.87779 | valid_auc: 0.87366 |  0:00:03s\n",
      "epoch 7  | loss: 0.41835 | train_auc: 0.88406 | valid_auc: 0.88209 |  0:00:04s\n",
      "epoch 8  | loss: 0.42317 | train_auc: 0.881   | valid_auc: 0.87975 |  0:00:04s\n",
      "epoch 9  | loss: 0.42456 | train_auc: 0.88382 | valid_auc: 0.88258 |  0:00:05s\n",
      "epoch 10 | loss: 0.41665 | train_auc: 0.88781 | valid_auc: 0.88626 |  0:00:06s\n",
      "epoch 11 | loss: 0.41792 | train_auc: 0.88823 | valid_auc: 0.88653 |  0:00:06s\n",
      "epoch 12 | loss: 0.4131  | train_auc: 0.88918 | valid_auc: 0.88755 |  0:00:07s\n",
      "epoch 13 | loss: 0.4181  | train_auc: 0.88949 | valid_auc: 0.88755 |  0:00:07s\n",
      "epoch 14 | loss: 0.41176 | train_auc: 0.89049 | valid_auc: 0.88931 |  0:00:08s\n",
      "epoch 15 | loss: 0.41776 | train_auc: 0.8918  | valid_auc: 0.89064 |  0:00:08s\n",
      "epoch 16 | loss: 0.41403 | train_auc: 0.89144 | valid_auc: 0.89103 |  0:00:09s\n",
      "epoch 17 | loss: 0.40999 | train_auc: 0.8913  | valid_auc: 0.89052 |  0:00:09s\n",
      "epoch 18 | loss: 0.41245 | train_auc: 0.89169 | valid_auc: 0.89063 |  0:00:10s\n",
      "epoch 19 | loss: 0.41102 | train_auc: 0.89125 | valid_auc: 0.8914  |  0:00:11s\n",
      "epoch 20 | loss: 0.41018 | train_auc: 0.89079 | valid_auc: 0.88906 |  0:00:11s\n",
      "epoch 21 | loss: 0.41589 | train_auc: 0.89245 | valid_auc: 0.89182 |  0:00:12s\n",
      "epoch 22 | loss: 0.40984 | train_auc: 0.8914  | valid_auc: 0.89077 |  0:00:12s\n",
      "epoch 23 | loss: 0.40722 | train_auc: 0.89121 | valid_auc: 0.89053 |  0:00:13s\n",
      "epoch 24 | loss: 0.41349 | train_auc: 0.8921  | valid_auc: 0.89131 |  0:00:13s\n",
      "epoch 25 | loss: 0.41209 | train_auc: 0.89194 | valid_auc: 0.89158 |  0:00:14s\n",
      "epoch 26 | loss: 0.40799 | train_auc: 0.89235 | valid_auc: 0.89077 |  0:00:14s\n",
      "epoch 27 | loss: 0.41645 | train_auc: 0.8927  | valid_auc: 0.89135 |  0:00:15s\n",
      "epoch 28 | loss: 0.41531 | train_auc: 0.89271 | valid_auc: 0.89196 |  0:00:15s\n",
      "epoch 29 | loss: 0.41567 | train_auc: 0.89282 | valid_auc: 0.89207 |  0:00:16s\n",
      "epoch 30 | loss: 0.42029 | train_auc: 0.89217 | valid_auc: 0.89123 |  0:00:16s\n",
      "epoch 31 | loss: 0.42167 | train_auc: 0.89233 | valid_auc: 0.89146 |  0:00:17s\n",
      "epoch 32 | loss: 0.41656 | train_auc: 0.89259 | valid_auc: 0.89205 |  0:00:18s\n",
      "epoch 33 | loss: 0.41444 | train_auc: 0.89253 | valid_auc: 0.89174 |  0:00:18s\n",
      "epoch 34 | loss: 0.41275 | train_auc: 0.8922  | valid_auc: 0.89157 |  0:00:19s\n",
      "epoch 35 | loss: 0.40976 | train_auc: 0.89276 | valid_auc: 0.89163 |  0:00:19s\n",
      "epoch 36 | loss: 0.40954 | train_auc: 0.8926  | valid_auc: 0.89191 |  0:00:20s\n",
      "epoch 37 | loss: 0.40969 | train_auc: 0.89284 | valid_auc: 0.89169 |  0:00:20s\n",
      "epoch 38 | loss: 0.41832 | train_auc: 0.89249 | valid_auc: 0.89169 |  0:00:21s\n",
      "epoch 39 | loss: 0.41173 | train_auc: 0.89305 | valid_auc: 0.89291 |  0:00:21s\n",
      "epoch 40 | loss: 0.40743 | train_auc: 0.89336 | valid_auc: 0.8924  |  0:00:22s\n",
      "epoch 41 | loss: 0.4065  | train_auc: 0.8926  | valid_auc: 0.89235 |  0:00:22s\n",
      "epoch 42 | loss: 0.42218 | train_auc: 0.893   | valid_auc: 0.89246 |  0:00:23s\n",
      "epoch 43 | loss: 0.40847 | train_auc: 0.89324 | valid_auc: 0.8931  |  0:00:24s\n",
      "epoch 44 | loss: 0.40686 | train_auc: 0.89248 | valid_auc: 0.8932  |  0:00:24s\n",
      "epoch 45 | loss: 0.40849 | train_auc: 0.8931  | valid_auc: 0.89319 |  0:00:25s\n",
      "epoch 46 | loss: 0.40887 | train_auc: 0.89326 | valid_auc: 0.89291 |  0:00:25s\n",
      "epoch 47 | loss: 0.41047 | train_auc: 0.89312 | valid_auc: 0.89285 |  0:00:26s\n",
      "epoch 48 | loss: 0.4083  | train_auc: 0.89309 | valid_auc: 0.8927  |  0:00:26s\n",
      "epoch 49 | loss: 0.40898 | train_auc: 0.89307 | valid_auc: 0.89325 |  0:00:27s\n",
      "epoch 50 | loss: 0.41169 | train_auc: 0.89363 | valid_auc: 0.89345 |  0:00:27s\n",
      "epoch 51 | loss: 0.41835 | train_auc: 0.89327 | valid_auc: 0.89302 |  0:00:28s\n",
      "epoch 52 | loss: 0.41729 | train_auc: 0.89335 | valid_auc: 0.8932  |  0:00:28s\n",
      "epoch 53 | loss: 0.40399 | train_auc: 0.8933  | valid_auc: 0.89301 |  0:00:29s\n",
      "epoch 54 | loss: 0.41417 | train_auc: 0.89278 | valid_auc: 0.89216 |  0:00:30s\n",
      "epoch 55 | loss: 0.40339 | train_auc: 0.89293 | valid_auc: 0.89181 |  0:00:30s\n",
      "epoch 56 | loss: 0.40738 | train_auc: 0.89309 | valid_auc: 0.89172 |  0:00:31s\n",
      "epoch 57 | loss: 0.4138  | train_auc: 0.89326 | valid_auc: 0.89123 |  0:00:31s\n",
      "epoch 58 | loss: 0.41301 | train_auc: 0.89278 | valid_auc: 0.89216 |  0:00:32s\n",
      "epoch 59 | loss: 0.40833 | train_auc: 0.89305 | valid_auc: 0.89181 |  0:00:32s\n",
      "epoch 60 | loss: 0.41212 | train_auc: 0.89369 | valid_auc: 0.89265 |  0:00:33s\n",
      "epoch 61 | loss: 0.40773 | train_auc: 0.89369 | valid_auc: 0.89298 |  0:00:33s\n",
      "epoch 62 | loss: 0.40574 | train_auc: 0.89342 | valid_auc: 0.89238 |  0:00:34s\n",
      "epoch 63 | loss: 0.41083 | train_auc: 0.89342 | valid_auc: 0.89248 |  0:00:34s\n",
      "epoch 64 | loss: 0.41646 | train_auc: 0.8932  | valid_auc: 0.89199 |  0:00:35s\n",
      "epoch 65 | loss: 0.41434 | train_auc: 0.89354 | valid_auc: 0.89252 |  0:00:36s\n",
      "epoch 66 | loss: 0.40876 | train_auc: 0.8939  | valid_auc: 0.89299 |  0:00:36s\n",
      "epoch 67 | loss: 0.40711 | train_auc: 0.89329 | valid_auc: 0.89183 |  0:00:37s\n",
      "epoch 68 | loss: 0.41042 | train_auc: 0.89356 | valid_auc: 0.89279 |  0:00:37s\n",
      "epoch 69 | loss: 0.40996 | train_auc: 0.89254 | valid_auc: 0.89231 |  0:00:38s\n",
      "epoch 70 | loss: 0.42098 | train_auc: 0.89325 | valid_auc: 0.89256 |  0:00:38s\n",
      "epoch 71 | loss: 0.40907 | train_auc: 0.89325 | valid_auc: 0.89313 |  0:00:39s\n",
      "epoch 72 | loss: 0.41037 | train_auc: 0.89356 | valid_auc: 0.89289 |  0:00:39s\n",
      "epoch 73 | loss: 0.41892 | train_auc: 0.89329 | valid_auc: 0.89185 |  0:00:40s\n",
      "epoch 74 | loss: 0.40428 | train_auc: 0.8935  | valid_auc: 0.89277 |  0:00:40s\n",
      "epoch 75 | loss: 0.40771 | train_auc: 0.89278 | valid_auc: 0.89102 |  0:00:41s\n",
      "epoch 76 | loss: 0.41265 | train_auc: 0.89351 | valid_auc: 0.89226 |  0:00:42s\n",
      "epoch 77 | loss: 0.40924 | train_auc: 0.89359 | valid_auc: 0.89241 |  0:00:42s\n",
      "epoch 78 | loss: 0.41085 | train_auc: 0.89336 | valid_auc: 0.89302 |  0:00:43s\n",
      "epoch 79 | loss: 0.40968 | train_auc: 0.8936  | valid_auc: 0.89294 |  0:00:43s\n",
      "epoch 80 | loss: 0.4086  | train_auc: 0.89375 | valid_auc: 0.89283 |  0:00:44s\n",
      "epoch 81 | loss: 0.40603 | train_auc: 0.89387 | valid_auc: 0.89278 |  0:00:44s\n",
      "epoch 82 | loss: 0.40637 | train_auc: 0.89383 | valid_auc: 0.89311 |  0:00:45s\n",
      "epoch 83 | loss: 0.416   | train_auc: 0.89354 | valid_auc: 0.89196 |  0:00:45s\n",
      "epoch 84 | loss: 0.4023  | train_auc: 0.8936  | valid_auc: 0.89311 |  0:00:46s\n",
      "epoch 85 | loss: 0.40992 | train_auc: 0.89342 | valid_auc: 0.89213 |  0:00:46s\n",
      "epoch 86 | loss: 0.41296 | train_auc: 0.89408 | valid_auc: 0.89288 |  0:00:47s\n",
      "epoch 87 | loss: 0.40786 | train_auc: 0.89394 | valid_auc: 0.89297 |  0:00:48s\n",
      "epoch 88 | loss: 0.40812 | train_auc: 0.89348 | valid_auc: 0.89254 |  0:00:48s\n",
      "epoch 89 | loss: 0.39803 | train_auc: 0.89364 | valid_auc: 0.89297 |  0:00:49s\n",
      "epoch 90 | loss: 0.41326 | train_auc: 0.89335 | valid_auc: 0.89205 |  0:00:49s\n",
      "epoch 91 | loss: 0.40952 | train_auc: 0.89389 | valid_auc: 0.89224 |  0:00:50s\n",
      "epoch 92 | loss: 0.41071 | train_auc: 0.89335 | valid_auc: 0.89197 |  0:00:50s\n",
      "epoch 93 | loss: 0.41275 | train_auc: 0.8934  | valid_auc: 0.89223 |  0:00:51s\n",
      "epoch 94 | loss: 0.41232 | train_auc: 0.89303 | valid_auc: 0.8925  |  0:00:51s\n",
      "epoch 95 | loss: 0.40469 | train_auc: 0.89344 | valid_auc: 0.89336 |  0:00:52s\n",
      "epoch 96 | loss: 0.4098  | train_auc: 0.89351 | valid_auc: 0.89286 |  0:00:52s\n",
      "epoch 97 | loss: 0.41686 | train_auc: 0.89341 | valid_auc: 0.89307 |  0:00:53s\n",
      "epoch 98 | loss: 0.4073  | train_auc: 0.89287 | valid_auc: 0.89217 |  0:00:53s\n",
      "epoch 99 | loss: 0.41406 | train_auc: 0.89122 | valid_auc: 0.89072 |  0:00:54s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 50 and best_valid_auc = 0.89345\n",
      "sampling:  30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.71283 | train_auc: 0.52785 | valid_auc: 0.53442 |  0:00:00s\n",
      "epoch 1  | loss: 0.46992 | train_auc: 0.66395 | valid_auc: 0.67538 |  0:00:01s\n",
      "epoch 2  | loss: 0.4478  | train_auc: 0.76282 | valid_auc: 0.77301 |  0:00:01s\n",
      "epoch 3  | loss: 0.43766 | train_auc: 0.84567 | valid_auc: 0.85253 |  0:00:02s\n",
      "epoch 4  | loss: 0.4347  | train_auc: 0.86095 | valid_auc: 0.86418 |  0:00:02s\n",
      "epoch 5  | loss: 0.43358 | train_auc: 0.87386 | valid_auc: 0.87727 |  0:00:03s\n",
      "epoch 6  | loss: 0.42093 | train_auc: 0.873   | valid_auc: 0.87715 |  0:00:03s\n",
      "epoch 7  | loss: 0.42298 | train_auc: 0.87259 | valid_auc: 0.87635 |  0:00:04s\n",
      "epoch 8  | loss: 0.42134 | train_auc: 0.88072 | valid_auc: 0.88561 |  0:00:04s\n",
      "epoch 9  | loss: 0.42146 | train_auc: 0.8842  | valid_auc: 0.88901 |  0:00:05s\n",
      "epoch 10 | loss: 0.42168 | train_auc: 0.88596 | valid_auc: 0.88939 |  0:00:06s\n",
      "epoch 11 | loss: 0.41842 | train_auc: 0.88673 | valid_auc: 0.88986 |  0:00:06s\n",
      "epoch 12 | loss: 0.42495 | train_auc: 0.88494 | valid_auc: 0.88763 |  0:00:07s\n",
      "epoch 13 | loss: 0.42322 | train_auc: 0.88399 | valid_auc: 0.88685 |  0:00:07s\n",
      "epoch 14 | loss: 0.42085 | train_auc: 0.88619 | valid_auc: 0.88894 |  0:00:08s\n",
      "epoch 15 | loss: 0.42346 | train_auc: 0.88685 | valid_auc: 0.8906  |  0:00:08s\n",
      "epoch 16 | loss: 0.4182  | train_auc: 0.88748 | valid_auc: 0.89128 |  0:00:09s\n",
      "epoch 17 | loss: 0.42451 | train_auc: 0.88742 | valid_auc: 0.89111 |  0:00:09s\n",
      "epoch 18 | loss: 0.41181 | train_auc: 0.88839 | valid_auc: 0.89211 |  0:00:10s\n",
      "epoch 19 | loss: 0.42049 | train_auc: 0.88861 | valid_auc: 0.89214 |  0:00:11s\n",
      "epoch 20 | loss: 0.41824 | train_auc: 0.88834 | valid_auc: 0.89238 |  0:00:11s\n",
      "epoch 21 | loss: 0.41695 | train_auc: 0.88793 | valid_auc: 0.8919  |  0:00:12s\n",
      "epoch 22 | loss: 0.41051 | train_auc: 0.88818 | valid_auc: 0.89291 |  0:00:12s\n",
      "epoch 23 | loss: 0.42451 | train_auc: 0.88905 | valid_auc: 0.89323 |  0:00:13s\n",
      "epoch 24 | loss: 0.41522 | train_auc: 0.88879 | valid_auc: 0.89238 |  0:00:13s\n",
      "epoch 25 | loss: 0.41988 | train_auc: 0.88741 | valid_auc: 0.89147 |  0:00:14s\n",
      "epoch 26 | loss: 0.40913 | train_auc: 0.88804 | valid_auc: 0.89123 |  0:00:14s\n",
      "epoch 27 | loss: 0.41544 | train_auc: 0.88903 | valid_auc: 0.89255 |  0:00:15s\n",
      "epoch 28 | loss: 0.42511 | train_auc: 0.88921 | valid_auc: 0.89285 |  0:00:15s\n",
      "epoch 29 | loss: 0.41762 | train_auc: 0.88902 | valid_auc: 0.89225 |  0:00:16s\n",
      "epoch 30 | loss: 0.41285 | train_auc: 0.88895 | valid_auc: 0.89279 |  0:00:16s\n",
      "epoch 31 | loss: 0.41168 | train_auc: 0.88953 | valid_auc: 0.89374 |  0:00:17s\n",
      "epoch 32 | loss: 0.42543 | train_auc: 0.88939 | valid_auc: 0.8918  |  0:00:18s\n",
      "epoch 33 | loss: 0.41516 | train_auc: 0.88937 | valid_auc: 0.89212 |  0:00:18s\n",
      "epoch 34 | loss: 0.41514 | train_auc: 0.88842 | valid_auc: 0.89245 |  0:00:19s\n",
      "epoch 35 | loss: 0.4145  | train_auc: 0.88888 | valid_auc: 0.89265 |  0:00:19s\n",
      "epoch 36 | loss: 0.41898 | train_auc: 0.88976 | valid_auc: 0.89333 |  0:00:20s\n",
      "epoch 37 | loss: 0.4202  | train_auc: 0.88905 | valid_auc: 0.89262 |  0:00:20s\n",
      "epoch 38 | loss: 0.4207  | train_auc: 0.88908 | valid_auc: 0.89249 |  0:00:21s\n",
      "epoch 39 | loss: 0.41108 | train_auc: 0.88972 | valid_auc: 0.89282 |  0:00:21s\n",
      "epoch 40 | loss: 0.41795 | train_auc: 0.88861 | valid_auc: 0.89168 |  0:00:22s\n",
      "epoch 41 | loss: 0.41467 | train_auc: 0.88958 | valid_auc: 0.89294 |  0:00:23s\n",
      "epoch 42 | loss: 0.42529 | train_auc: 0.88964 | valid_auc: 0.89189 |  0:00:23s\n",
      "epoch 43 | loss: 0.42215 | train_auc: 0.88907 | valid_auc: 0.8924  |  0:00:24s\n",
      "epoch 44 | loss: 0.41051 | train_auc: 0.88929 | valid_auc: 0.89227 |  0:00:24s\n",
      "epoch 45 | loss: 0.41369 | train_auc: 0.88974 | valid_auc: 0.89313 |  0:00:25s\n",
      "epoch 46 | loss: 0.41794 | train_auc: 0.89003 | valid_auc: 0.8932  |  0:00:25s\n",
      "epoch 47 | loss: 0.4102  | train_auc: 0.89019 | valid_auc: 0.8937  |  0:00:26s\n",
      "epoch 48 | loss: 0.41368 | train_auc: 0.89001 | valid_auc: 0.89327 |  0:00:26s\n",
      "epoch 49 | loss: 0.42301 | train_auc: 0.88985 | valid_auc: 0.89284 |  0:00:27s\n",
      "epoch 50 | loss: 0.4201  | train_auc: 0.88972 | valid_auc: 0.89234 |  0:00:27s\n",
      "epoch 51 | loss: 0.41586 | train_auc: 0.8888  | valid_auc: 0.89215 |  0:00:28s\n",
      "epoch 52 | loss: 0.4153  | train_auc: 0.88995 | valid_auc: 0.89291 |  0:00:28s\n",
      "epoch 53 | loss: 0.41344 | train_auc: 0.88912 | valid_auc: 0.89138 |  0:00:29s\n",
      "epoch 54 | loss: 0.41087 | train_auc: 0.88887 | valid_auc: 0.89206 |  0:00:30s\n",
      "epoch 55 | loss: 0.41166 | train_auc: 0.88955 | valid_auc: 0.89256 |  0:00:30s\n",
      "epoch 56 | loss: 0.40657 | train_auc: 0.88986 | valid_auc: 0.89265 |  0:00:31s\n",
      "epoch 57 | loss: 0.421   | train_auc: 0.89024 | valid_auc: 0.89316 |  0:00:31s\n",
      "epoch 58 | loss: 0.42362 | train_auc: 0.89006 | valid_auc: 0.89288 |  0:00:32s\n",
      "epoch 59 | loss: 0.40756 | train_auc: 0.88983 | valid_auc: 0.89305 |  0:00:32s\n",
      "epoch 60 | loss: 0.41968 | train_auc: 0.88997 | valid_auc: 0.89229 |  0:00:33s\n",
      "epoch 61 | loss: 0.41662 | train_auc: 0.89005 | valid_auc: 0.89313 |  0:00:33s\n",
      "epoch 62 | loss: 0.41701 | train_auc: 0.88981 | valid_auc: 0.89334 |  0:00:34s\n",
      "epoch 63 | loss: 0.42016 | train_auc: 0.89022 | valid_auc: 0.89385 |  0:00:35s\n",
      "epoch 64 | loss: 0.41551 | train_auc: 0.8906  | valid_auc: 0.8937  |  0:00:35s\n",
      "epoch 65 | loss: 0.41454 | train_auc: 0.89042 | valid_auc: 0.89365 |  0:00:36s\n",
      "epoch 66 | loss: 0.41184 | train_auc: 0.89033 | valid_auc: 0.8935  |  0:00:36s\n",
      "epoch 67 | loss: 0.4091  | train_auc: 0.89036 | valid_auc: 0.89268 |  0:00:37s\n",
      "epoch 68 | loss: 0.4052  | train_auc: 0.89045 | valid_auc: 0.89234 |  0:00:37s\n",
      "epoch 69 | loss: 0.42285 | train_auc: 0.89049 | valid_auc: 0.89226 |  0:00:38s\n",
      "epoch 70 | loss: 0.41442 | train_auc: 0.88991 | valid_auc: 0.89112 |  0:00:38s\n",
      "epoch 71 | loss: 0.41677 | train_auc: 0.88935 | valid_auc: 0.89327 |  0:00:39s\n",
      "epoch 72 | loss: 0.40597 | train_auc: 0.88961 | valid_auc: 0.89322 |  0:00:39s\n",
      "epoch 73 | loss: 0.4102  | train_auc: 0.89021 | valid_auc: 0.8942  |  0:00:40s\n",
      "epoch 74 | loss: 0.41154 | train_auc: 0.89057 | valid_auc: 0.89362 |  0:00:41s\n",
      "epoch 75 | loss: 0.40673 | train_auc: 0.89066 | valid_auc: 0.8934  |  0:00:41s\n",
      "epoch 76 | loss: 0.41803 | train_auc: 0.89013 | valid_auc: 0.89275 |  0:00:42s\n",
      "epoch 77 | loss: 0.41155 | train_auc: 0.88974 | valid_auc: 0.89296 |  0:00:42s\n",
      "epoch 78 | loss: 0.41832 | train_auc: 0.8905  | valid_auc: 0.8935  |  0:00:43s\n",
      "epoch 79 | loss: 0.41975 | train_auc: 0.89027 | valid_auc: 0.89302 |  0:00:43s\n",
      "epoch 80 | loss: 0.41336 | train_auc: 0.89037 | valid_auc: 0.89391 |  0:00:44s\n",
      "epoch 81 | loss: 0.41443 | train_auc: 0.8904  | valid_auc: 0.89354 |  0:00:44s\n",
      "epoch 82 | loss: 0.40597 | train_auc: 0.88943 | valid_auc: 0.89129 |  0:00:45s\n",
      "epoch 83 | loss: 0.41839 | train_auc: 0.88997 | valid_auc: 0.89184 |  0:00:45s\n",
      "epoch 84 | loss: 0.4088  | train_auc: 0.89102 | valid_auc: 0.89304 |  0:00:46s\n",
      "epoch 85 | loss: 0.4159  | train_auc: 0.89098 | valid_auc: 0.89286 |  0:00:47s\n",
      "epoch 86 | loss: 0.41109 | train_auc: 0.89103 | valid_auc: 0.89314 |  0:00:47s\n",
      "epoch 87 | loss: 0.41092 | train_auc: 0.89079 | valid_auc: 0.89284 |  0:00:48s\n",
      "epoch 88 | loss: 0.41209 | train_auc: 0.89029 | valid_auc: 0.89225 |  0:00:48s\n",
      "epoch 89 | loss: 0.41131 | train_auc: 0.89022 | valid_auc: 0.89274 |  0:00:49s\n",
      "epoch 90 | loss: 0.42031 | train_auc: 0.89014 | valid_auc: 0.89331 |  0:00:49s\n",
      "epoch 91 | loss: 0.40886 | train_auc: 0.89073 | valid_auc: 0.89277 |  0:00:50s\n",
      "epoch 92 | loss: 0.41176 | train_auc: 0.89053 | valid_auc: 0.89222 |  0:00:50s\n",
      "epoch 93 | loss: 0.41462 | train_auc: 0.89061 | valid_auc: 0.89265 |  0:00:51s\n",
      "epoch 94 | loss: 0.4084  | train_auc: 0.89058 | valid_auc: 0.89308 |  0:00:51s\n",
      "epoch 95 | loss: 0.4135  | train_auc: 0.89061 | valid_auc: 0.8924  |  0:00:52s\n",
      "epoch 96 | loss: 0.41175 | train_auc: 0.89006 | valid_auc: 0.89332 |  0:00:53s\n",
      "epoch 97 | loss: 0.41125 | train_auc: 0.89014 | valid_auc: 0.89359 |  0:00:53s\n",
      "epoch 98 | loss: 0.41648 | train_auc: 0.8901  | valid_auc: 0.89231 |  0:00:54s\n",
      "epoch 99 | loss: 0.41161 | train_auc: 0.88705 | valid_auc: 0.88901 |  0:00:54s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 73 and best_valid_auc = 0.8942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "# For TabNet\n",
    "tn_a = ['waist', 'sbp', 'BRI', 'dbp', 'AVI', 'BFP', 'bmi', 'sex']\n",
    "tn_b = ['smoke_merge', 'w014', 'exer_merge', 'w095', 'dr_fruwine', 'w099', 'w097', 'w074']\n",
    "tn_c = ['AVI', 'sbp', 'dbp', 'BRI']\n",
    "tn_d = ['bWC', 'BP', 'regrp38', 'regrp19']\n",
    "all_tn = ['WC','BP','BPWC_add','BPWC_mul','BPWC_dif',\n",
    "          'bWC','whr','CUNBAE','clbe','G1_INT','ss18','fate','smoke_merge',\n",
    "          'regrp15','regrp18','regrp19','regrp38']\n",
    "args = {#'gamma': 0.7, 'momentum': 0.03, 'n_independent': 3, 'n_shared': 4, 'n_steps': 2, \n",
    "    'seed': 100}\n",
    "\n",
    "prob_all = None\n",
    "res_all = pd.DataFrame()\n",
    "\n",
    "for i, s in enumerate(seeds) :\n",
    "    print('sampling: ', i+1)\n",
    "    X_train_tn, y_train, X_valid_tn, y_valid, X_test_tn, y_test, cat_idxs, cat_dims, valid_info, test_info, beta, tau, cat_col = get_mets_data(\n",
    "        one_hot=False, \n",
    "        resampling = False,\n",
    "        feature_set = None, # 7 = 'anthropometric', 8 = 'lifestyle', 9 = 'anthropometric+lifestyle', 10 = 'anthropometric+lifestyle+synthesis'\n",
    "        set_feature = tn_d,\n",
    "        add_feature= False,\n",
    "        is_tabnet = True,\n",
    "        is_eval = False,\n",
    "        seed = s\n",
    "    )\n",
    "    \n",
    "    estimator = TabNetClassifier(cat_idxs=cat_idxs, cat_dims=cat_dims, **args)\n",
    "    tn = train_with_valid(estimator, X_train_tn, y_train, X_valid_tn, y_valid)\n",
    "    \n",
    "    prob = tn.predict_proba(X_valid_tn.values[:])\n",
    "    \n",
    "    res = get_metric(prob, y_valid, 0.5)\n",
    "    res = pd.DataFrame.from_dict(res, orient='index')\n",
    "    res_all = pd.concat([res_all,res], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d36551d-1e11-4f60-b299-8e9d7fbe8c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>bac</th>\n",
       "      <th>recall</th>\n",
       "      <th>ppv</th>\n",
       "      <th>npv</th>\n",
       "      <th>sepecificity</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.761930</td>\n",
       "      <td>0.813331</td>\n",
       "      <td>0.884087</td>\n",
       "      <td>0.353540</td>\n",
       "      <td>0.975928</td>\n",
       "      <td>0.742575</td>\n",
       "      <td>0.504652</td>\n",
       "      <td>0.891294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.018106</td>\n",
       "      <td>0.007056</td>\n",
       "      <td>0.021403</td>\n",
       "      <td>0.017182</td>\n",
       "      <td>0.003991</td>\n",
       "      <td>0.023483</td>\n",
       "      <td>0.016131</td>\n",
       "      <td>0.004733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.688664</td>\n",
       "      <td>0.791922</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.294659</td>\n",
       "      <td>0.968656</td>\n",
       "      <td>0.650356</td>\n",
       "      <td>0.447928</td>\n",
       "      <td>0.881317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.758802</td>\n",
       "      <td>0.809521</td>\n",
       "      <td>0.871307</td>\n",
       "      <td>0.342628</td>\n",
       "      <td>0.973209</td>\n",
       "      <td>0.736059</td>\n",
       "      <td>0.495456</td>\n",
       "      <td>0.889370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.763893</td>\n",
       "      <td>0.814280</td>\n",
       "      <td>0.885122</td>\n",
       "      <td>0.356172</td>\n",
       "      <td>0.976290</td>\n",
       "      <td>0.745570</td>\n",
       "      <td>0.507859</td>\n",
       "      <td>0.891457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.773248</td>\n",
       "      <td>0.819199</td>\n",
       "      <td>0.896949</td>\n",
       "      <td>0.364483</td>\n",
       "      <td>0.977896</td>\n",
       "      <td>0.756775</td>\n",
       "      <td>0.516569</td>\n",
       "      <td>0.894948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.782444</td>\n",
       "      <td>0.822543</td>\n",
       "      <td>0.933489</td>\n",
       "      <td>0.380039</td>\n",
       "      <td>0.984787</td>\n",
       "      <td>0.772843</td>\n",
       "      <td>0.526926</td>\n",
       "      <td>0.899301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             acc        bac     recall        ppv        npv  sepecificity  \\\n",
       "count  30.000000  30.000000  30.000000  30.000000  30.000000     30.000000   \n",
       "mean    0.761930   0.813331   0.884087   0.353540   0.975928      0.742575   \n",
       "std     0.018106   0.007056   0.021403   0.017182   0.003991      0.023483   \n",
       "min     0.688664   0.791922   0.845238   0.294659   0.968656      0.650356   \n",
       "25%     0.758802   0.809521   0.871307   0.342628   0.973209      0.736059   \n",
       "50%     0.763893   0.814280   0.885122   0.356172   0.976290      0.745570   \n",
       "75%     0.773248   0.819199   0.896949   0.364483   0.977896      0.756775   \n",
       "max     0.782444   0.822543   0.933489   0.380039   0.984787      0.772843   \n",
       "\n",
       "              f1        auc  \n",
       "count  30.000000  30.000000  \n",
       "mean    0.504652   0.891294  \n",
       "std     0.016131   0.004733  \n",
       "min     0.447928   0.881317  \n",
       "25%     0.495456   0.889370  \n",
       "50%     0.507859   0.891457  \n",
       "75%     0.516569   0.894948  \n",
       "max     0.526926   0.899301  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prob_all_tn_a = prob_all\n",
    "#prob_all_tn_b = prob_all\n",
    "#prob_all_tn_c = prob_all\n",
    "#prob_all_tn_d = prob_all # prob_all_tn_a, prob_all_tn_b,prob_all_tn_c\n",
    "res_all_tn = res_all\n",
    "res_all_tn.transpose().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2b66a13-6f58-434c-bc4d-cdde66ef539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_all.transpose().to_csv('./fig/feature_d_tabnet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "b7f8a922-1c83-44c9-b043-9fed36aa3da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7630175658720201,\n",
       " 'bac': 0.8120140086436312,\n",
       " 'recall': 0.8788235294117647,\n",
       " 'ppv': 0.34663573085846866,\n",
       " 'npv': 0.9755981994787964,\n",
       " 'sepecificity': 0.7452044878754976,\n",
       " 'f1': 0.49717138103161396,\n",
       " 'auc': 0.8853640544165549}"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metric(tn.predict_proba(X_valid_tn.values[:]), y_valid, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "id": "695ce1ff-8cae-41df-bf4f-ec209a0c0adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.760446075663467,\n",
       " 'bac': 0.8122309292613555,\n",
       " 'recall': 0.8841893252769386,\n",
       " 'ppv': 0.3569105691056911,\n",
       " 'npv': 0.9751297577854672,\n",
       " 'sepecificity': 0.7402725332457725,\n",
       " 'f1': 0.5085432956849117,\n",
       " 'auc': 0.8867687835535003}"
      ]
     },
     "execution_count": 869,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metric(tn.predict_proba(X_test_tn.values[:]), y_test, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "id": "08236aa4-30c2-4790-adfb-66145fa3df30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.015244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.015244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.257028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.013536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.040385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7079</th>\n",
       "      <td>0</td>\n",
       "      <td>0.003861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7080</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7081</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7082</th>\n",
       "      <td>0</td>\n",
       "      <td>0.013536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7083</th>\n",
       "      <td>1</td>\n",
       "      <td>0.205696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7084 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred      prob\n",
       "0        0  0.015244\n",
       "1        0  0.015244\n",
       "2        1  0.257028\n",
       "3        0  0.013536\n",
       "4        0  0.040385\n",
       "...    ...       ...\n",
       "7079     0  0.003861\n",
       "7080     0  0.000000\n",
       "7081     0  0.000000\n",
       "7082     0  0.013536\n",
       "7083     1  0.205696\n",
       "\n",
       "[7084 rows x 2 columns]"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = {'pred':tn.predict(X_test_tn.values[:]), 'prob':prob_tn_cali}\n",
    "pd.DataFrame(data=ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dc507f-6639-4889-877c-48bce37f3918",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3627d2e0-e669-48a1-9631-605e788f4ca5",
   "metadata": {},
   "source": [
    "### Ensamble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "f0a429a4-db98-4a7d-a131-aa3f3e4d0b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_lr_train = lr.predict_proba(X_train_lr)\n",
    "prob_lr_valid = lr.predict_proba(X_valid_lr)\n",
    "prob_lr_test = lr.predict_proba(X_test_lr)\n",
    "\n",
    "prob_dt_train = dt.predict_proba(X_train_dt)\n",
    "prob_dt_valid = dt.predict_proba(X_valid_dt)\n",
    "prob_dt_test = dt.predict_proba(X_test_dt)\n",
    "\n",
    "prob_rf_train = rf.predict_proba(X_train_rf)\n",
    "prob_rf_valid = rf.predict_proba(X_valid_rf)\n",
    "prob_rf_test = rf.predict_proba(X_test_rf)\n",
    "\n",
    "prob_xgb_train = xgb.predict_proba(X_train_xgb)\n",
    "prob_xgb_valid = xgb.predict_proba(X_valid_xgb)\n",
    "prob_xgb_test = xgb.predict_proba(X_test_xgb)\n",
    "\n",
    "prob_tn_train = tn.predict_proba(X_train_tn.values[:])\n",
    "prob_tn_valid = tn.predict_proba(X_valid_tn.values[:])\n",
    "prob_tn_test = tn.predict_proba(X_test_tn.values[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abcb098-09e0-4dae-8cd1-a2d5644ad38c",
   "metadata": {},
   "source": [
    "##### with Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "6cd41c4f-0bd0-4e7a-b2e2-8510dd48299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_meta = np.vstack((prob_lr_train[:,1],prob_dt_train[:,1],prob_rf_train[:,1],prob_xgb_train[:,1],prob_tn_train[:,1]))\n",
    "X_valid_meta = np.vstack((prob_lr_valid[:,1],prob_dt_valid[:,1],prob_rf_valid[:,1],prob_xgb_valid[:,1],prob_tn_valid[:,1]))\n",
    "X_test_meta = np.vstack((prob_lr_test[:,1],prob_dt_test[:,1],prob_rf_test[:,1],prob_xgb_test[:,1],prob_tn_test[:,1]))\n",
    "\n",
    "X_train_meta = X_train_meta.T\n",
    "X_valid_meta = X_valid_meta.T\n",
    "X_test_meta = X_test_meta.T\n",
    "\n",
    "X_train_meta = pd.DataFrame(X_train_meta, columns={'lr','dt','rf','xgb','tn'})\n",
    "X_valid_meta = pd.DataFrame(X_valid_meta, columns={'lr','dt','rf','xgb','tn'})\n",
    "X_test_meta = pd.DataFrame(X_test_meta, columns={'lr','dt','rf','xgb','tn'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9689ada-4c80-45ea-a594-3e365f6fc459",
   "metadata": {},
   "source": [
    "without Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "606b6897-5f38-4325-84b3-f3713f4eac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_meta = np.vstack((prob_lr_train[:,1],prob_dt_train[:,1],prob_rf_train[:,1],prob_xgb_train[:,1]))\n",
    "X_valid_meta = np.vstack((prob_lr_valid[:,1],prob_dt_valid[:,1],prob_rf_valid[:,1],prob_xgb_valid[:,1]))\n",
    "\n",
    "X_train_meta = X_train_meta.T\n",
    "X_valid_meta = X_valid_meta.T\n",
    "\n",
    "X_train_meta = pd.DataFrame(X_train_meta, columns={'lr','dt','rf','xgb'})\n",
    "X_valid_meta = pd.DataFrame(X_valid_meta, columns={'lr','dt','rf','xgb'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fdfccd-fe18-4313-825b-09f89fe40839",
   "metadata": {},
   "source": [
    "#### Calibration Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "f6615b07-87b9-4890-8de7-bcd1132e58e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_meta = np.vstack((prob_lr_cali, prob_dt_cali, prob_rf_cali, prob_xgb_cali, prob_tn_cali)).T\n",
    "X_test_meta = pd.DataFrame(X_test_meta, columns={'lr','dt','rf','xgb','tn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "b436a454-c0d3-4934-b6bb-6f5eee717de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7531055900621118,\n",
       " 'bac': 0.8130195724033098,\n",
       " 'recall': 0.8962739174219537,\n",
       " 'ppv': 0.3509463722397476,\n",
       " 'npv': 0.9773526824978013,\n",
       " 'sepecificity': 0.7297652273846659,\n",
       " 'f1': 0.5043921790875602,\n",
       " 'auc': 0.8887942902897858}"
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = X_test_meta.mean(axis=1)\n",
    "get_metric2(prob, y_test, tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c5c507-d4bf-48c6-b6b9-41ddb45f6eb2",
   "metadata": {},
   "source": [
    "#### soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73c7c649-ced1-46ab-aef1-15da669f5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, auc, roc_curve\n",
    "\n",
    "def get_metric2(prob, label, threshold):\n",
    "    #prob = prob[:,1]\n",
    "    prd = np.where(prob>=threshold, 1, 0)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(label, prd, labels=[0,1]).ravel()\n",
    "    \n",
    "    accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "    sensitivity = tp/(fn+tp)\n",
    "    specificity = tn/(fp+tn)\n",
    "    ppv = tp/(tp+fp)\n",
    "    npv = tn/(tn+fn)\n",
    "    f1 = 2*(sensitivity*ppv)/(sensitivity+ppv)\n",
    "    balaced_accuracy = 0.5*(sensitivity+specificity)\n",
    "    fpr, tpr, thresholds = roc_curve(label, prob, pos_label=1)\n",
    "    auc_roc = auc(fpr,tpr)\n",
    "    \n",
    "    res = {'acc' : accuracy,\n",
    "           'bac' : balaced_accuracy,\n",
    "           'recall': sensitivity,\n",
    "           'ppv':ppv,\n",
    "           'npv':npv,\n",
    "           'sepecificity':specificity,\n",
    "           'f1':f1,\n",
    "           'auc':auc_roc}\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "35dac3e6-be6a-43ca-ab9f-0acee4d83e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7531055900621118,\n",
       " 'bac': 0.8130195724033098,\n",
       " 'recall': 0.8962739174219537,\n",
       " 'ppv': 0.3509463722397476,\n",
       " 'npv': 0.9773526824978013,\n",
       " 'sepecificity': 0.7297652273846659,\n",
       " 'f1': 0.5043921790875602,\n",
       " 'auc': 0.8887942902897858}"
      ]
     },
     "execution_count": 858,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = X_test_meta.mean(axis=1)\n",
    "get_metric2(prob, y_test, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f69dfe-34f2-46cc-987e-a6d3f0d4c6f9",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "e94104dc-6355-4f33-8258-aec9080bd804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-55 {color: black;background-color: white;}#sk-container-id-55 pre{padding: 0;}#sk-container-id-55 div.sk-toggleable {background-color: white;}#sk-container-id-55 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-55 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-55 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-55 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-55 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-55 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-55 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-55 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-55 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-55 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-55 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-55 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-55 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-55 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-55 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-55 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-55 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-55 div.sk-item {position: relative;z-index: 1;}#sk-container-id-55 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-55 div.sk-item::before, #sk-container-id-55 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-55 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-55 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-55 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-55 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-55 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-55 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-55 div.sk-label-container {text-align: center;}#sk-container-id-55 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-55 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-55\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-55\" type=\"checkbox\" checked><label for=\"sk-estimator-id-55\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "args = {\n",
    "    #'penalty' : 'l1',\n",
    "    #'solver' : 'liblinear',\n",
    "    #'random_state' : 100\n",
    "}\n",
    "meta = LinearRegression(**args)\n",
    "meta.fit(X_train_meta, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "73704bb8-732c-48a3-b051-100874e600a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.758450847829838,\n",
       " 'bac': 0.818430677200757,\n",
       " 'recall': 0.9007739401640291,\n",
       " 'ppv': 0.34909123466738295,\n",
       " 'npv': 0.9792577210054814,\n",
       " 'sepecificity': 0.736087414237485,\n",
       " 'f1': 0.503177931924504,\n",
       " 'auc': 0.8980555579935069}"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = meta.predict(X_valid_meta)\n",
    "get_metric2(prob, y_valid, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "6af0d1b7-9a21-4fc9-a32c-2133d0380e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7446357989836251,\n",
       " 'bac': 0.8038799093242255,\n",
       " 'recall': 0.8862034239677744,\n",
       " 'ppv': 0.3416149068322981,\n",
       " 'npv': 0.9749334516415262,\n",
       " 'sepecificity': 0.7215563946806764,\n",
       " 'f1': 0.49313533202577753,\n",
       " 'auc': 0.8813449192781583}"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = meta.predict(X_test_meta)\n",
    "get_metric2(prob, y_test, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd43514-902c-4d12-b525-f21833e6bef9",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "199c124d-510b-4c99-a761-63cb3f9ace78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-57 {color: black;background-color: white;}#sk-container-id-57 pre{padding: 0;}#sk-container-id-57 div.sk-toggleable {background-color: white;}#sk-container-id-57 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-57 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-57 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-57 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-57 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-57 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-57 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-57 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-57 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-57 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-57 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-57 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-57 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-57 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-57 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-57 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-57 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-57 div.sk-item {position: relative;z-index: 1;}#sk-container-id-57 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-57 div.sk-item::before, #sk-container-id-57 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-57 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-57 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-57 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-57 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-57 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-57 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-57 div.sk-label-container {text-align: center;}#sk-container-id-57 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-57 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-57\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=6, min_samples_leaf=4, min_samples_split=10,\n",
       "                       n_estimators=300, random_state=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-57\" type=\"checkbox\" checked><label for=\"sk-estimator-id-57\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=6, min_samples_leaf=4, min_samples_split=10,\n",
       "                       n_estimators=300, random_state=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=6, min_samples_leaf=4, min_samples_split=10,\n",
       "                       n_estimators=300, random_state=100)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {'bootstrap': True,\n",
    "        'max_depth': 6,\n",
    "        'min_samples_leaf': 4,\n",
    "        'min_samples_split': 10,\n",
    "        'n_estimators': 300,\n",
    "        'random_state' : 100}\n",
    "\n",
    "meta = RandomForestClassifier(**args)\n",
    "meta.fit(X_train_meta, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a8214b5e-cc9f-4b28-97fc-59fc629e3c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7797992471769134,\n",
       " 'bac': 0.8022831108556343,\n",
       " 'recall': 0.8329411764705882,\n",
       " 'ppv': 0.35939086294416245,\n",
       " 'npv': 0.9677712210621879,\n",
       " 'sepecificity': 0.7716250452406804,\n",
       " 'f1': 0.5021276595744681,\n",
       " 'auc': 0.8410172233931574}"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = meta.predict_proba(X_valid_meta)\n",
    "get_metric(prob, y_valid, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "00ba435e-84fd-414d-a74b-b848e9311837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7632693393562959,\n",
       " 'bac': 0.8046010961974339,\n",
       " 'recall': 0.8620342396777442,\n",
       " 'ppv': 0.3572621035058431,\n",
       " 'npv': 0.9707764505119454,\n",
       " 'sepecificity': 0.7471679527171237,\n",
       " 'f1': 0.505163765122455,\n",
       " 'auc': 0.8046010961974338}"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = meta.predict(X_test_meta)\n",
    "get_metric2(prob, y_test, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5269e2-f1cf-44ac-aa48-07f805a56b06",
   "metadata": {},
   "source": [
    "#### Tabnet (Feature + Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "22f17d5d-a433-44af-8b24-586a1d0c9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tn.reset_index(drop=True, inplace=True)\n",
    "X_train_meta.reset_index(drop=True, inplace=True)\n",
    "X_valid_tn.reset_index(drop=True, inplace=True)\n",
    "X_valid_meta.reset_index(drop=True, inplace=True)\n",
    "X_test_tn.reset_index(drop=True, inplace=True)\n",
    "X_test_meta.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "7e74dcd4-9557-4b1b-a8f0-57b7201a208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_meta = pd.concat([X_train_tn,X_train_meta], axis=1)\n",
    "X_valid_meta = pd.concat([X_valid_tn,X_valid_meta], axis=1)\n",
    "X_test_meta = pd.concat([X_test_tn,X_valid_meta], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "1293d6f5-41d6-4252-8690-4ae6ac379b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/shinhseok/anaconda3/envs/mets_gpu/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.43698 |  0:00:00s\n",
      "epoch 1  | loss: 0.39559 |  0:00:00s\n",
      "epoch 2  | loss: 0.3919  |  0:00:01s\n",
      "epoch 3  | loss: 0.38764 |  0:00:01s\n",
      "epoch 4  | loss: 0.3846  |  0:00:02s\n",
      "epoch 5  | loss: 0.38922 |  0:00:02s\n",
      "epoch 6  | loss: 0.38507 |  0:00:03s\n",
      "epoch 7  | loss: 0.3876  |  0:00:03s\n",
      "epoch 8  | loss: 0.37832 |  0:00:03s\n",
      "epoch 9  | loss: 0.37751 |  0:00:04s\n",
      "epoch 10 | loss: 0.38024 |  0:00:04s\n",
      "epoch 11 | loss: 0.38144 |  0:00:05s\n",
      "epoch 12 | loss: 0.38836 |  0:00:05s\n",
      "epoch 13 | loss: 0.38068 |  0:00:05s\n",
      "epoch 14 | loss: 0.38547 |  0:00:06s\n",
      "epoch 15 | loss: 0.3851  |  0:00:06s\n",
      "epoch 16 | loss: 0.37948 |  0:00:07s\n",
      "epoch 17 | loss: 0.38141 |  0:00:07s\n",
      "epoch 18 | loss: 0.38486 |  0:00:07s\n",
      "epoch 19 | loss: 0.37353 |  0:00:08s\n",
      "epoch 20 | loss: 0.38684 |  0:00:08s\n",
      "epoch 21 | loss: 0.3712  |  0:00:09s\n",
      "epoch 22 | loss: 0.37168 |  0:00:09s\n",
      "epoch 23 | loss: 0.37071 |  0:00:09s\n",
      "epoch 24 | loss: 0.37599 |  0:00:10s\n",
      "epoch 25 | loss: 0.37521 |  0:00:10s\n",
      "epoch 26 | loss: 0.37713 |  0:00:11s\n",
      "epoch 27 | loss: 0.37899 |  0:00:11s\n",
      "epoch 28 | loss: 0.37706 |  0:00:12s\n",
      "epoch 29 | loss: 0.37948 |  0:00:12s\n",
      "epoch 30 | loss: 0.37755 |  0:00:12s\n",
      "epoch 31 | loss: 0.38192 |  0:00:13s\n",
      "epoch 32 | loss: 0.37504 |  0:00:13s\n",
      "epoch 33 | loss: 0.38235 |  0:00:14s\n",
      "epoch 34 | loss: 0.38347 |  0:00:14s\n",
      "epoch 35 | loss: 0.37601 |  0:00:14s\n",
      "epoch 36 | loss: 0.37734 |  0:00:15s\n",
      "epoch 37 | loss: 0.38228 |  0:00:15s\n",
      "epoch 38 | loss: 0.37188 |  0:00:16s\n",
      "epoch 39 | loss: 0.37908 |  0:00:16s\n",
      "epoch 40 | loss: 0.3876  |  0:00:16s\n",
      "epoch 41 | loss: 0.37413 |  0:00:17s\n",
      "epoch 42 | loss: 0.37787 |  0:00:17s\n",
      "epoch 43 | loss: 0.38114 |  0:00:18s\n",
      "epoch 44 | loss: 0.37467 |  0:00:18s\n",
      "epoch 45 | loss: 0.37798 |  0:00:18s\n",
      "epoch 46 | loss: 0.3786  |  0:00:19s\n",
      "epoch 47 | loss: 0.36751 |  0:00:19s\n",
      "epoch 48 | loss: 0.3766  |  0:00:20s\n",
      "epoch 49 | loss: 0.37205 |  0:00:20s\n",
      "epoch 50 | loss: 0.37572 |  0:00:21s\n",
      "epoch 51 | loss: 0.37146 |  0:00:21s\n",
      "epoch 52 | loss: 0.38336 |  0:00:21s\n",
      "epoch 53 | loss: 0.37484 |  0:00:22s\n",
      "epoch 54 | loss: 0.37982 |  0:00:22s\n",
      "epoch 55 | loss: 0.37348 |  0:00:23s\n",
      "epoch 56 | loss: 0.36977 |  0:00:23s\n",
      "epoch 57 | loss: 0.38306 |  0:00:23s\n",
      "epoch 58 | loss: 0.38166 |  0:00:24s\n",
      "epoch 59 | loss: 0.37075 |  0:00:24s\n",
      "epoch 60 | loss: 0.37193 |  0:00:25s\n",
      "epoch 61 | loss: 0.37465 |  0:00:25s\n",
      "epoch 62 | loss: 0.37548 |  0:00:25s\n",
      "epoch 63 | loss: 0.37408 |  0:00:26s\n",
      "epoch 64 | loss: 0.37261 |  0:00:26s\n",
      "epoch 65 | loss: 0.3649  |  0:00:27s\n",
      "epoch 66 | loss: 0.38387 |  0:00:27s\n",
      "epoch 67 | loss: 0.3778  |  0:00:27s\n",
      "epoch 68 | loss: 0.37926 |  0:00:28s\n",
      "epoch 69 | loss: 0.37358 |  0:00:28s\n",
      "epoch 70 | loss: 0.37458 |  0:00:29s\n",
      "epoch 71 | loss: 0.37834 |  0:00:29s\n",
      "epoch 72 | loss: 0.37275 |  0:00:29s\n",
      "epoch 73 | loss: 0.37288 |  0:00:30s\n",
      "epoch 74 | loss: 0.3774  |  0:00:30s\n",
      "epoch 75 | loss: 0.37516 |  0:00:31s\n",
      "epoch 76 | loss: 0.37371 |  0:00:31s\n",
      "epoch 77 | loss: 0.37092 |  0:00:31s\n",
      "epoch 78 | loss: 0.37138 |  0:00:32s\n",
      "epoch 79 | loss: 0.37328 |  0:00:32s\n",
      "epoch 80 | loss: 0.37001 |  0:00:33s\n",
      "epoch 81 | loss: 0.37554 |  0:00:33s\n",
      "epoch 82 | loss: 0.37688 |  0:00:34s\n",
      "epoch 83 | loss: 0.37852 |  0:00:34s\n",
      "epoch 84 | loss: 0.37068 |  0:00:34s\n",
      "epoch 85 | loss: 0.3741  |  0:00:35s\n",
      "epoch 86 | loss: 0.37393 |  0:00:35s\n",
      "epoch 87 | loss: 0.37064 |  0:00:35s\n",
      "epoch 88 | loss: 0.37888 |  0:00:36s\n",
      "epoch 89 | loss: 0.37589 |  0:00:36s\n",
      "epoch 90 | loss: 0.37129 |  0:00:37s\n",
      "epoch 91 | loss: 0.37118 |  0:00:37s\n",
      "epoch 92 | loss: 0.37319 |  0:00:38s\n",
      "epoch 93 | loss: 0.37329 |  0:00:38s\n",
      "epoch 94 | loss: 0.37253 |  0:00:38s\n",
      "epoch 95 | loss: 0.37374 |  0:00:39s\n",
      "epoch 96 | loss: 0.37365 |  0:00:39s\n",
      "epoch 97 | loss: 0.36747 |  0:00:40s\n",
      "epoch 98 | loss: 0.36463 |  0:00:40s\n",
      "epoch 99 | loss: 0.3684  |  0:00:40s\n"
     ]
    }
   ],
   "source": [
    "args = {'seed':100}\n",
    "\n",
    "estimator = TabNetClassifier(cat_idxs=cat_idxs, cat_dims=cat_dims, **args)\n",
    "#meta = train_model_with_valid(estimator, X_train_meta, y_train, X_valid_meta, y_valid)\n",
    "meta = train_model(estimator, X_train_meta, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "e3f4c2db-bf83-4ab0-b032-f503526d0586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7161229611041405,\n",
       " 'bac': 0.8028792233505779,\n",
       " 'recall': 0.9211764705882353,\n",
       " 'ppv': 0.30997624703087884,\n",
       " 'npv': 0.9825974025974026,\n",
       " 'sepecificity': 0.6845819761129207,\n",
       " 'f1': 0.4638625592417061,\n",
       " 'auc': 0.8779280832854314}"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = meta.predict_proba(X_valid_meta.values[:])\n",
    "get_metric(prob,y_valid,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "a608a0fa-6110-47d7-a363-309f37fe6ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7780914737436476,\n",
       " 'bac': 0.78287695364845,\n",
       " 'recall': 0.7895266868076536,\n",
       " 'ppv': 0.36516068933395435,\n",
       " 'npv': 0.957666599149281,\n",
       " 'sepecificity': 0.7762272204892464,\n",
       " 'f1': 0.4993630573248408,\n",
       " 'auc': 0.8667796559168159}"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = meta.predict_proba(X_test_meta.values[:])\n",
    "get_metric(prob,y_test,0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
